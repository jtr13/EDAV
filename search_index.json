[["index.html", "edav.info/ Welcome 0.1 Everything you need for Exploratory Data Analysis &amp; Visualization 0.2 Contact 0.3 License 0.4 Colophon", " edav.info/ Zach Bogart, Joyce Robbins 2021-03-27 Welcome 0.1 Everything you need for Exploratory Data Analysis &amp; Visualization This resource has everything you need and more to be successful with R, this course, and beyond. Let’s get started! With this resource, we try to give you a curated collection of tools and references that will make it easier to learn how to work with data in R. In addition, we include sections on basic chart types/tools so you can learn by doing. There are also several walkthroughs where we work with data and discuss problems as well as some tips/tricks that will help you. We hope this resource serves you well! This resource is specifically tailored to the GR5702 Exploratory Data Analysis and Visualization course offered at Columbia University. However, anyone interested in working with data in R will benefit from perusing these pages. Happy coding! A French translation is also available! 0.2 Contact Zach Bogart: Website / Twitter / GitHub Joyce Robbins: Columbia Profile / GitHub 0.3 License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. 0.4 Colophon The EDAV Logo, the url/404 banners, and associated chapter icon designs are designed by Zach Bogart and published with permission. The url and 404 banners have been adapted into a typeface called Koji. Selected chapter icons can be accessed/purchased at The Noun Project. Please attribute the creator if using them for external purposes (see their icon attribution guidelines for more information). "],["intro.html", "1 Introduction 1.1 Overview 1.2 Types of Assistance 1.3 Help improve edav.info/ 1.4 Fun stuff 1.5 Acknowledgments", " 1 Introduction 1.1 Overview This chapter introduces how this resource is organized, explains how you can add to this resource, and includes some general acknowledgments. 1.2 Types of Assistance Chapters in this resources are color-coded to indicate the type of assistance the chapter provides. Below is an explanation of each type: 1.2.1 Information (Blue) Blue pages contain basic information. Examples of blue pages include this introduction page and the basics page, which explains how to setup R/RStudio as well as ways to get help if you need it. Blue pages are the help desk of this resource: look to them if you are lost and need to find your way. 1.2.2 Walkthroughs (Red) Red pages contain more extensive walkthroughs. An example of a red page is the iris walkthrough, where a well-known dataset is presented as a pretty scatterplot and steps are shown from start to finish. This page type is the most thorough: it tries to provide full documentation, explanations of design choices, and advice on best practices. It’s like going to office hours and having a great clarifying chat with a course assistant…in article form. If you would like to see a fully-worked-through example of something with a lot of guidance along the way, check out the red pages. 1.2.3 Documentation (Green) Green pages contain more compact documentation. An example of a green page is the histogram page, which includes simple examples of how to create histograms, when to use them, and things to be aware of/watch out for. The green pages hold your hand much less than the red pages: they explain how to use a chart/tool using examples and simple terms. If you have an idea in mind and are just wondering how to execute it, the green pages will help fill in those gaps. 1.2.4 References (Yellow) Yellow pages contain simple collections of references. An example of a yellow page is the external resources page, which is a list of materials that you can look through and learn from. Yellow pages have the least amount of hand-holding: they are collections of resources and bare-boned tutorials that will help you learn about new things. 1.3 Help improve edav.info/ This resource is an ongoing creation made by students, for students. We welcome you to help make it better. Not finding what you are looking for? Think a section could be made clearer? Consider helping improve edav.info/ by submitting a pull request to the github page. Don’t understand that last sentence? We have a page on how you can contribute to edav.info/. 1.4 Fun stuff 1.4.1 T-shirts Zach Bogart has made a few t-shirts available on Teespring so you can show your love for EDAV and R. Hope you enjoy! 1.5 Acknowledgments 1.5.1 Our Contributors Thank you so much to everyone who has contributed. You make edav.info/ possible. Aashna Kanuga (@aashnakanuga), @Akanksha1Raj, Akhil Punia (@AkhilPunia), Akshata Patel (@akshatapatel), Angela Li (@angela-li), @anipin, @AshwinJay101, Eric Boxer (@Ecboxer), @excited-student, @hao871563506, Harin Sanghirun (@harin), @jw2531, @kiransaini, @leahparreztnik, Louis Massera (@louismassera), @naotominakawa, Neha Saraf (@nehasaraf1994), Oleh Dubno (@odubno), Ramy Jaber (@ramyij), Rod Bogart (@rodbogart), @Somendratripathi, Tim Kartawijaya (@TimKartawijaya), @ujjwal95, Zhida Zhang (@ZhangZhida) "],["basics.html", "2 R Basics 2.1 Top 10 Essentials Checklist 2.2 Troubleshooting 2.3 Tips &amp; Tricks 2.4 Submitting Assignments 2.5 Getting help", " 2 R Basics So…there is soooo much to the world of R. Textbooks, cheatsheets, exercises, and other buzzwords full of resources you could go through. As of 2021-03-27, there are over 17300 packages on CRAN, the network through which R code and packages are distributed. It can be overwhelming. However, bear in mind that R is being used for a lot of different things, not all of which are relevant to EDAV. In an effort to get everyone on the same page, here is a checklist of essentials so you can get up and running with this course. The best resources are scattered in different places online, so bear with links to various sites depending on the topic. 2.1 Top 10 Essentials Checklist (r4ds = R for Data Science by Garrett Grolemund and Hadley Wickham, free online) Install R (r4ds) – You need to have this installed but you won’t open the application since you’ll be working in RStudio. If you already installed R, make sure you’re current! The latest version of R (as of 2021-03-27) is R 4.0.4 “Lost Library Book” released on 2021/02/15. Install RStudio (r4ds) – Download the free, Desktop version for your OS. Working in this IDE will make working in R much more enjoyable. As with R, stay current. RStudio is constantly adding new features. The latest version (as of 2021-03-27) is . Get comfortable with RStudio – In this chapter of Bruno Rodriguez’s Modern R with the Tidyverse, you’ll learn about panes, options, getting help, keyboard shortcuts, projects, add-ins, and packages. Be sure to try out: Do some math in the console Create an R Markdown file (.Rmd) and render it to .html Install some packages like tidyverse or MASS Another great option for learning the IDE: Watch Writing Code in RStudio (RStudio webinar) Learn “R Nuts and Bolts” – Roger Peng’s chapter in R Programming will give you a solid foundation in the basic building blocks of R. It’s worth making the investing in understanding how R objects work now so they don’t cause you problems later. Focus on vectors and especially data frames; matrices and lists don’t come up often in data visualization. Get familiar with R classes: integer, numeric, character, and logical. Understand how factors work; they are very important for graphing. Tidy up (r4ds) – Install the tidyverse, and get familiar with what it is. We will discuss differences between base R and the tidyverse in class. Learn ggplot2 basics (r4ds) – In class we will study the grammar of graphics on which ggplot2 is based, but it will help to familiarize yourself with the syntax in advance. Avail yourself of the “Data Visualization with ggplot2” cheatsheet by clicking “Help” “Cheatsheets…” within RStudio. Learn some RMarkdown – For this class you will write assignments in R Markdown (stored as .Rmd files) and then render them into pdfs for submission. You can jump right in and open a new R Markdown file (File &gt; New File &gt; R Markdown…), and leave the Default Output Format as HTML. You will get a R Markdown template you can tinker with. Click the “knit” button and see what happens. For more detail, watch the RStudio webinar Getting Started with R Markdown Use RStudio projects (r4ds) – If you haven’t already, drink the Kool-Aid. Make each problem set a separate project. You will never have to worry about getwd() or setwd() again because everything will just be in the right places. Or watch the webinar: “Projects in RStudio” Learn the basic dplyr verbs for data manipulation (r4ds) – Concentrate on the main verbs: filter() (rows), select() (columns), mutate(), arrange() (rows), group_by(), and summarize(). Learn the pipe %&gt;% operator. Know how to tidy your data – The pivot_longer() function from the tidyr package – successor to gather() – will help you get your data in the right form for plotting. More on this in class. Check out these super cool animations, which follow a data frame as it is transformed by tidyr functions. General advice: don’t get caught up in the details. Keep a list of questions and move on. 2.2 Troubleshooting 2.2.1 Document doesn’t knit Click “Session” “Restart R” and then run the chunks one by one from the top until you find the error. 2.2.2 Functions stop working Strange behavior from functions that previously worked are often caused by function conflicts. This can happen if you have two packages loaded with the same function names. To indicate the proper package, namespace it. Conflicts commonly occur with select and filter and map. If you intend the tidyverse ones use: dplyr::select, dplyr::filter and purrr::map. Other culprits: dplyr::summarise() and vcdExtra::summarise() ggmosaic::mosaic() and vcd::mosaic() leaflet::addLegend() and xts::addLegend() Run across other conflicts or have more troubleshooting tips? Submit an issue. 2.3 Tips &amp; Tricks 2.3.1 Knitr Up your game with chunk options: check out the official list of options – and bookmark it! Some favorites are: warning=FALSE message=FALSE – especially useful when loading packages cache=TRUE – only changed chunks will be evaluated, be careful though since changes in dependencies will not be detected. fig.… options, see below 2.3.2 RStudio keyboard shortcuts option-command-i (“insert R chunk”) ```{r} ``` shift-command-M %&gt;% (“the pipe”) 2.3.3 Sizing figures (and more) Always use chunk options to size figures. You can set a default size in the YAML at the beginning of the .Rmd file as so: output: pdf_document: fig_height: 3 fig_width: 5 Another method is to click the gear ⚙️ next to the Knit button, then Output Options…, and finally the Figures tab. Then as needed override one or more defaults in particular chunks: {r, fig.width=4, fig.height=2} Figure related chunk options include fig.width, fig.height, fig.asp, and fig.align; there are many more. 2.3.4 Viewing plots in plot window Would you like your plots to appear in the plot window instead of below each chunk in the .Rmd file? Click ⚙️ and then Chunk Output in Console. 2.4 Submitting Assignments Here’s a quick run-down of how to submit your assignments using R Markdown and Knitr. Create R Markdown file with PDF output format: We will often provide you with a template, and feel free to add on to it directly, but make sure its output format is set to pdf_document. Write out your explanations and insert code chunks to answer the questions provided. If you want to make a new file, go to File &gt; New File &gt; R Markdown… and set the Default Output Format to PDF. Either way, the header of the .Rmd file should look something like this: Add PDF Dependencies: As stated when you create a new R Markdown file, the PDF output format requires TeX: Make sure you download TeX for your machine. Here are some Medium articles on the process of creating PDF reports (the articles cover starting from scratch with no installs at all, but you can skip over to installing TeX only): Mac OS Windows This can be a little complicated, but it will make that Knit button near the top of the IDE magically generate a PDF for you. If you are in a rush and want a shortcut, you can instead set the Default Output Format to HTML. When you open the file in your browser, you can save it as a PDF. It will not be as nicely formatted, but it will still work. 2.5 Getting help via https://dev.to/rly First off…breeeeeeathe. We can fix this. There are a bunch of resources out there that can help you. 2.5.1 Things to try Remember: Always try to help yourself! This article has a great list of tools to help you learn about anything you may be confused by. This includes learning about functions and packages as well as searching for info about a function/package/problem/etc. This is the perfect place to learn how to get the info you need. The RStudio Help menu (in the top toolbar) is a fantastic place to go for understanding/fixing any problems. There are links to documentation and manuals as well as cheatsheets and a lovely collection of keyboard shortcuts. Vignettes are a great way to learn about packages and how they work. Vignettes are like stylized manuals that can do a better job at explaining a package’s contents. For example, ggplot2 has a vignette on aesthetics called ggplot2-specs that talks about different ways you can map data to different formats. Typing browseVignettes() in the console will show you all the vignettes for all of the packages you have installed. You can also see vignettes by package by typing vignette(package = \"&lt;package_name&gt;\") into the console. To run a specific vignette, use vignette(\"&lt;vignette_name&gt;\"). If the vignette can’t be resolved, include the package name as well: vignette(\"&lt;vignette_name\", package = \"&lt;package_name&gt;\") Don’t ignore errors. They are telling you so much! If you give up because red text showed up in your console, take the time to see what that red text is saying. Learn how to read errors and what they are telling you. They usually include where the problem happened and what R thinks the problem stems from. More Advanced: Learn to love debugger mode. Debugging can have a steep learning curve, but huge payoffs. Take a look at these videos about debugging with R. Topics include running the debugger, setting breakpoints, customizing preferences, and more. Note: R Markdown files have some limitations for debugging, as discussed in this article. You could also consider working out your code in a .R file before including it in your R Markdown homework submission. 2.5.2 Help me, R community! Relax. There are a bunch of people using the same tools you are. Your fellow classmates are a good place to start! Post questions to Piazza asking for help. There is a lot of great documentation on R and its functions/packages/etc. Get comfy with R Documentation and it will help you immensely. There is a vibrant RStudio Community page. Also, R likes twitter. Check out #rstats or maybe let Hadley Wickham know about a wonky error message. "],["project.html", "3 Final Project Assignment 3.1 Overview 3.2 General info 3.3 Report format 3.4 Reproducible workflow 3.5 Code 3.6 Feedback 3.7 Peer review 3.8 Writing advice 3.9 Audience ready style 3.10 Grading 3.11 Resources", " 3 Final Project Assignment 3.1 Overview This section goes over what’s expected for the final project, Fall 2020. General Note: Please note that this sheet cannot possibly cover all the “do’s and don’ts” of data analysis and visualization. You are expected to follow all of the best practices discussed in class throughout the semester. 3.2 General info 3.2.1 Goal The goal of this project is to perform an exploratory data analysis / create visualizations with data of your choosing in order to gain preliminary insights on questions of interest to you. 3.2.2 Teams For this assignment you will work in teams of 2-4 people. If you wish to select your own partner, please do so by the date specified in CourseWorks. To indicate who you are working with, sign up in the People section of CourseWorks. Do not click the +Group button; rather, drag your names into one of the groups already created with the name “Final Project ”. (If you don’t follow these instructions and create your own group, it will not be linked to the Final Project assignment and therefore you won’t be able to submit your project properly as a team.) Please join the groups in order starting with group #1 so that we don’t end up with gaps in the group numbers. Piazza is a good place to post your interests and look for partners. After the date indicated, partners will be randomly assigned. (This is actually a better option in terms of preparing yourself for a work environment with colleagues that you don’t know well.) If you wish to be assigned to a partner before that date so you can get started earlier (recommended) please email me. Once the groups are set up, we will ask for a short description of your project ideas, so start planning! 3.2.3 Topics The topic you choose is open-ended… choose something that you are intereted in and genuinely curious about! Think of some questions that you don’t know the answer to. Next look for data that might help you answer those questions. 3.2.4 Data The data can be pulled from multiple sources; it does not need to be a single dataset. Be sure to get data from the original source. For example, if you wish to work with data collected and distributed by the Centers for Disease Control, that is where you should go to access the data, not a third party that has posted the data. Do not use datasets that have been processed and cannot be traced to the source. For this reason, datasets from Kaggle are not good choices unless you can find the original source of the data. 3.3 Report format Your report should be submitted as a bookdown book, which you can create from the final project template. Be sure to follow the instructions provided in the repo README. (There are other templates in my account. Make sure you use this one.) Your report will include the following chapters, as shown in this rendered version of the final project template. (The links to the GitHub repo won’t work since the placeholders in the Note that each chapter is an .Rmd file in the top level directory of the GitHub repo. You can have additional folders in your repo that will not be part of the bookdown book, for example, pre-processing scripts, experimental work, etc. However, do not include additional .Rmd files in the top level directory because they either will inadvertently end up in the bookdown book, or cause errors / warnings. To turn in your final project, just submit the link to the rendered book (such as “https://jtr13.github.io/EDAVtemplate/” to the CourseWorks final project assignment.) After the deadline, do not make any changes to the main branch of your GitHub repo. I. Introduction Explain why you chose this topic, and the questions you are interested in studying. Provide context for readers who are not familiar with the topic. (suggested: approximately 1/2 page) II. Data sources Describe the data sources: who is responsible for collecting the data? How is it collected? If there were a choice of options, explain how you chose. Provide some basic information about the dataset: types of variables, number of records, etc. Describe any issues / problems with the data, either known or that you discover. (suggested: approximately 1 page) III. Data transformation Describe the process of getting the data into a form in which you could work with it in R. If your code does not lend itself to being including in the .Rmd file, provide a link to the folder or file(s) that contain(s) that code. (suggested: approximately 1/2 page) IV. Missing values Describe any patterns you discover in missing values. (suggested: 2 graphs plus commentary) V. Results You have a lot of freedom to choose what to do, as long as you restrict yourselves to exploratory techniques (rather than modeling / prediction approaches). In addition, your analysis must be clearly documented and reproducible. Provide a short nontechnical summary of the most revealing findings of your analysis written for a nontechnical audience. Take extra care to clean up your graphs, ensuring that best practices for presentation are followed, as described in the audience ready style section below. Use subheadings as appropriate. See Todd Schneider’s blog posts for examples of thoughtful, informative subheadings. (suggested: 5-10 graphs plus commentary, a plot with multiple facets counts as 1 graph) VI. Interactive or animated component Select one (or more) of your key findings to present in an interactive or animated format using D3 version 6. Be thoughtful about how you use interactivity and/or animation: using these features to add value in a way that would not be possible with a static graph. Design with the goal of users walking away with a new understanding of the data and a clear understanding of the purpose of the visualization. If the tool is interactive, provide clear instructions on how the user should engage. Thoughtfulness is more important than technical prowess; in all aspects of the project, think quality, not quantity. Interactive graphs must follow all of the best practices as with static graphs in terms of perception, labeling, accuracy, etc. Please see this chapter on sharing your D3 code online for options for sharing the interactive portion of your project. You are encouraged to share experiences on Piazza to help classmates with the publishing process. Note: the interactive component is worth approximately 25% of the final project grade. Therefore, do not spend 90% of your time on it… concentrate on the exploratory data analysis piece. VII. Conclusion Discuss limitations and future directions, lessons learned. (suggested: approximately 1/2 page) 3.4 Reproducible workflow It is important to think about a reproducible workflow from the very start. Everything you do should be guided by the question: Could others repeat what you’ve done based on the materials and explanations you’ve provided? In practical terms, this means: Use git/GitHub for version control, collaboration, and sharing code. All team members should have write access (i.e. be collaborators) on the same repo, either belonging to one team member or part of a GitHub organization that you create. With write access, think of the repo as “yours” and follow the “Your repo with branching” workflow. In general, keep code, output, and analysis together in the .Rmd chapter files. If there are pieces of your project that can’t be included in the .Rmd, then document why this is the case and provide clear steps on how your process can be reproduced. Use scripts for importing, cleaning and transforming data. As above, if it is not feasible or practical to do so, explain why and clearly document your workflow. Document as you go. Copy and paste helpful links, Stack Overflow posts, whatever you use into comments in the places you use them. You will be happy you did so. Recommended reading: Ten Simple Rules for Reproducible Computational Research. (Not all rules are applicable to this assignment but it will help you think about and develop a solid personal workflow plan going forward.) 3.5 Code All of your code should be stored on GitHub. We will spend some time in class learning how to set up a Git/GitHub workflow. All group members should have write access to the same repo. You will need to discuss your work plan to avoid merge conflicts. Keep your project organized. Create folders that work for you, such as data/raw, data/clean, preprocess, etc. The static visualizations should be done in R, but other pieces, such as data importation and cleaning do not have to be. 3.6 Feedback At any point, you may ask the TAs or the instructor (Joyce) for advice. Our primary role in this regard will be to provide general guidance on your choice of data / topic / direction. As always, you are encouraged to post specific questions to Piazza, particularly coding questions and issues. You may also volunteer to discuss your project with the class in order to get feedback–if you’d like to do this, email the instructor to schedule a date. 3.7 Peer review After final projects are turned in, you will be asked write peer reviews of other projects. Each individual will be assigned two project groups to review, and instructions will be provided. Note: part of the grade you receive for the class is based on the quality of review that you write, not on the feedback that your project receives. Your grade for the project (as for all other assignments for the class) will be determined solely by the instructor and TAs. 3.8 Writing advice 3.8.1 Start early Don’t wait to start writing. Your overall project will undoubtedly be better if you give up trying to get that last graph perfect or the last bit of analysis done and get to the writing. 3.8.2 Tell it like it is Be as intellectually honest as possible. That means pointing out flaws in your work, detailing obstacles, disagreements, decision points, etc. – the kinds of “behind-the-scene” things that are important but often left out of reports. 3.8.3 Hide code The bookdown template is set up to do this for you. It includes an _common.R file, referenced in the _bookdown.yml file, which sets knitr chunk options for all chapters. Since we will not view the code in the bookdown book, be sure that the View and Edit GitHub link buttons work properly. 3.8.4 Interpret your graphs All graphs should be accompanied by textual description / interpretation. 3.8.5 Use simple formatting Using RMarkdown makes it easy to combine code, text and graphs into a reproducible workflow. Fancy formatting is not its strength. Therefore, focus on the text and graphs, not the formatting. If you’re not sure if something is important to focus on or not, please ask. 3.8.6 Link effectively Link to your sources; you do not need to create a reference or bibliography section. In addition, link to your own materials that cannot be included directly in the report. Use descriptive links: “See this GitHub repo” is better than “See here.” 3.9 Audience ready style As we’ve discussed throughout the semester, standards are higher for clarity in graphs designed to be shared with others. While “good enough” is our standard for EDA, we need to go the extra mile for presentation. The following is checklist of items to address to make your graphs presentation ready. (You do not have to worry about these items for the EDA section.) Title, axis labels, tick mark labels, and legends should be comprehensible (easy to understand) and legible (easy to read / decipher). Tick marks should not be labeled in scientific notation or with long strings of zeros, such as 3000000000. Instead, convert to smaller numbers and change the units: 3000000000 becomes “3” and the axis label “billions of views”. Units should be intuitive (An axis labeled in month/day/year format is intuitive; one labeled in seconds since January 1, 1970 is not.) The font size should be large enough to read clearly. The default in ggplot2 is generally too small. You can easily change it by passing the base font size to the theme, such as + theme_grey(16) (The default base font size is 11). The order of items on the axes and legends should be logical. (Alphabetical is usually not the best option.) Colors should be color-vision-deficiency-friendly. If categorical variable levels are long, set up the graph so the categorical variable is on the y-axis and the names are horizontal. A better option, if possible, is to shorten the names of the levels. Err on the side of simplicity. Don’t, for example, overuse color when it’s not necessary. Ask yourself: does color make this graph any clearer? If it doesn’t, leave it out. Test your graphs on nontechnical friends and family and ask for feedback. Above all, have fun with it 3.10 Grading We grade holistically, which means that we consider the project as a whole, we do not follow a rubric with dozens of items each worth a few points. Each project is different and this gives you the flexibility to devote more or less to various aspects of the project depending on what’s appropriate, within reason. We are more impressed by quality than quantity. In determining grades, we take the following into account: Originality Are your questions thought-provoking? Do they encourage the reader to think about the topic in a new way? Real world context Do your graphs and textual descriptions reflect a solid understanding of what your data mean? Is it clear why you are asking the questions that you are asking? Are your interpretations reasonable? Reproducibility Did you provide all of your code in a manner that will be easy for the reader to rerun your analysis, and include an explanation for any pieces that cannot be reproduced? Is your code clear? Multidimensionality Do you examine multidimensional relationships and present them clearly? Choice of graph forms Are your graph forms good choices for your data? Parameters / design decisions Have you made good choices in parameters, color, etc.? Standards Do your graphs meet audience ready style standards? Interactive part Are the instructions clear? How well does the interactive component connect with the goals of the project? Does it help the reader understand the main conclusions? Technical competence Do your links work? Are your graphs displaying properly? 3.11 Resources “Tidy Tuesday Screencast: analyzing college major &amp; income data in R” David Robinson explores a dataset in R live, without looking at the data in advance. This may be helpful in figuring out how to get started. "],["contribute.html", "4 Contribute to this Resource 4.1 Why contribute? 4.2 What to contribute 4.3 Ways to contribute 4.4 Submit an issue 4.5 GitHub only walkthrough 4.6 Resources", " 4 Contribute to this Resource 4.1 Why contribute? We don’t want edav.info/ to be just another resource. Rather, we want it to be your resource. If there are things that trip you up or cause you frustration, chances are you’re not alone. Everyone comes to this course with different backgrounds and expertise. Being able to collect all that knowledge in one place is this resource’s mission and you can help move that mission forward. 4.2 What to contribute The focus of edav.info is the coding aspect of data visualization using R. It is not meant to substitute for course lectures or provide much theory about exploratory data analysis and visualization. We want to make it easy for students to find what they need quickly to produce a particular kind of graph. We are open to a wide range of contributions but we do have some ground rules: We are happy to receive original work that will help someone else. For example, if you worked out how to use a function that is not documented well, or mastered a type of graph that was difficult to get right, please share! On the other hand, if good tutorials already exist, there’s no need to repeat the work. Just submit a link to the resource with a short description to the appropriate chapter. And of course don’t forget to cite your sources by providing links. 4.3 Ways to contribute There are many ways you can contribute: For large or small ideas, without providing the code submit an issue (very simple, much appreciated) For simple changes contribute on GitHub (this can all be done on GitHub.com… we’ve got a full walkthrough explaining how) For more complex changes install Git and and work locally. Detailed instructions can be found in our Git/GitHub Resources chapter (the next level, also much appreciated) – adventurous users may solve an open issue (more advanced/open-ended) 4.4 Submit an issue If your proposed change is more complex, consider letting us know by submitting an issue. Maybe you have a great idea for a brand new chapter, something we have not covered but would like to see here in this resource (a new chart page, say; or a walkthrough using a specific tool/package). It may be a little too complicated to contribute directly. What to do? Submit an issue, of course! Issues are tasks you can post to a GitHub repo that people can then take on and fix. They can be small (“this link is broken” / “add this resource”) or complex (“I would love to have a chapter on…” / “reformat this code chunk in this way”). Once posted, issues can be taken on by anyone. You do not have to know how to code up your issue; from fixing a bug to proposing a resource we should link to, we appreciate any feedback you have and will take it all into consideration. How to submit issues: Go to our GitHub repo and click on the Issues Tab Click on “New Issue” Propose your Issue and click “Submit new issue” That’s it! We appreciate your input and will take your issue into account in improving edav.info/ Notes about submitting issues: Make sure your changes are not already an open issue (so as not to have redundant issues) Please thoroughly explain your proposed change when posting a new issue Consider using labels to specify the kind of issue, such as “bug”, “enhancement”, “help wanted”, “question”, or create your own. For more info, please consider reading the Open Source Guide on how to contribute. 4.5 GitHub only walkthrough You will need to create a github.com account if you don’t have one, but you do not need to install git locally. Note: This is a full walkthough that follows a hypothetical student that spots a typo and uses a pull request to fix it. Although the instructions are written for proposing a change to edav.info, they apply to making changes to any repo. Navigate to the file you wish to edit, and click the pencil button: Do not click the fork button on the home page of the repo. Then jump ahead to Step 4 below. One way to contribute to edav.info is to contribute directly by editing a chapter. At the top of every page of this resource, you will see an icon that looks like this: . Clicking it will open a new tab where you can edit the markdown for that page on our GitHub repo and submit your change as a pull request. Essentially, you will create a copy of our repo, make your desired changes, and suggest to us that we include them. If we approve of your changes, they will be rendered and published to the site. Contributing directly in this manner works best if the change you are proposing is something relatively small, such as a typo/grammatical error or an unclear phrasing/explanation. In general, it doesn’t work well to propose changes to code directly on GitHub. Ok, we’re good to go. Get ready to hit lots of big green buttons! One last thing: remember there’s no way to make changes to someone else’s repo unless you submit a pull request and the owner merges it. So don’t be afraid to act like the repo owner and click to edit files. You will be editing a copy of the files; it’s ok! Step 1: Find something to change I’m pretty sure they meant to write “repository” here. Oops. Let’s fix it for them! That’s not how you spell “repository”! Let’s fix it. Step 2: Click the edit button To make the fix, we click on the edit icon, , at the top of the page. This will take us to their GitHub repo, where all the code for this resource is stored. Note: You need to have a valid GitHub account to contribute. In this example, we are using a dummy account called excited-student so if you see it in a screenshot, know that it would be replaced by your own username. Hit this icon to go to GitHub. Step 3: Fork the repo This is our first edit to the repo, so GitHub shows us a page like the one below. No worries! We just hit the big green button labeled Fork this repository and propose changes and we’ll be good to go (as you will see, big green buttons are our friends). Note: you will not have to fork the repo every time. If you propose another change in the future, the edit icon, , will jump you directly to this point of the walkthrough. Haven’t forked the repo before? No worries; the big green button will solve everything. Now that we have successfully forked the repo, we can see the code for the page we want to edit. Note: That little blue blurb at the top is spelling out what is happening/going to happen: we have made a copy of a repo because we don’t have write access to it . So, after we make our change on this page, we will inform the owners of the repo about our edits by using a pull request. GitHub can be super overwhelming, but it will try its darndest to inform you what will happen along the way. Ready to edit the code. The blue blurb is worth reading. Step 4: Fix the typo Let’s fix that embarrassing typo! We update the code right in this editor, include an explanation for what we changed/why we made the change, and then hit the big green button labeled Propose file change. Gotta love those big green buttons! Make your edits, include a quick explanation, and hit the big green button. Step 5: Comparing changes Now GitHub is once again helping out by letting us review the changes we made. On this page we can review our proposed changes by scrolling down and looking at the diffs. Our fix is very simple so there isn’t much to see. Once again, we are going to push the big green button, this time labeled Create pull request. This will start the process of letting the edav.info/ people know that we would like them to include our changes (in git-speak, we are requesting that the edav.info/ people do a git merge to update their files with our proposed changes.) Note that the proposed changes are in a branch called patch-1 on our repo; we are asking to merge them into the master branch on their repo. Chance to review your changes. Once satisfied, hit the big green button to start a pull request. Step 6: Open a pull request Here we are at the pull request page. Notice the green checkmark that says “Able to merge” (a good sign that everything is going smoothly). Now we explain our pull request with some comments and, once again, hit the big green button labeled Create pull request. Note: You may be asking, “Why do I have to type this explanation in again?”. This is because the explanation we wrote in Step 5 (where we edited the file) is a commit. We could have had multiple commits at once that we wanted to bundle into one pull request. This step is a way to explain the pull request as a whole. It is redundant for us because our change is so small and only has one commit. Still totally lost? This GitHub Guide on Understanding the GitHub Flow is an incredibly helpful read and our GitHub Resources page also has a lot of helpful links. Explain your pull request and hit the big green button. What now? Congratulations are in order! We have successfully opened a pull request on a GitHub repo! Now one of the repo owners (like the guy writing this tutorial, for example ) has to decide if they want to include your pull request or not. In this case they’ll certainly approve it, but know that they may decide against adding your changes. For more info, read the section of the Open Source Guides on what happens after you submit a contribution. Note: Be aware that the icon shown below may initially be yellow to signal that some tests are being performed to check the conflicts of your proposal with the original repo. It should turn green if everything passes. We did it! Now the maintainers will review our changes and get back to us… And now we wait… via GIPHY What’s this!? We have received an email from one of the repo owners, Zach Bogart. And it says that they merged the change! Huzzah! We click on the number to take us back to the pull request we opened. We got an email! And it says they merged! Click that number to see the updated pull request. Here we are at the updated pull request page. Notice that everything has turned purple. Purple is the best color to see on GitHub; it’s the color of victory. It signals that our pull request was merged with the repo, meaning our change has become part of the repo! Also, notice the button that says Delete branch. Since all the work on our branch was merged with the repo, it has served its purpose and can be deleted safely. Everything is purple! Woot! Can safely delete our branch Now if we go back to the main page of the repo, we can see our merge was the most recent addition. And, if we scroll down, we will see that github_resources.Rmd, the file we edited, has been updated recently and it shows our commit message “fix typo”. We did it! Let’s check out the site to see our change published for the whole internet to see! Look! There’s our merged pull request added to the repo! And the edits we made to github_resources.Rmd! There it is! We go back to the page we edited and now our typo fix has been included!Note: The changes will take several minutes to appear on the site after notification of a successful merge. This is because we use Travis CI on the backend of our repo and it takes a little time for it to re-render the site pages. If you want to learn more about how you can use Travis CI to auto-magically generate your work, checkout our section on Hooking Up Travis to a GitHub bookdown book in the Publishing Resources page. Look at that! It’s published! So many exclamation points!!! We contributed to a GitHub repo! Hooray! Time to celebrate! via GIPHY Looking ahead: the next edit If you have a second edit to propose, simply follow the instructions again. As noted above, the second time through you will not be asked to fork the repo again. If you look closely at the pull request for the second edit, the branch to be merged will be named patch-2 instead of patch-1. Although GitHub keeps mentioning “your fork” as you proceed through the process, this is not really something that you have to concern yourself with. In fact, you’re better off not! In fact, you should stay away from your fork – that is, your copy of the EDAV repo in your GitHub account, because it will inevitably get behind the main one and cause you trouble if you work on the old version and then try to create a pull request. So, the bottom line is, each time you have an edit to propose, go directly to either edav.info or https://github.com/jtr13/EDAV and start the editing process there, not on your fork!. 4.6 Resources Our GitHub repo: Link to the GitHub repository for edav.info/ Open Source Guide: Fantastic guide on how to contribute to projects like this one "],["github.html", "5 GitHub/git Resources 5.1 Overview 5.2 First things first 5.3 Now what? 5.4 The no branch workflow 5.5 Your repo with branching 5.6 1st PR on another repo with branching 5.7 2nd-nth PR on another repo with branching 5.8 Fixing mistakes 5.9 Troubleshooting 5.10 Other resources", " 5 GitHub/git Resources 5.1 Overview This section describes workflows for working with GitHub/git and advice on how to collaborate in teams on large coding projects. If you’re interested in learning how to propose changes to edav.info (or any other repo) without leaving github.com, see our Contributing to this Resource chapter. Ok, not satisfied with fixing typos on GitHub? Ready to work locally and move code back and forth between your repositories (or someone else’s) and your machine? For that you’ll need Git, a widely used version control system. It is super useful and powerful, but people also find it quite annoying and difficult to understand. Rather than trying to master the whole system, we suggest beginning with some basic workflows, as outlined below. You can derive great benefits from it without being an expert (trust me, I know!) 5.2 First things first Install Git To do so, follow the instructions in the Install Git chapter of Happy Git with R. Tell git your name and email address. Introduce yourself to Git in Happy Git explains it all. (Optional) Make sure that you can pull from and push to GitHub from your computer Connect to GitHub. 5.3 Now what? Choose the right section based on what you’re trying to accomplish: If you are working by yourself and just getting started: the no branch workflow. If you are working by yourself and want to learn how to use branches, or are a collaborator (i.e. have write access) to another repo: your repo with branching. If you are not a collaborator (that is, don’t have write access to a repository) but wish to contribute to a project for the first time: 1st PR on another repo with branching. If you are not a collaborator but wish to contribute to the same project again: 2nd-nth PR on another repo with branching. 5.4 The no branch workflow To get comfortable with Git, start with this basic workflow in which you will be pulling from and pushing to your repo on GitHub. Just you, no collaboration: The Connect RStudio to Git and GitHub chapter of Happy Git will get you set up: you will create a repo on GitHub, clone the repo into an RStudio project, and practice making changes. Once you’re set up, your local workflow will be pull, work, commit/push. PULL Each time you open RStudio and switch to the project, you will pull down any changes made to the repo on GitHub by clicking the Down Arrow in the Git pane of RStudio. You may think that no changes have been made to GitHub and there’s nothing to pull, but you may forget the typos that you fixed online, so it’s good practice to always start by pulling changes just in case. WORK Do your stuff. Make changes to files. Add new files. Keep an eye on the Git panel in RStudio; it will show you which files were changes. COMMIT/PUSH When you’re done working, you’ll want to think about what to do with the files that have changed. I like to keep the Git panel clear, so when I’m done I do one of three things with each file: 1) click “Staged” to get it ready for a sendoff to GitHub, 2) delete it if it’s not needed, 3) add it to .gitignore if it’s a file I want to keep locally but not send to GitHub. (Keep in mind that files in .gitignore are not backed up unless you have another backup system.) If I have a file that belongs somewhere else, I will move it there, so the only files left are the ones to send to GitHub. The next step is to click the Commit button and enter a commit message that meaningful describes what was done. Finally clicking the Up Arrow will send the commit to GitHub. It’s not considered good practice to commit too often, but as a beginner, it’s useful to do so to learn how it all works. 5.5 Your repo with branching Once you’ve comfortable with the workflow described above, you’re ready to start branching. If it’s your repo–or you have write access to someone else’s repo–begin by cloning the repo as in the workflow above. For emphasis: if you have write access to someone else’s repo, you should still clone the repo, not fork it. The repo will be referred to as “origin” even though you’re not the owner; origin simply means the repo from which the project was cloned. Next, continue with step 4 below, or follow the steps in these slides, which provide step-by-step detail on creating a branch, doing work on the branch and then submitting a pull request to merge the changes into origin/master. At any point, you can check the remotes (in this case, GitHub repositories) that are linked to your project with git remote -v. If your GitHub username is person1 and you have write access to a repo called finalproject created by person2, your remotes will look like this: $ git remote -v origin https://github.com/person2/finalproject.git(fetch) origin https://github.com/person2/finalproject.git(push) 5.6 1st PR on another repo with branching Step 1: Fork the upstream repo (once) Skip this step if you are syncing with a repo that you have write access to, whether it’s your own or someone else’s. Let’s say you want to contribute to EDAV! Fork our GitHub repo and then on your own GitHub page, you will see a forked EDAV repo under the repositories section. Note, from now on, the term upstream repository refers to the original repo of the project that you forked and the term origin repository refers to the repo that you created or forked on GitHub. From your respective, both upstream and origin are remote repositories. A fork of jtr13/EDAV Step 2: Clone origin and create a local repository (once) A local repository is the repo that resides on your computer. In order to be able to work locally, we need to create a local copy of the remote reposiotry. (For this to work you must first follow the instructions in First things first section.) On your GitHub repo page, copy the url of the origin repo by clicking on the green Code button and then the clipboard icon. It should look like this: https://github.com/jtr13/EDAV.git Then switch to RStudio, and click File -&gt; New Project -&gt; Version Control -&gt; Git. Now you can paste in the url of the origin repo and click Create Project to create a local repository. It is best to choose a location that is outside of other version control systems such as Dropbox to avoid conflicts. Step 3: Configure remote that points to the upstream repository (once) Skip this step if you’re syncing with a repo that you have write access to. The purpose of this step is to specify the location of the upstream repository, that is, the original project, not your copy of it. To complete this step, type in the following at the command line: $ git remote add upstream &lt;upstream repo url&gt; Source: Configuring a remote for a fork Once the upstream remote is added, you will have two remotes: origin and upstream. For example, the remotes for my (jtr13) local forcats repository are: $ git remote -v origin https://github.com/jtr13/forcats.git (fetch) origin https://github.com/jtr13/forcats.git (push) upstream https://github.com/tidyverse/forcats (fetch) upstream https://github.com/tidyverse/forcats (push) (Although four options are listed, that is, fetching or pushing from either remote, as the diagram above indicates, we will only fetch from upsteam and push to origin.) Step 4: Branch With this workflow, all new work is done on a branch, so it’s important to remember to create a new branch before you begin working. Once the work is complete, a pull request is submitted and if all goes well the new code will be merged into the master branch of the project on GitHub. When you’re ready to start working on something new, create a new branch. Do not reuse a merged branch. Each “fix” should get its own branch and be deleted after it’s been merged. To create a branch, click on the button shown below: Give your new branch a meaningful name. For example, if you intend to add a faceting example to the histogram chapter, you might call your branch add_hist_facet. Leave the “Sync branch with remote” box checked. Thereby you will not only create a local branch but also a remote branch on origin, and the local branch will be set up to track the remote branch. In short, they will be linked and git will take note of any changes on the other. Step 5: Work, commit and push When you create a branch following the method in Step 4, you will be automatically switched to the new branch. You can switch branches by clicking on the branch dropdown box to the right of the new branch button. However, be careful doing so. Work that isn’t committed, even if it is saved, doesn’t belong to a branch so it will move with you as you change branches. This makes it easy to accidentally be on the wrong branch. Check that you are in the right place and as you work keep an eye on changed files in the Git pane. Recall that there are three steps to moving saved work from your working directory to GitHub, represented by the git commands: add, commit, and push. In RStudio, to add, you simply click the checkbox for each file you have modified in the “staged” column on the left of the Git pane. To commit, you just click on the commit button under the Git tab. Entering a commit message is mandatory; choose a meaningful description of the code changes. Finally, to push changes to GitHub, click on the push button, which is represented by an upward pointing arrow. You can combine multiple commits into one “push”. It is not considered good practice to commit too enough because all the commits are entered into the commit history and it’s hard to find what you need if you commit your work every five minutes. (As you’re starting out, though, I wouldn’t be concerned about this. It’s more important to use the commands frequently to gain experience.) The Repeated Amend chapter of Happy Git with R describes one approach to dealing with the how-often-should-I-commit dilemma. Step 6: Submit a pull request Now you are able to see the branch you have created on the GitHub page. The next step is to submit a pull request and the process is very similar to the process described in the GitHub only walkthrough, beginning with step 6. Step 7: Merge the pull request If you submitted a PR to another project, you are not the one who will be merging the pull request, so there’s nothing for you to do here. If it is your project, and it is your job to do so, be aware that there are many methods to merge a request. The most direct simple and direct is to merge the PR on GitHub. This method works well for merging fixed typos and the like. If you want to be able to test code, you may want to check out the PR locally, test it, and perhaps even make edits to it before merging. Best practices in this area are evolving. My current recommendation is to use the usethis package, which makes complex tasks very simple. “How to edit a pull request locally” explains how to do so. Another great resource is “Explore and extend a pull request” in Happy Git with R. This chapter describes two official GitHub versions of merging a pull request, as well as a workflow in development using git2r. 5.7 2nd-nth PR on another repo with branching After the first pull request, the process changes a little. We no longer need to fork and clone the repo. What we do need to do though is make sure that our local copy of the repository is up to date with the GitHub version. There is some other cleanup we need to do, so after the first pull request, we’ll replace steps #1 - #3 above with the following: Step 1: Sync How you sync depends on whether you are syncing with your own repo (“origin”) or someone else’s repo (“upstream”). This should be done at the beginning of every work session. Your repo Switch to the master branch (important!), then click the pull button (down arrow) in the Git pane in RStudio. Or you can type the following in the Terminal: $ git checkout master $ git pull There are no reminders that you’re behind, so it’s up to you. Make it a habit. Someone else’s repo If you’re working on someone else’s repo, make sure you’ve configured an upstream remote. Then do the following to update your fork: $ git fetch upstream $ git checkout master $ git merge upstream/master Source: Syncing a fork Note that these commands bring in changes directly from the upstream repo. Step 2: Delete the old branch If your previous pull request was merged, it’s good practice to delete the associated branch since the upstream already contains all the changes you have made. To fully delete a branch you will need to 1) delete the local branch, 2) delete the remote branch, 3) stop tracking the branch: One way to delete the remote branch is to do so on GitHub. Navigate to the closed pull request on the upstream repo. If your branch has been merged, the pull request dialogue will display the following message: “You’re all set—the &lt;branchname&gt; branch can be safely deleted.” Simply click on the Delete branch button next to the message. If you prefer to work in the terminal, you can delete the remote branch with: $ git push origin --delete &lt;branchname&gt; To delete the local branch, switch to the master branch in RStudio and then type the following in the terminal: $ git branch -d branchname Take note that git doesn’t stop tracking the remote branch even though it’s gone in both places! To stop tracking deleted branches use the following: $ git fetch -p Otherwise you will still see the deleted branches listed in RStudio’s Git pane, and they will still appear when you look at all of your branches with $ git branch -a (* = checked out branch) Speaking of which, be aware that the Git pane doesn’t tend to update in real time, so you’ll likely still see branches listed that have been deleted. Be careful not to switch to them, or you will inadvertently recreate them. (Deleted branches have a habit of coming back.) Clicking on master (even though you’re already on master) appears to trigger an update of the dropdown list. If that doesn’t work, switching out of the project and back in will do so, if you want to be sure that the branches you deleted are really gone. Step 3: Update your fork on GitHub Skip this step if there’s no upstream repo. Yes, it’s odd, but once you’ve forked and cloned the project repo, the copy on GitHub becomes fairly irrelevant. However it’s not a bad idea to keep it up to date, if for no other reason than it’s disturbing to see messages like the following in your Git pane: Thankfully, your fork on GitHub can be updated easily by clicking the green up arrow or entering git push in the Terminal. Steps 4-7: See above Now we’re ready to repeat the branch, work, commit, push, submit a pull request workflow. To do so, follow Steps 4-7 above. 5.8 Fixing mistakes Fixing things generally involves returning to an earlier point in git history. To do so we need a way of referring to the point we want to return to. There are multiple ways of referring to the past. Most of the examples below use HEAD (the last commit) or HEAD~1 (the parent of the last commit, a.k.a. the 2nd to last commit). If you are contributing to someone else’s repo and things on your end get hopelessly messed up, the easiest way out is to start over. First be sure to keep a local copy of the files you need in a new folder, then delete your fork on GitHub, delete the local clone, fork again on GitHub, clone again to get a fresh local copy, and add the files you need back into the project. This is a variation of the “Burn it all down” described in Happy Git with R. 5.8.1 Forgot to branch if you didn’t commit anything yet: Just create the new branch and your work will be moved there, as changes in the working directory do not belong to a branch until they are committed. if you committed but didn’t push to GitHub: Undo the last commit with git reset --soft HEAD~1 (i.e. return to the parent commit). --soft means your changes won’t be erased. Then create the new branch. (Your work will move to it, see above.) if you committed and pushed to GitHub: Undo the last commit with git reset --soft HEAD~1, push to GitHub (you will need to use git push --force since the push will cause origin to lose commits, then create the new branch. (Your work will move to it, see above.) Note: Be very, very careful with --force as it is a dangerous tool. However, since the stakes are lower if you are force pushing to a fork, as would be the case if you are contributing to someone else’s repo, it’s ok to use it in this particular case. 5.8.2 Undoing stuff Undo the last commit: git reset --soft HEAD~1 (Fun fact: “How do I undo the most recent local commits in Git?” has the second highest number of votes of any question on Stack Overflow, and over 8 million views.) Undo changes since the last commit: git reset --hard HEAD Undo changes in one file since the last commit:git checkout -- [filename] SO link Undo deleted branch: Look for the SHA (hash) returned when you deleted the branch. Then:git checkout -b &lt;branch-name&gt; &lt;SHA&gt; SO link Remove all new, untracked files: git clean -f Remove all new, untracked files, including in new subdirectories: git clean -f -d 5.8.3 Random Make local the same as origin:git fetch origingit reset --hard origin/master SO link Add back a file that was deleted but still exists on another branch:git checkout otherbranch myfile.txt SO link Get rid of a file on GitHub that was added to .gitignore but is still there:git rm --cached [filename](then commit and push changes to GitHub) SO link Completely remove a folder from git history (use with caution) SO link Slides from Karthik Ram’s workshop, Getting Unstuck from Git, offer clear ways out of git problems, expressed in English (such as “I royally screwed up the last several commits. I just want life to be good again”) , not git speak. An excellent resource! 5.9 Troubleshooting 5.9.1 Deleting a branch isn’t working Make sure you’re not on the branch you’re trying to delete. Note that if you try to delete a branch that hasn’t been fully merged, you’ll get a warning, or perhaps an error depending on what’s transpired. It’s possible that it just thinks it hasn’t been merged even though it has, since you’re not up to date. This can be remedied with git pull. In other cases, you’ll need to follow the instructions to use -D instead of -d, for example, if you decide to abandon and delete a branch without submitting a pull request. If you have trouble getting rid of branches, rest assured, that you’re not alone. How do I delete a Git branch locally and remotely? is the third most asked question on StackOverflow! 5.10 Other resources Getting Help If you’re lost, these might help. GitHub Guides: This is a phenomenal collection of short articles from GitHub to help you learn about the fundamentals around their product. They are so great, we have already listed their Hello World article. Here are some other important ones: Understanding the GitHub Flow: Explains how working with GitHub generally goes. Git Handbook: Explains what version control is. GitHub Help: This is the yellow-pages of GitHub. Ask a question and it will try to push you in the right direction. Get it? Branching out GitHub is super social. Learn how to git involved! Open Source Guide: Info on how to contribute to open source projects. Great links to the GitHub skills involved as well as good GitHub etiquette to adopt. Forking Projects: Quick read from GitHub on how to fork a repository so you can contribute to it. Mastering Issues: On what Issues are in GitHub and how they can help get things done. Our Page on Contributing: You can contribute to edav.info/ with your new-found GitHub skills! Checkout our page on how to contribute through pull requests and/or issues. More Resources To hit the ground running, checkout GitHub Learning Lab. This application will teach you how to use GitHub with hands-on courses using actual repos. Its the perfect way to understand what using GitHub looks like. For the nerds in the room… Git For Ages 4 And Up: There’s a lot going on under the hood. This talk will help explain how it all works…with kids toys! Make pretty git logs: Always remember (A DOG). Also, this alias command is nice to have around: git config --global alias.adog \"log --all --decorate --oneline --graph\" add and commit with one command: Another (even more) helpful alias command: git config --global alias.add-commit '!git add -A &amp;&amp; git commit' Git Aware Prompt: An excellent add-on to the Terminal that informs you which branch you have checked out. Someone also made an even spiffier version where it will inform you of your git status using helpful emojis. Contributing with git2r, on the Population Genetics in R provides helpful information on using git commands within R through the git2r package. In particular it explains how to create a GITHUB_PAT and then set the credential parameter in certain functions to find the PAT. (Note though that the site was created in 2015 and as of February 2019 has not been updated.) Want a little reading as well?: Resources to learn Git is a simple site split into two main sections: Learn by reading and Learn by doing. Take your pick. A Newbie’s Guide to Making A Pull Request (for an R package), Tony Elhabr’s experience submitting a pull request to an R package at tidyverse developer day (part of rstudio::conf 2019. "],["histo.html", "6 Chart: Histogram 6.1 Overview 6.2 tl;dr 6.3 Simple examples 6.4 Theory 6.5 Types of histograms 6.6 Parameters 6.7 Interactive histograms with ggvis 6.8 External resources", " 6 Chart: Histogram 6.1 Overview This section covers how to make histograms. 6.2 tl;dr Gimme a full-fledged example! Here’s an application of histograms that looks at how the beaks of Galapagos finches changed due to external factors: And here’s the code: library(Sleuth3) # data library(ggplot2) # plotting # load data finches &lt;- Sleuth3::case0201 # finch histograms by year with overlayed density curves ggplot(finches, aes(x = Depth, y = ..density..)) + # plotting geom_histogram(bins = 20, colour = &quot;#80593D&quot;, fill = &quot;#9FC29F&quot;, boundary = 0) + geom_density(color = &quot;#3D6480&quot;) + facet_wrap(~Year) + # formatting ggtitle(&quot;Severe Drought Led to Finches with Bigger Chompers&quot;, subtitle = &quot;Beak Depth Density of Galapagos Finches by Year&quot;) + labs(x = &quot;Beak Depth (mm)&quot;, caption = &quot;Source: Sleuth3::case0201&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?Sleuth3::case0201 into the console. 6.3 Simple examples Whoa whoa whoa! Much simpler please! Let’s use a very simple dataset: # store data x &lt;- c(50, 51, 53, 55, 56, 60, 65, 65, 68) 6.3.1 Histogram using base R # plot data hist(x, col = &quot;lightblue&quot;, main = &quot;Base R Histogram of x&quot;) For the Base R histogram, it’s advantages are in it’s ease to setup. In truth, all you need to plot the data x in question is hist(x), but we included a little color and a title to make it more presentable. Full documentation on hist() can be found here 6.3.2 Histogram using ggplot2 # import ggplot library(ggplot2) # must store data as dataframe df &lt;- data.frame(x) # plot data ggplot(df, aes(x)) + geom_histogram(color = &quot;grey&quot;, fill = &quot;lightBlue&quot;, binwidth = 5, center = 52.5) + ggtitle(&quot;ggplot2 histogram of x&quot;) The ggplot version is a little more complicated on the surface, but you get more power and control as a result. Note: as shown above, ggplot expects a dataframe, so if you are getting an error where “R doesn’t know what to do” like this: ggplot dataframe error make sure you are using a dataframe. 6.4 Theory Generally speaking, the histogram is one of many options for displaying continuous data. The histogram is clear and quick to make. Histograms are relatively self-explanatory: they show your data’s empirical distribution within a set of intervals. Histograms can be employed on raw data to quickly show the distribution without much manipulation. Use a histogram to get a basic sense of the distribution with minimal processing necessary. For more info about histograms and continuous variables, check out Chapter 3 of the textbook. 6.5 Types of histograms Use a histogram to show the distribution of one continuous variable. The y-scale can be represented in a variety of ways to express different results: 6.5.1 Frequency or count y = number of values that fall in each bin 6.5.2 Relative frequency historgram y = number of values that fall in each bin / total number of values 6.5.3 Cumulative frequency histogram y = total number of values &lt;= (or &lt;) right boundary of bin 6.5.4 Density y = relative frequency / binwidth 6.6 Parameters 6.6.1 Bin boundaries Be mindful of the boundaries of the bins and whether a point will fall into the left or right bin if it is on a boundary. # format layout op &lt;- par(mfrow = c(1, 2), las = 1) # right closed hist(x, col = &quot;lightblue&quot;, ylim = c(0, 4), xlab = &quot;right closed ex. (55, 60]&quot;, font.lab = 2) # right open hist(x, col = &quot;lightblue&quot;, right = FALSE, ylim = c(0, 4), xlab = &quot;right open ex. [55, 60)&quot;, font.lab = 2) 6.6.2 Bin number The default bin number of 30 in ggplot2 is not always ideal, so consider altering it if things are looking strange. You can specify the width explicitly with binwidth or provide the desired number of bins with bins. # default...note the pop-up about default bin number ggplot(finches, aes(x = Depth)) + geom_histogram() + ggtitle(&quot;Default with pop-up about bin number&quot;) Here are examples of changing the bins using the two ways described above: # using binwidth p1 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(binwidth = 0.5, boundary = 6) + ggtitle(&quot;Changed binwidth value&quot;) # using bins p2 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(bins = 48, boundary = 6) + ggtitle(&quot;Changed bins value&quot;) # format plot layout library(gridExtra) grid.arrange(p1, p2, ncol = 2) 6.6.3 Bin alignment Make sure the axes reflect the true boundaries of the histogram. You can use boundary to specify the endpoint of any bin or center to specify the center of any bin. ggplot2 will be able to calculate where to place the rest of the bins (Also, notice that when the boundary was changed, the number of bins got smaller by one. This is because by default the bins are centered and go over/under the range of the data.) df &lt;- data.frame(x) # default alignment ggplot(df, aes(x)) + geom_histogram(binwidth = 5, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Default Bin Alignment&quot;) # specify alignment with boundary p3 &lt;- ggplot(df, aes(x)) + geom_histogram(binwidth = 5, boundary = 60, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Bin Alignment Using boundary&quot;) # specify alignment with center p4 &lt;- ggplot(df, aes(x)) + geom_histogram(binwidth = 5, center = 67.5, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Bin Alignment Using center&quot;) # format layout library(gridExtra) grid.arrange(p3, p4, ncol = 2) Note: Don’t use both boundary and center for bin alignment. Just pick one. 6.7 Interactive histograms with ggvis The ggvis package is not currently in development, but does certain things very well, such as adjusting parameters of a histogram interactively while coding. Since images cannot be shared by knitting (as with other packages, such as plotly), we present the code here, but not the output. To try them out, copy and paste into an R session. 6.7.1 Change binwidth interactively library(tidyverse) library(ggvis) faithful %&gt;% ggvis(~eruptions) %&gt;% layer_histograms(fill := &quot;lightblue&quot;, width = input_slider(0.1, 2, value = .1, step = .1, label = &quot;width&quot;)) 6.7.2 GDP example df &lt;-read.csv(&quot;countries2012.csv&quot;) df %&gt;% ggvis(~GDP) %&gt;% layer_histograms(fill := &quot;green&quot;, width = input_slider(500, 10000, value = 5000, step = 500, label = &quot;width&quot;)) 6.7.3 Change center interactively df &lt;- data.frame(x = c(50, 51, 53, 55, 56, 60, 65, 65, 68)) df %&gt;% ggvis(~x) %&gt;% layer_histograms(fill := &quot;red&quot;, width = input_slider(1, 10, value = 5, step = 1, label = &quot;width&quot;), center = input_slider(50, 55, value = 52.5, step = .5, label = &quot;center&quot;)) 6.7.4 Change center (with data values shown) df &lt;- data.frame(x = c(50, 51, 53, 55, 56, 60, 65, 65, 68), y = c(.5, .5, .5, .5, .5, .5, .5, 1.5, .5)) df %&gt;% ggvis(~x, ~y) %&gt;% layer_histograms(fill := &quot;lightcyan&quot;, width = 5, center = input_slider(45, 55, value = 45, step = 1, label = &quot;center&quot;)) %&gt;% layer_points(fill := &quot;blue&quot;, size := 200) %&gt;% add_axis(&quot;x&quot;, properties = axis_props(labels = list(fontSize = 20))) %&gt;% scale_numeric(&quot;x&quot;, domain = c(46, 72)) %&gt;% add_axis(&quot;y&quot;, values = 0:3, properties = axis_props(labels = list(fontSize = 20))) 6.7.5 Change boundary interactively df %&gt;% ggvis(~x) %&gt;% layer_histograms(fill := &quot;red&quot;, width = input_slider(1, 10, value = 5, step = 1, label = &quot;width&quot;), boundary = input_slider(47.5, 50, value = 50, step = .5, label = &quot;boundary&quot;)) 6.8 External resources hist documentation: base R histogram documentation page. ggplot2 cheatsheet: Always good to have close by. "],["box.html", "7 Chart: Boxplot 7.1 tl;dr 7.2 Simple examples 7.3 Theory 7.4 When to use 7.5 Considerations 7.6 External resources", " 7 Chart: Boxplot 7.1 tl;dr I want a nice example and I want it NOW! Here’s a look at the weights of newborn chicks split by the feed supplement they received: And here’s the code: library(ggplot2) # boxplot by feed supplement ggplot(chickwts, aes(x = reorder(feed, -weight, median), y = weight)) + # plotting geom_boxplot(fill = &quot;#cc9a38&quot;, color = &quot;#473e2c&quot;) + # formatting ggtitle(&quot;Casein Makes You Fat?!&quot;, subtitle = &quot;Boxplots of Chick Weights by Feed Supplement&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;, caption = &quot;Source: datasets::chickwts&quot;) + theme_grey(16) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?datasets::chickwts into the console. 7.2 Simple examples Okay…much simpler please. 7.2.1 Single boxplots Base R will give you a quick boxplot of a vector or a single column of a data frame with very little typing: # vector boxplot(rivers) Or, the horizontal version: # single column of a data frame boxplot(chickwts$weight, horizontal = TRUE) Creating a single boxplot in ggplot2 is somewhat problematic. (The joke is that it’s the package author’s way of saying that if you only have one group, make a histogram instead!) If you only include one aesthetic mapping, it will be assumed to the x (group) variable and you will get an error: ggplot(chickwts, aes(weight)) + geom_boxplot() Error: stat_boxplot requires the following missing aesthetics: y This can be remedied by adding y = to indicate that weight is the numeric variable, but you’ll still get a meaningless x-axis: ggplot(chickwts, aes(y = weight)) + geom_boxplot() + theme_grey(16) # make all font sizes larger (default is 11) Another, cleaner approach is to create a name for the single group as the x aesthetic and remove the x-axis label: ggplot(chickwts, aes(x = &quot;all 71 chickens&quot;, y = weight)) + geom_boxplot() + xlab(&quot;&quot;) + theme_grey(16) 7.2.2 Multiple boxplots using ggplot2 To create multiple boxplots with ggplot2, your data frame needs to be tidy, that is you need to have a column with levels of the grouping variable. It can be be factor, character, or integer class. str(chickwts) ## &#39;data.frame&#39;: 71 obs. of 2 variables: ## $ weight: num 179 160 136 227 217 168 108 124 143 140 ... ## $ feed : Factor w/ 6 levels &quot;casein&quot;,&quot;horsebean&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... We see that chickwts is in the right form: we have a feed column with six factor levels, so we can set the the x aesthetic to feed. We also order the boxplots by decreasing median weight: ggplot(chickwts, aes(x = reorder(feed, -weight, median), y = weight)) + geom_boxplot() + xlab(&quot;feed type&quot;) + theme_grey(16) Data frames that contain a separate column of values for each desired boxplot must be tidied first. (For more detail on using tidy::gather(), see this tutorial.) library(tidyverse) head(attitude) ## rating complaints privileges learning raises critical advance ## 1 43 51 30 39 61 92 45 ## 2 63 64 51 54 63 73 47 ## 3 71 70 68 69 76 86 48 ## 4 61 63 45 47 54 84 35 ## 5 81 78 56 66 71 83 47 ## 6 43 55 49 44 54 49 34 tidyattitude &lt;- attitude %&gt;% gather(key = &quot;question&quot;, value = &quot;rating&quot;) head(tidyattitude) ## question rating ## 1 complaints 51 ## 2 complaints 64 ## 3 complaints 70 ## 4 complaints 63 ## 5 complaints 78 ## 6 complaints 55 Now we’re ready to plot: ggplot(tidyattitude, aes(reorder(question, -rating, median), rating)) + geom_boxplot() + xlab(&quot;question short name&quot;) + theme_grey(16) 7.3 Theory Here’s a quote by Hadley Wickham that sums up boxplots nicely: The boxplot is a compact distributional summary, displaying less detail than a histogram or kernel density, but also taking up less space. Boxplots use robust summary statistics that are always located at actual data points, are quickly computable (originally by hand), and have no tuning parameters. They are particularly useful for comparing distributions across groups. - Hadley Wickham Another important use of the boxplot is in showing outliers. A boxplot shows how much of an outlier a data point is with quartiles and fences. Use the boxplot when you have data with outliers so that they can be exposed. What it lacks in specificity it makes up with its ability to clearly summarize large data sets. For more info about boxplots and continuous variables, check out Chapter 3 of the textbook. 7.4 When to use Boxplots should be used to display continuous variables. They are particularly useful for identifying outliers and comparing different groups. Aside: Boxplots may even help you convince someone you are their outlier (If you like it when people over-explain jokes, here is why that comic is funny.). 7.5 Considerations 7.5.1 Flipping orientation Often you want boxplots to be horizontal. Super easy to do in ggplot2: just tack on + coord_flip() and remove the - from the reordering so that the factor level with the highest median will be on top: ggplot(tidyattitude, aes(reorder(question, rating, median), rating)) + geom_boxplot() + coord_flip() + xlab(&quot;question short name&quot;) + theme_grey(16) Note that switching x and y insteading of using coord_flip() doesn’t work! ggplot(tidyattitude, aes(rating, reorder(question, rating, median))) + geom_boxplot() + ggtitle(&quot;This is not what we wanted!&quot;) + ylab(&quot;question short name&quot;) + theme_grey(16) 7.5.2 NOT for categorical data Boxplots are great, but they do NOT work with categorical data. Make sure your variable is continuous before using boxplots. The data in this example are variables from the pisaitems dataset in the likert package with ratings of 1, 2, 3 or 4: head(pisa, 4) ## ST24Q01 ST24Q02 ST24Q03 ST24Q04 ST24Q05 ST24Q06 ## 1 2 4 4 1 4 1 ## 2 3 1 1 4 1 3 ## 3 4 1 1 3 1 4 ## 4 2 2 3 1 2 2 Creating a boxplot from this data is a good example of what not to do: 7.6 External resources Tukey, John W. 1977. Exploratory Data Analysis. Addison-Wesley. (Chapter 2): the primary source in which boxplots are first presented. Article on boxplots with ggplot2: An excellent collection of code examples on how to make boxplots with ggplot2. Covers layering, working with legends, faceting, formatting, and more. If you want a boxplot to look a certain way, this article will help. Boxplots with plotly package: boxplot examples using the plotly package. These allow for a little interactivity on hover, which might better explain the underlying statistics of your plot. ggplot2 Boxplot: Quick Start Guide: Article from STHDA on making boxplots using ggplot2. Excellent starting point for getting immediate results and custom formatting. ggplot2 cheatsheet: Always good to have close by. Hadley Wickhan and Lisa Stryjewski on boxplots: good for understanding basics of more complex boxplots and some of the history behind them. "],["violin.html", "8 Chart: Violin Plot 8.1 Overview 8.2 Some Examples in R 8.3 Adding Statistics to the Violin Plot 8.4 Description 8.5 When to use 8.6 External Resources", " 8 Chart: Violin Plot This chapter originated as a community contribution created by AshwinJay101 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 8.1 Overview This section covers how to make violin plots. 8.2 Some Examples in R Let’s use the chickwts dataset from the datasets package to plot a violin plot using ggplot2. Here’s the code for that: # import ggplot and the Datasets Package library(datasets) library(ggplot2) supps &lt;- c(&quot;horsebean&quot;, &quot;linseed&quot;, &quot;soybean&quot;, &quot;meatmeal&quot;, &quot;sunflower&quot;, &quot;casein&quot;) # plot data ggplot(chickwts, aes(x = factor(feed, levels = supps), y = weight)) + # plotting geom_violin(fill = &quot;lightBlue&quot;, color = &quot;#473e2c&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;) 8.3 Adding Statistics to the Violin Plot 8.3.1 Adding the median and the interquartile range We can add the median and the interquartile range to the violin plot ggplot(chickwts, aes(x = factor(feed, levels = supps), y = weight)) + # plotting geom_violin(fill = &quot;lightBlue&quot;, color = &quot;#473e2c&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;) + geom_boxplot(width=0.1) To get the result, we just add a boxplot geom. 8.3.2 Displaying data as dots ggplot(chickwts, aes(x = factor(feed, levels = supps), y = weight)) + # plotting geom_violin(fill = &quot;lightBlue&quot;, color = &quot;#473e2c&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;) + geom_dotplot(binaxis=&#39;y&#39;, dotsize=0.5, stackdir=&#39;center&#39;) 8.4 Description Violin plots are similar to box plots. The advantage they have over box plots is that they allow us to visualize the distribution of the data and the probability density. We can think of violin plots as a combination of boxplots and density plots. This plot type allows us to see whether the data is unimodal, bimodal or multimodal. These simple details will be hidden in the boxplot. The distribution can be seen through the width of the violin plot. 8.5 When to use Violin plots should be used to display continuous variables only. 8.6 External Resources ggplot2 Violin Plot: Excellent resource for showing the various customizations that can be added to the violin plot. "],["ridgeline.html", "9 Chart: Ridgeline Plots 9.1 Overview 9.2 tl;dr 9.3 Simple examples 9.4 Ridgeline Plots using ggridge 9.5 When to Use 9.6 Considerations 9.7 External Resources", " 9 Chart: Ridgeline Plots This chapter originated as a community contribution created by nehasaraf1994 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 9.1 Overview This section covers how to make ridgeline plots. 9.2 tl;dr I want a nice example and I want it NOW! Here’s a look at the dose of theophylline administered orally to the subject on which the concentration of theophylline is observed: Here is the code: library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) Theoph_data &lt;- Theoph ggplot(Theoph_data, aes(x=Dose,y=Subject,fill=Subject))+ geom_density_ridges_gradient(scale = 4, show.legend = FALSE) + theme_ridges() + scale_y_discrete(expand = c(0.01, 0)) + scale_x_continuous(expand = c(0.01, 0)) + labs(x = &quot;Dose of theophylline(mg/kg)&quot;,y = &quot;Subject #&quot;) + ggtitle(&quot;Density estimation of dosage given to various subjects&quot;) + theme(plot.title = element_text(hjust = 0.5)) For more info on this dataset, type ?datasets::Theoph into the console. 9.3 Simple examples Okay…much simpler please. Let’s use the Orange dataset from the datasets package: library(&quot;datasets&quot;) head(Orange, n=5) ## Grouped Data: circumference ~ age | Tree ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 9.4 Ridgeline Plots using ggridge library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) ggplot(Orange, aes(x=circumference,y=Tree,fill = Tree))+ geom_density_ridges(scale = 2, alpha=0.5) + theme_ridges()+ scale_fill_brewer(palette = 4)+ scale_y_discrete(expand = c(0.8, 0)) + scale_x_continuous(expand = c(0.01, 0))+ labs(x=&quot;Circumference at Breast Height&quot;, y=&quot;Tree with ordering of max diameter&quot;)+ ggtitle(&quot;Density estimation of circumference of different types of Trees&quot;)+ theme(plot.title = element_text(hjust = 0.5)) ggridge uses two main geoms to plot the ridgeline density plots: “geom_density_ridges” and “geom_ridgeline”. They are used to plot the densities of categorical variable factors and see their distribution over a continuous scale. 9.5 When to Use Ridgeline plots can be used when a number of data segments have to be plotted on the same horizontal scale. It is presented with slight overlap. Ridgeline plots are very useful to visualize the distribution of a categorical variable over time or space. A good example using ridgeline plots will be a great example is visualizing the distribution of salary over different departments in a company. 9.6 Considerations The overlapping of the density plot can be controlled by adjusting the value of scale. Scale defines how much the peak of the lower curve touches the curve above. library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) OrchardSprays_data &lt;- OrchardSprays ggplot(OrchardSprays_data, aes(x=decrease,y=treatment,fill=treatment))+ geom_density_ridges_gradient(scale=3) + theme_ridges()+ scale_y_discrete(expand = c(0.3, 0)) + scale_x_continuous(expand = c(0.01, 0))+ labs(x=&quot;Response in repelling honeybees&quot;,y=&quot;Treatment&quot;)+ ggtitle(&quot;Density estimation of response by honeybees to a treatment for scale=3&quot;)+ theme(plot.title = element_text(hjust = 0.5)) ggplot(OrchardSprays_data, aes(x=decrease,y=treatment,fill=treatment))+ geom_density_ridges_gradient(scale=5) + theme_ridges()+ scale_y_discrete(expand = c(0.3, 0)) + scale_x_continuous(expand = c(0.01, 0))+ labs(x=&quot;Response in repelling honeybees&quot;,y=&quot;Treatment&quot;)+ ggtitle(&quot;Density estimation of response by honeybees to a treatment for scale=5&quot;)+ theme(plot.title = element_text(hjust = 0.5)) Ridgeline plots can also be used to plot histograms on the common horizontal axis rather than density plots. But doing that may not give us any valuable results. library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) ggplot(InsectSprays, aes(x = count, y = spray, height = ..density.., fill = spray)) + geom_density_ridges(stat = &quot;binline&quot;, bins = 20, scale = 0.7, draw_baseline = FALSE) If the same thing is done in ridgeline plots, it gives better results. library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) ggplot(InsectSprays, aes(x=count,y=spray,fill=spray))+ geom_density_ridges_gradient() + theme_ridges()+ labs(x=&quot;Count of Insects&quot;,y=&quot;Types of Spray&quot;)+ ggtitle(&quot;The counts of insects treated with different insecticides.&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 9.7 External Resources Introduction to ggridges: An excellent collection of code examples on how to make ridgeline plots with ggplot2. Covers every parameter of ggridges and how to modify them for better visualization. If you want a ridgeline plot to look a certain way, this article will help. Article on ridgeline plots with ggplot2: Few examples using different examples. Great for starting with ridgeline plots. History of Ridgeline plots: To refer to the theory of ridgeline plots. "],["qqplot.html", "10 Chart: QQ-Plot 10.1 Introduction 10.2 Interpreting qqplots 10.3 Normal or not (examples using qqnorm) 10.4 Different kinds of qqplots 10.5 qqplot using ggplot 10.6 References", " 10 Chart: QQ-Plot This chapter originated as a community contribution created by hao871563506 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 10.1 Introduction In statistics, a Q-Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile. 10.2 Interpreting qqplots 10.3 Normal or not (examples using qqnorm) 10.3.1 Normal qqplot x &lt;- rnorm(1000, 50, 10) qqnorm(x) qqline(x, col = &quot;red&quot;) The points seem to fall along a straight line. Notice the x-axis plots the theoretical quantiles. Those are the quantiles from the standard Normal distribution with mean 0 and standard deviation 1. 10.3.2 Non-normal qqplot x &lt;- rexp(1000, 5) qqnorm(x) qqline(x, col = &quot;red&quot;) Notice the points form a curve instead of a straight line. Normal Q-Q plots that look like this usually mean your sample data are skewed. 10.4 Different kinds of qqplots The following graph is a conclusion of all the kinds of qqplot: via Stack Exchange Normal qqplot: The normal distribution is symmetric, so it has no skew (the mean is equal to the median). Right skewed qqplot: Right-skew is also known as positive skew. Left skewed qqplot: Left-skew is also known as negative skew. Light tailed qqplot: meaning that compared to the normal distribution there is little more data located at the extremes of the distribution and less data in the center of the distribution. Heavy tailed qqplot: meaning that compared to the normal distribution there is much more data located at the extremes of the distribution and less data in the center of the distribution. Biomodel qqplot: illustrate a bimodal distribution. 10.5 qqplot using ggplot In order to use ggplot2 to plot a qqplot, we must use a dataframe, so here we convert it to one. We can see that using ggplot to plot a qqplot has a similar outcome as using qqnorm library(ggplot2) x &lt;- rnorm(1000, 50, 10) x &lt;- data.frame(x) ggplot(x, aes(sample = x)) + stat_qq() + stat_qq_line() However, when we need to plot different groups, ggplot will be very helpful with its coloring by factor. library(ggplot2) ggplot(mtcars, aes(sample = mpg, colour = factor(cyl))) + stat_qq() + stat_qq_line() 10.6 References Understanding Q-Q Plots: A discussion from the University of Virginia Library on qqplots. How to interpret a QQ plot: Another resource for interpreting qqplots. A QQ Plot Dissection Kit: An excellent walkthrough on qqplots by Sean Kross. Probability plotting methods for the analysis of data: Paper on plotting techniques, which discusses qqplots. (Wilk, M.B.; Gnanadesikan, R. (1968)) QQ-Plot Wiki: Wikipedia entry on qqplots "],["bar.html", "11 Chart: Bar Chart 11.1 Overview 11.2 tl;dr 11.3 Simple examples 11.4 Theory 11.5 When to use 11.6 Considerations 11.7 Modifications 11.8 External resources", " 11 Chart: Bar Chart 11.1 Overview This section covers how to make bar charts 11.2 tl;dr I want a nice example. Not tomorrow, not after breakfast. NOW! Here’s a bar chart showing the survival rates of passengers aboard the RMS Titanic: And here’s the code: library(datasets) # data library(ggplot2) # plotting library(dplyr) # manipulation # Combine Children and Adult stats together ship_grouped &lt;- as.data.frame(Titanic) %&gt;% group_by(Class, Sex, Survived) %&gt;% summarise(Total = sum(Freq)) ggplot(ship_grouped, aes(x = Survived, y = Total, fill = Sex)) + geom_bar(position = &quot;dodge&quot;, stat = &quot;identity&quot;) + geom_text(aes(label = Total), position = position_dodge(width = 0.9), vjust = -0.4, color = &quot;grey68&quot;) + facet_wrap(~Class) + # formatting ylim(0, 750) + ggtitle(&quot;Don&#39;t Be A Crew Member On The Titanic&quot;, subtitle = &quot;Survival Rates of Titanic Passengers by Class and Gender&quot;) + scale_fill_manual(values = c(&quot;#b2df8a&quot;, &quot;#a6cee3&quot;)) + labs(y = &quot;Passenger Count&quot;, caption = &quot;Source: titanic::titanic_train&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?datasets::Titanic into the console. 11.3 Simple examples My eyes were bigger than my stomach. Much simpler please! Let’s use the HairEyeColor dataset. To start, we will just look at the different categories of hair color among females: colors &lt;- as.data.frame(HairEyeColor) # just female hair color, using dplyr colors_female_hair &lt;- colors %&gt;% filter(Sex == &quot;Female&quot;) %&gt;% group_by(Hair) %&gt;% summarise(Total = sum(Freq)) # take a look at data head(colors_female_hair) ## # A tibble: 4 x 2 ## Hair Total ## &lt;fct&gt; &lt;dbl&gt; ## 1 Black 52 ## 2 Brown 143 ## 3 Red 37 ## 4 Blond 81 Now let’s make some graphs with this data. 11.3.1 Bar graph using base R barplot(colors_female_hair[[&quot;Total&quot;]], names.arg = colors_female_hair[[&quot;Hair&quot;]], main = &quot;Bar Graph Using Base R&quot;) We recommend using Base R only for simple bar graphs for yourself. Like all of Base R, it is simple to setup. Note: Base R expects a vector or matrix, hence the double brackets in the barplot call (gets columns as lists). 11.3.2 Bar graph using ggplot2 library(ggplot2) # plotting ggplot(colors_female_hair, aes(x = Hair, y = Total)) + geom_bar(stat = &quot;identity&quot;) + ggtitle(&quot;Bar Graph Using ggplot2&quot;) Bar plots are very easy in ggplot2. You pass in a dataframe and let it know which parts you want to map to different aesthetics. Note: In this case, we have a table of values and want to plot them as explicit bar heights. Because of this, we specify the y aesthetic as the Total column, but we also have to specify stat = \"identity\" in geom_bar() so it knows to plot them correctly. Often you will have datasets where each row is one observation and you want to group them into bars. In that case, the y aesthetic and stat = \"identity\" do not have to be specified. 11.4 Theory For more info about plotting categorical data, check out Chapter 4 of the textbook. 11.5 When to use Bar Charts are best for categorical data. Often you will have a collection of factors that you want to split into different groups. 11.6 Considerations 11.6.1 Not for continuous data If you are finding that your bar graphs aren’t looking right, make sure your data is categorical and not continuous. If you want to plot continuous data using bars, that is what histograms are for! 11.7 Modifications These modifications assume you are using ggplot2. 11.7.1 Flip Bars To flip the orientation, just tack on coord_flip(): ggplot(colors_female_hair, aes(x = Hair, y = Total)) + geom_bar(stat = &quot;identity&quot;) + ggtitle(&quot;Bar Graph Using ggplot2&quot;) + coord_flip() 11.7.2 Reorder the bars With both base R and ggplot2 bars are drawn in alphabetical order for character data and in the order of factor levels for factor data. However, since the default order of levels for factor data is alphabetical, the bars will be alphabetical in both cases. Please see this tutorial for a detailed explanation on how bars should be ordered in a bar chart, and how the forcats package can help you accomplish the reordering. 11.7.3 Facet Wrap You can split the graph into small multiples using facet_wrap() (don’t forget the tilde, ~): ggplot(colors, aes(x = Sex, y = Freq)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~Hair) 11.8 External resources Cookbook for R: Discussion on reordering the levels of a factor. DataCamp Exercise: Simple exercise on making bar graphs with ggplot2. ggplot2 cheatsheet: Always good to have close by. "],["cleveland.html", "12 Chart: Cleveland Dot Plot 12.1 Overview 12.2 Multiple dots", " 12 Chart: Cleveland Dot Plot This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 12.1 Overview This section covers how to make Cleveland dot plots. Cleveland dot plots are a great alternative to a simple bar chart, particularly if you have more than a few items. It doesn’t take much for a bar chart to look cluttered. In the same amount of space, many more values can be included in a dot plot, and it’s easier to read as well. R has a built-in base function, dotchart(), but since it’s such an easy graph to draw, doing it “from scratch” in ggplot2 or base allows for more customization. The code: library(tidyverse) # create a theme for dot plots, which can be reused theme_dotplot &lt;- theme_bw(14) + theme(axis.text.y = element_text(size = rel(.75)), axis.ticks.y = element_blank(), axis.title.x = element_text(size = rel(.75)), panel.grid.major.x = element_blank(), panel.grid.major.y = element_line(size = 0.5), panel.grid.minor.x = element_blank()) # move row names to a dataframe column df &lt;- swiss %&gt;% tibble::rownames_to_column(&quot;Province&quot;) # create the plot ggplot(df, aes(x = Fertility, y = reorder(Province, Fertility))) + geom_point(color = &quot;blue&quot;) + scale_x_continuous(limits = c(35, 95), breaks = seq(40, 90, 10)) + theme_dotplot + xlab(&quot;\\nannual live births per 1,000 women aged 15-44&quot;) + ylab(&quot;French-speaking provinces\\n&quot;) + ggtitle(&quot;Standardized Fertility Measure\\nSwitzerland, 1888&quot;) 12.2 Multiple dots For this example we’ll use 2010 data on SAT mean scores for a sample of New York City public schools: df &lt;- read_csv(&quot;data/SAT2010.csv&quot;, na = &quot;s&quot;) set.seed(5293) tidydf &lt;- df %&gt;% filter(!is.na(`Critical Reading Mean`)) %&gt;% sample_n(20) %&gt;% rename(Reading = &quot;Critical Reading Mean&quot;, Math = &quot;Mathematics Mean&quot;, Writing = &quot;Writing Mean&quot;) %&gt;% gather(key = &quot;Test&quot;, value = &quot;Mean&quot;, &quot;Reading&quot;, &quot;Math&quot;, &quot;Writing&quot;) ggplot(tidydf, aes(Mean, `School Name`, color = Test)) + geom_point() + ggtitle(&quot;Schools are sorted alphabetically&quot;, sub = &quot;not the best option&quot;) + ylab(&quot;&quot;) + theme_dotplot Note that School Name is sorted by factor level, which by default is alphabetical. A better choice is to sort by one of the levels of Test. It’s usually best to try sorting on different factor levels and observe the patterns that appear. To perform the double sort, that is, sorting School Name by Test and then Mean, we use forcats::fct_reorder2(). This function sorts .f (a factor or character vector) by two sorting vectors, .x and .y. For this type of plot, .x is the variable represented by the colored dots and .y is the continuous variable mapped to the y-axis. Suppose we wish to sort the schools by mean reading score. We can do this by limiting the Test variable to “Reading” when sorting on Mean: ggplot(tidydf, aes(Mean, fct_reorder2(`School Name`, Test==&quot;Reading&quot;, Mean, .desc = FALSE), color = Test)) + geom_point() + ggtitle(&quot;Schools sorted by Reading mean&quot;) + ylab(&quot;&quot;) + theme_dotplot (Many thanks to Zeyu Qiu for the tip on setting .x directly to the factor level, a much better approach than reordering factor levels to conform with fct_reorder2() defaults, as discussed below.) While this is the go-to method, there may be cases in which it’s easier to specify that you wish to sort by the first or the last factor level of the first sorting variable (Test), without spelling it out. If a factor level is not specified, fct_reorder2() by default will sort on the last factor level of .x. In this case, “Writing” is the last factor level of Test: ggplot(tidydf, aes(Mean, fct_reorder2(`School Name`, Test, Mean, .desc = FALSE), color = Test)) + geom_point() + ggtitle(&quot;Schools sorted by Writing mean&quot;) + ylab(&quot;&quot;) + theme_dotplot If you desire to sort by the first factor level of .x, “Math” in this case, you’ll need the development version of forcats, which you can install with: devtools::install_github(\"tidyverse/forcats\") Change the default sorting function, last2(), to first2(): ggplot(tidydf, aes(Mean, fct_reorder2(`School Name`, Test, Mean, .fun = first2, .desc = FALSE), color = Test)) + geom_point() + ggtitle(&quot;Schools sorted by Math mean&quot;) + ylab(&quot;&quot;) + theme_dotplot "],["scatter.html", "13 Chart: Scatterplot 13.1 Overview 13.2 tl;dr 13.3 Simple examples 13.4 Theory 13.5 When to use 13.6 Considerations 13.7 Modifications 13.8 External resources", " 13 Chart: Scatterplot 13.1 Overview This section covers how to make scatterplots 13.2 tl;dr Fancy Example NOW! Gimme Gimme GIMME! Here’s a look at the relationship between brain weight vs. body weight for 62 species of land mammals: And here’s the code: library(ggplot2) # plotting mammals &lt;- MASS::mammals # ratio for color choices ratio &lt;- mammals$brain / (mammals$body*1000) ggplot(mammals, aes(x = body, y = brain)) + # plot points, group by color geom_point(aes(fill = ifelse(ratio &gt;= 0.02, &quot;#0000ff&quot;, ifelse(ratio &gt;= 0.01 &amp; ratio &lt; 0.02, &quot;#00ff00&quot;, ifelse(ratio &gt;= 0.005 &amp; ratio &lt; 0.01, &quot;#00ffff&quot;, ifelse(ratio &gt;= 0.001 &amp; ratio &lt; 0.005, &quot;#ffff00&quot;, &quot;#ffffff&quot;))))), col = &quot;#656565&quot;, alpha = 0.5, size = 4, shape = 21) + # add chosen text annotations geom_text(aes(label = ifelse(row.names(mammals) %in% c(&quot;Mouse&quot;, &quot;Human&quot;, &quot;Asian elephant&quot;, &quot;Chimpanzee&quot;, &quot;Owl monkey&quot;, &quot;Ground squirrel&quot;), paste(as.character(row.names(mammals)), &quot;→&quot;, sep = &quot; &quot;),&#39;&#39;)), hjust = 1.12, vjust = 0.3, col = &quot;grey35&quot;) + geom_text(aes(label = ifelse(row.names(mammals) %in% c(&quot;Golden hamster&quot;, &quot;Kangaroo&quot;, &quot;Water opossum&quot;, &quot;Cow&quot;), paste(&quot;←&quot;, as.character(row.names(mammals)), sep = &quot; &quot;),&#39;&#39;)), hjust = -0.12, vjust = 0.35, col = &quot;grey35&quot;) + # customize legend/color palette scale_fill_manual(name = &quot;Brain Weight, as the\\n% of Body Weight&quot;, # values = c(&#39;#e66101&#39;,&#39;#fdb863&#39;,&#39;#b2abd2&#39;,&#39;#5e3c99&#39;), values = c(&#39;#d7191c&#39;,&#39;#fdae61&#39;,&#39;#ffffbf&#39;,&#39;#abd9e9&#39;,&#39;#2c7bb6&#39;), breaks = c(&quot;#0000ff&quot;, &quot;#00ff00&quot;, &quot;#00ffff&quot;, &quot;#ffff00&quot;, &quot;#ffffff&quot;), labels = c(&quot;Greater than 2%&quot;, &quot;Between 1%-2%&quot;, &quot;Between 0.5%-1%&quot;, &quot;Between 0.1%-0.5%&quot;, &quot;Less than 0.1%&quot;)) + # formatting scale_x_log10(name = &quot;Body Weight&quot;, breaks = c(0.01, 1, 100, 10000), labels = c(&quot;10 g&quot;, &quot;1 kg&quot;, &quot;100 kg&quot;, &quot;10K kg&quot;)) + scale_y_log10(name = &quot;Brain Weight&quot;, breaks = c(1, 10, 100, 1000), labels = c(&quot;1 g&quot;, &quot;10 g&quot;, &quot;100 g&quot;, &quot;1 kg&quot;)) + ggtitle(&quot;An Elephant Never Forgets...How Big A Brain It Has&quot;, subtitle = &quot;Brain and Body Weights of Sixty-Two Species of Land Mammals&quot;) + labs(caption = &quot;Source: MASS::mammals&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) + theme(legend.position = c(0.832, 0.21)) For more info on this dataset, type ?MASS::mammals into the console. And if you are going crazy not knowing what species is in the top right corner, it’s another elephant. Specifically, it’s the African elephant. It also never forgets how big a brain it has. 13.3 Simple examples That was too fancy! Much simpler please! Let’s use the SpeedSki dataset from GDAdata to look at how the speed achieved by the participants related to their birth year: library(GDAdata) head(SpeedSki, n = 7) ## Rank Bib FIS.Code Name Year Nation Speed Sex Event ## 1 1 61 7039 ORIGONE Simone 1979 ITA 211.67 Male Speed One ## 2 2 59 7078 ORIGONE Ivan 1987 ITA 209.70 Male Speed One ## 3 3 66 190130 MONTES Bastien 1985 FRA 209.69 Male Speed One ## 4 4 57 7178 SCHROTTSHAMMER Klaus 1979 AUT 209.67 Male Speed One ## 5 5 69 510089 MAY Philippe 1970 SUI 209.19 Male Speed One ## 6 6 75 7204 BILLY Louis 1993 FRA 208.33 Male Speed One ## 7 7 67 7053 PERSSON Daniel 1975 SWE 208.03 Male Speed One ## no.of.runs ## 1 4 ## 2 4 ## 3 4 ## 4 4 ## 5 4 ## 6 4 ## 7 4 13.3.1 Scatterplot using base R x &lt;- SpeedSki$Year y &lt;- SpeedSki$Speed # plot data plot(x, y, main = &quot;Scatterplot of Speed vs. Birth Year&quot;) Base R scatterplots are easy to make. All you need are the two variables you want to plot. Although scatterplots can be made with categorical data, the variables you are plotting will usually be continuous. 13.3.2 Scatterplot using ggplot2 library(GDAdata) # data library(ggplot2) # plotting # main plot scatter &lt;- ggplot(SpeedSki, aes(Year, Speed)) + geom_point() # show with trimmings scatter + labs(x = &quot;Birth Year&quot;, y = &quot;Speed Achieved (km/hr)&quot;) + ggtitle(&quot;Ninety-One Skiers by Birth Year and Speed Achieved&quot;) ggplot2 makes it very easy to create scatterplots. Using geom_point(), you can easily plot two different aesthetics in one graph. It also is simple to add on extra formatting to make your plots look nice (All that is really necessary is the data, the aesthetics, and the geom). 13.4 Theory Scatterplots are very useful in understanding the correlation (or lack thereof) between variables. For example, in section 13.2 notice the positive relationship between brain and body weight in species of land mammals. The scatterplot gives a good idea of whether that relationship is positive or negative and if there’s a correlation. However, don’t mistake correlation in a scatterplot for causation! Below we show variations on the scatterplot which can be used to enhance interpretability. For more info about adding lines/contours, comparing groups, and plotting continuous variables check out Chapter 5 of the textbook. 13.5 When to use Scatterplots are great for exploring relationships between variables. Basically, if you are interested in how variables relate to each other, the scatterplot is a great place to start. 13.6 Considerations 13.6.1 Overlapping data Data with similar values will overlap in a scatterplot and may lead to problems. Consider exploring alpha blending or jittering as remedies (links from Overlapping Data section of Iris Walkthrough). 13.6.2 Scaling Consider how scaling can modify how your data will be perceived: library(ggplot2) num_points &lt;- 100 wide_x &lt;- c(rnorm(n = 50, mean = 100, sd = 2), rnorm(n = 50, mean = 10, sd = 2)) wide_y &lt;- rnorm(n = num_points, mean = 5, sd = 2) df &lt;- data.frame(wide_x, wide_y) ggplot(df, aes(wide_x, wide_y)) + geom_point() + ggtitle(&quot;Linear X-Axis&quot;) ggplot(df, aes(wide_x, wide_y)) + geom_point() + ggtitle(&quot;Log-10 X-Axis&quot;) + scale_x_log10() 13.7 Modifications 13.7.1 Contour lines Contour lines give a sense of the density of the data at a glance. For these contour maps, we will use the SpeedSki dataset. Contour lines can be added to the plot call using geom_density_2d(): ggplot(SpeedSki, aes(Year, Speed)) + geom_density_2d() Contour lines work best when combined with other layers: ggplot(SpeedSki, aes(Year, Speed)) + geom_point() + geom_density_2d(bins = 5) 13.7.2 Scatterplot matrices If you want to compare multiple parameters to each other, consider using a scatterplot matrix. This will allow you to show many comparisons in a compact and efficient manner. For these scatterplot matrices, we will use the movies dataset from the ggplot2movies package. As a default, the base R plot() function will create a scatterplot matrix when given multiple variables: library(ggplot2movies) # data library(dplyr) # manipulation index &lt;- sample(nrow(movies), 500) #sample data moviedf &lt;- movies[index,] # data frame splomvar &lt;- moviedf %&gt;% dplyr::select(length, budget, votes, rating, year) plot(splomvar) While this is quite useful for personal exploration of a datset, it is not recommended for presentation purposes. Something called the Hermann grid illusion makes this plot very difficult to examine. To remove this problem, consider using the splom() function from the lattice package: library(lattice) #sploms splom(splomvar) 13.8 External resources Quick-R article about scatterplots using Base R. Goes from the simple into the very fancy, with Matrices, High Density, and 3D versions. STHDA Base R: article on scatterplots in Base R. More examples of how to enhance the humble graph. STHDA ggplot2: article on scatterplots in ggplot2. Heavy on the formatting options available and facet warps. Stack Overflow on adding labels to points from geom_point() ggplot2 cheatsheet: Always good to have close by. "],["iris.html", "14 Walkthrough: Iris Scatterplot 14.1 Overview 14.2 Quick note on doing it the lazy way 14.3 Viewing data 14.4 Plotting data 14.5 Markdown etiquette 14.6 Overlapping data 14.7 Formatting for presentation 14.8 Alter appearance 14.9 Consider themes 14.10 Going deeper 14.11 Helpful links", " 14 Walkthrough: Iris Scatterplot 14.1 Overview This example goes through some work with the iris dataset to get to a finished scatterplot that is ready to present. 14.1.1 tl;dr Here’s what we end up with: library(ggplot2) base_plot &lt;- ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), size = 3, alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) base_plot + theme_minimal() Wondering how we got there? Read on. 14.1.2 Packages ggplot2 dplyr stats Base datasets (gridExtra) 14.1.3 Techniques Keyboard Shortcuts Viewing Data Structure/Dimensions/etc. Accessing Documentation Plotting with ggplot2 Layered Nature of ggplot2/Grammar of Graphics Mapping aesthetics in ggplot2 Overlapping Data: alpha and jitter Presenting Graphics Themes 14.2 Quick note on doing it the lazy way Shortcuts are your best friend to get work done faster. And they are easy to find. In the toolbar: Tools &gt; Keyboard Shortcuts Help OR ⌥⇧K Some good ones: Insert assignment operator (&lt;-): Alt/Option+- Insert pipe (%&gt;%): Ctrl/Cmd+Shift+M Comment Code: Ctrl/Cmd+Shift+C Run current line/selection: Ctrl/Cmd+Enter Re-run previous region: Ctrl/Cmd+Shift+P Be on the lookout for things you do often and try to see if there is a faster way to do them. Additionally, the RStudio IDE can be a little daunting, but it is full of useful tools that you can read about in this cheatsheet or go through with this DataCamp course: Part 1, Part 2. Okay, now let’s get to it… 14.3 Viewing data Let’s start with loading the package so we can get the data as a dataframe. library(datasets) class(iris) ## [1] &quot;data.frame&quot; This is not a huge dataset, but it is helpful to get into the habit of treating datasets as large no matter what. Because of this, make sure you inspect the size and structure of your dataset before going and printing it to the console. Here we can see that we have 150 observations across 5 different variables. dim(iris) ## [1] 150 5 There are a bunch of ways to get information on your dataset. Here are a few: str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## # This one requires dplyr, but it&#39;s worth it :) library(dplyr) glimpse(iris) ## Rows: 150 ## Columns: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s… Plotting the data by calling iris to the console will print the whole thing. Go ahead and try it in this case, but this is not recommended for larger datasets. Instead, use head() in the console or View(). If you want to learn more about these commands, or anything for that matter, just type ?&lt;command&gt; into the console. ?head, for example, will reveal that there is an additional argument to head called n for the number of lines printed, which defaults to 6. Also, you may notice there is something called tail. I wonder what that does? 14.4 Plotting data Let’s plot something! # Something&#39;s missing library(ggplot2) ggplot(iris) Where is it? Maybe if we add some aesthetics. I remember that was an important word that came up somewhere: # Still not working... ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) Still nothing. Remember, you have to add a geom for something to show up. # There we go! ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() Yay! Something showed up! Notice where we put the data, inside of ggplot(). ggplot is built on layers. Here we put it in the main call to ggplot. The data argument is also available in geom_point(), but in that case it would only apply to that layer. Here, we are saying, for all layers, unless specified, make the data be iris. Now let’s add a color mapping by Species: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species)) Usually it is helpful to store the main portion of the plot in a variable and add on the layers. The code below achieves the same output as above: sepal_plot &lt;- ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) sepal_plot + geom_point(aes(color = Species)) 14.5 Markdown etiquette I’m seeing that my R Markdown file is getting a little messy. Working with markdown and chunks can get out of hand, but there are some helpful tricks. First, consider naming your chunks as you go. If you combine this with headers, your work will be much more organized. Specifically, the little line at the bottom of the editor becomes much more useful. From this: To this: Just add a name to the start of each chunk: {r &lt;cool-code-chunk-name&gt;, &lt;chunk_option&gt; = TRUE} Now you can see what the chunks were about as well as get a sense of where you are in the document. Just don’t forget, it is a space after the r and commas for the other chunk options you may have like eval or echo. For more info, see our section on communicating results. 14.6 Overlapping data Eagle-eyed viewers may notice that we seem to be a few points short. We should be seeing 150 points, but we only see 117 (yes, I counted). Where are those 33 missing points? They are actually hiding behind other points. This dataset rounds to the nearest tenth of a centimeter, which is what is giving us those regular placings of the points. How did I know the data was in centimeters? Running ?iris in the console of course! Ah, you ask a silly question, you get a silly answer. # This plot hides some of the points ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species)) What’s the culprit? The color aesthetic. The color by default is opaque and will hide any points that are behind it. As a rule, it is always beneficial to reduce the opacity a little no matter what to avoid this problem. To do this, change the alpha value to something other than it’s default 1, like 0.5. ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species, alpha = 0.5)) Okay…a couple things with this. 14.6.1 First: the legend First, did you notice the new addition to the legend? That looks silly! Why did that show up? Well, when we added the alpha into aes(), we got a new legend. Let’s look at what we are doing with geom_point(). Specifically, this is saying how we should map the color and alpha: geom_point(mapping = aes(color = Species, alpha = 0.5)) So, we are mapping these given aesthetics, color and alpha, to certain values. ggplot knows that usually the aesthetic mapping will vary since you are probably passing in data that varies, so it will create a legend for each mapping. However, we don’t need a legend for the alpha: we explicitly set it to be 0.5. To fix this, we can pull alpha out of aes and instead treat it like an attribute: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5) No more legend. So, in ggplot, there is a difference between where an aesthetic is placed. It is also called MAPPING an aesthetic (making it vary with data inside aes) or SETTING an aesthetic (make it a constant attribute across all datapoints outside of aes). 14.6.2 Second: jittering Secondly, did this alpha trick really help us? Are we able to see anything in the plot in an easier way? Not really. Since the points perfectly overlap, the opacity difference doesn’t help us much. Usually, opacity will work, but here the data is so regular that we don’t gain anything in the perception department. We can fix this by introducing some jitter to the datapoints. Jitter adds a little random noise and moves the datapoints so that they don’t fully overlap: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5, position = &quot;jitter&quot;) Consider your motives when using jittering. You are by definition altering the data, but it may be beneficial in some situations. 14.6.3 Aside: example where alpha blending works We are dealing with a case where jittering works best to see the data, while changing the alpha doesn’t help us much. Here’s a quick example where opacity using alpha might be more directly helpful. # lib for arranging plots side by side library(gridExtra) # make some normally distributed data x_points &lt;- rnorm(n = 10000, mean = 0, sd = 2) y_points &lt;- rnorm(n = 10000, mean = 6, sd = 2) df &lt;- data.frame(x_points, y_points) # plot with/without changed alpha plt1 &lt;- ggplot(df, aes(x_points, y_points)) + geom_point() + ggtitle(&quot;Before (alpha = 1)&quot;) plt2 &lt;- ggplot(df, aes(x_points, y_points)) + geom_point(alpha = 0.1) + ggtitle(&quot;After (alpha = 0.1)&quot;) # arrange plots gridExtra::grid.arrange(plt1, plt2, ncol = 2, nrow = 1) Here it is much easier to see where the dataset is concentrated. 14.7 Formatting for presentation Let’s say we have finished this plot and we are ready to present it to other people: We should clean it up a bit so it can stand on its own. 14.8 Alter appearance First, let’s make the x/y labels a little cleaner and more descriptive: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) Next, add a title that encapsulates the plot: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) And make the points a little bigger: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), size = 3, alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) Now it’s looking presentable. 14.9 Consider themes It may be better for your situation to change the theme of the plot (the background, axes, etc.; the “accessories” of the plot). Explore what different themes can offer and pick one that is right for you. base_plot &lt;- ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), size = 3, alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) base_plot base_plot + theme_light() base_plot + theme_minimal() base_plot + theme_classic() base_plot + theme_void() I’m going to go with theme_minimal() this time. So here we are! We got a lovely scatterplot ready to show the world! 14.10 Going deeper We have just touched the surface of ggplot and dipped our toes into grammar of graphics. If you want to go deeper, I highly recommend the DataCamp courses on Data Visualization with ggplot2 with Rick Scavetta. There are three parts and they are quite dense, but the first part is definitely worth checking out. 14.11 Helpful links RStudio ggplot2 Cheat Sheet DataCamp: Mapping aesthetics to things in ggplot R Markdown Reference Guide R for Data Science "],["parallelcoordinates.html", "15 Chart: Parallel Coordinate Plots 15.1 Overview 15.2 tl;dr 15.3 Simple examples 15.4 Theory 15.5 When to use 15.6 Considerations 15.7 Modifications 15.8 Other Packages 15.9 External Resources", " 15 Chart: Parallel Coordinate Plots This chapter originated as a community contribution created by aashnakanuga 15.1 Overview This section covers how to create static parallel coordinate plots with the GGally package. For interactive parallel coordinate plots, check out the parcoords package. The package vignette provides instructions on using this package. 15.2 tl;dr I want a Fancy Example! Not tomorrow, not after breakfast, NOW! Here’s a look at the effect of different attributes on each Fair cut diamond from the “diamonds” dataset: And here’s the code: library(GGally) library(dplyr) #subset the data to get the first thousand cases diamonds_subset &lt;- subset(diamonds[1:1000,]) #rename the variables to understand what they signify names(diamonds_subset)&lt;-c(&quot;carat&quot;,&quot;cut&quot;,&quot;color&quot;,&quot;clarity&quot;,&quot;depth_percentage&quot;,&quot;table&quot;,&quot;price&quot;,&quot;length&quot;,&quot;width&quot;,&quot;depth&quot;) #Create a new column to highlight the fair cut diamonds ds_fair&lt;-within(diamonds_subset, diamond_cut&lt;-if_else(cut==&quot;Fair&quot;, &quot;Fair&quot;, &quot;Other&quot;)) #Create the graph ggparcoord(ds_fair[order(ds_fair$diamond_cut, decreasing=TRUE),], columns=c(1,5,7:10), groupColumn = &quot;diamond_cut&quot;, alphaLines = 0.8, order=c(5,1,8,9,10,7), title = &quot;Parallel Coordinate Plot showing trends for Fair cut diamonds&quot;, scale = &quot;uniminmax&quot;) + scale_color_manual(values=c(&quot;maroon&quot;,&quot;gray&quot;)) For more information about the dataset, type ?diamonds into the console. 15.3 Simple examples Woah woah woah! Too complicated! Much simpler, please. Let us use the popular “iris” dataset for this example: library(datasets) library(GGally) ggparcoord(iris, columns=1:4, title = &quot;Parallel coordinate plot for Iris flowers&quot;) For more information about the dataset, type ?iris into the console. 15.4 Theory For more info about parallel coordinate plots and multivariate continuous data, check out Chapter 6 of the textbook. 15.5 When to use Generally, parallel coordinate plots are used to infer relationships between multiple continuous variables - we mostly use them to detect a general trend that our data follows, and also the specific cases that are outliers. Please keep in mind that parallel coordinate plots are not the ideal graph to use when there are just categorical variables involved. We can include a few categorical variables in our axes or for the sake of clustering, but using a lot of categorical variables results in overlapping profiles, which makes it difficult to interpret. We can also use parallel coordinate plots to identify trends in specific clusters - just highlight each cluster in a different color using the groupColumn attribute of ggparcoord() to specify your column, and you are good to go! Sometimes, parallel coordinate plots are very helpful in graphing time series data - where we have information stored at regular time intervals. Each vertical axis will now become a time point and we need to pass that column in ggparcoord’s “column” attribute. 15.6 Considerations 15.6.1 When do I use clustering? Generally, you use clustering when you want to observe a pattern in a set of cases with some specific properties. This may include divvying up all variables into clusters based on their value for a specific categorical variable. But you can even use a continuous variable; for example, dividing all cases into two sections based on some continuous variable height: those who have a height greater than 150cm and those who do not. Let us look at an example using our iris dataset, clustering on the “Species” column: library(GGally) #highlight the clusters based on the Species column graph&lt;-ggparcoord(iris, columns=1:4, groupColumn = 5, title = &quot;Plot for Iris data, where each color represents a specific Species&quot;) graph 15.6.2 Deciding the value of alpha In practice, parallel coordinate plots are not going to be used for very small datasets. Your data will likely have thousands and thousands of cases, and sometimes it can get very difficult to observe anything when so many of your cases will overlap. So we set the aplhaLines parameter to a value between zero and one, and it reduces the opacity of all lines so that you can get a clearer view of what is going on if you have too many overlapping cases. Again we use our iris data, but reduce alpha to 0.5. Observe how much easier it is now to trace the course of every case: library(ggplot2) library(GGally) #set the value of alpha to 0.5 ggparcoord(iris, columns=1:4, groupColumn = 5, alphaLines = 0.5, title = &quot;Iris data with a lower alpha value&quot;) 15.6.3 Scales When we use ggparcoord(), we have an option to set the scale attribute, which will scale all variables so we can compare their values. The different types of scales are as follows: std: default value, where it subtracts mean and divides by SD robust: subtract median and divide by median absolute deviation uniminmax: scale all values so that the minimum is at 0 and maximum at 1 globalminmax: no scaling, original values taken center: centers each variable according to the value given in scaleSummary centerObs: centers each variable according to the value of the observation given in centerObsID Let us create a sample dataset and see how values on the y-axis change for different scales: library(ggplot2) library(GGally) library(gridExtra) #creating a sample dataset df1&lt;-data.frame(col1=c(11,4,7,4,3,8,5,7,9), col2=c(105,94,138,194,173,129,156,163,148)) #pay attention to the different values on the y-axis g1&lt;-ggparcoord(df1, columns=1:2, scale = &quot;std&quot;, title = &quot;Standard Scale&quot;) g2&lt;-ggparcoord(df1, columns=1:2, scale = &quot;robust&quot;, title = &quot;Robust Scale&quot;) g3&lt;-ggparcoord(df1, columns=1:2, scale = &quot;uniminmax&quot;, title = &quot;Uniminmax Scale&quot;) g4&lt;-ggparcoord(df1, columns=1:2, scale = &quot;globalminmax&quot;, title = &quot;Globalminmax Scale&quot;) g5&lt;-ggparcoord(df1, columns=1:2, scale = &quot;center&quot;, scaleSummary = &quot;mean&quot;, title = &quot;Center Scale&quot;) g6&lt;-ggparcoord(df1, columns=1:2, scale = &quot;centerObs&quot;, centerObsID = 4, title = &quot;CenterObs Scale&quot;) grid.arrange(g1, g2, g3, g4, g5, g6, nrow=2) 15.6.4 Order of the variables Deciding the order of the variables on the y-axis depends on your application. It can be specified using the order parameter. The different types of order are as follows: default: the order in which we add our variables to the column attribute given vector: providing a vector of the order we need (used most frequently) anyClass: order based on the separation of a variable from the rest (F-statistic - each variable v/s the rest) allClass: order based on the variation between classes (F-statistic - group column v/s the rest) skewness: order from most to least skewed Outlying: order based on the Outlying measure 15.7 Modifications 15.7.1 Flipping the coordinates A good idea if we have too many variables and their names are overlapping on the x-axis: library(ggplot2) library(GGally) #using the iris dataset graph + coord_flip() 15.7.2 Highlighting trends Let us see what trend the versicolor Species of the iris dataset follows over the other variables: library(ggplot2) library(GGally) library(dplyr) #get a new column that says &quot;Yes&quot; of the Species is versicolor. ds_versi&lt;-within(iris, versicolor&lt;-if_else(Species==&quot;versicolor&quot;, &quot;Yes&quot;, &quot;No&quot;)) ggparcoord(ds_versi[order(ds_versi$versicolor),], columns = 1:4, groupColumn = &quot;versicolor&quot;, title = &quot;Highlighting trends of Versicolor species&quot;) + scale_color_manual(values=c(&quot;gray&quot;,&quot;maroon&quot;)) 15.7.3 Using splines Generally, we use splines if we have a column where there are a lot of repeating values, which adds a lot of noise. The case lines become more and more curved when we set a higher spline factor, which removes noise and makes for easier observations of trends. It can be set using the splineFactor attribute: library(ggplot2) library(GGally) library(gridExtra) #create a sample dataset df2&lt;-data.frame(col1=c(1:9), col2=c(11,11,14,15,15,15,17,18,18), col3=c(4,4,4,7,7,7,8,9,9), col4=c(3,3,3,4,6,6,6,8,8)) #plot without spline g7&lt;-ggparcoord(df2, columns = 1:4, scale = &quot;globalminmax&quot;, title = &quot;No Spline factor&quot;) #plot with spline g8&lt;-ggparcoord(df2, columns = 1:4, scale = &quot;globalminmax&quot;, splineFactor=10, title = &quot;Spline factor set to 10&quot;) grid.arrange(g7,g8) 15.7.4 Adding boxplots to the graph You can add boxplots to your graph, which can be useful for observing the trend of median values. Generally, they are added to data with a lot of variables - for example, if we plot time series data. 15.8 Other Packages There are a number of packages that have functions for creating parallel coordinate plots: [to do: add links] parcoords::parcoords() – great interactive option ggplot2::geom_line() – not specific to parallel coordinate plots but easy to create with the group= parameter. lattice::parallelplot() MASS::parcoord() 15.9 External Resources Introduction to parallel coordinate plots: An excellent resource giving details of all attributes and possible values. Also has some good examples. How to create interactive parallel coordinate plots: a nice walkthrough on using plotly to create an interactive parallel coordinate plot. Different methods to create parallel coordinate plots: This is specifically when we have categorical variables. "],["mosaic.html", "16 Chart: Mosaic 16.1 Overview 16.2 Direction of splits 16.3 Fill color 16.4 Labels 16.5 Cell spacing 16.6 Mosaic using ggplot 16.7 Theory 16.8 When to use 16.9 Considerations 16.10 External resources", " 16 Chart: Mosaic This chapter originated as a community contribution created by harin This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. df = read_csv(&quot;data/MusicIcecream.csv&quot;) 16.1 Overview Mosaic plots take some investment to learn to read and draw properly. Particularly when starting out, we recommend drawing them incrementally: start with splitting on one variable and then add additional variables one at a time. The full mosaic plot will have one split per variable. Important: if your data has a frequency column, as in the example below, the count column must be called Freq. (Tables and matrices also work, see ?vcd::structable for more details.) Also note that all of these plots are drawn with vcd::mosaic() not the base R function, mosaicplot(). The data: df ## # A tibble: 8 x 4 ## Age Music Favorite Freq ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 old classical bubble gum 1 ## 2 old rock bubble gum 1 ## 3 old classical coffee 3 ## 4 old rock coffee 1 ## 5 young classical bubble gum 2 ## 6 young rock bubble gum 5 ## 7 young classical coffee 1 ## 8 young rock coffee 0 Split on Age only: vcd::mosaic(~Age, df) Split on Age, then Music: vcd::mosaic(Music ~ Age, df) Note that the first split is between “young” and “old”, while the second set of splits divides each age group into “classical” and “rock”. Split on Age, then Music, then Favorite: vcd::mosaic(Favorite ~ Age + Music, df) 16.2 Direction of splits Note that in the previous example, the direction of the splits is as follows: Age – horizontal split Music – vertical split Favorite – horizontal split This is the default direction pattern: alternating directions beginning with horizontal. Therefore we get the same plot with the following: vcd::mosaic(Favorite ~ Age + Music, direction = c(&quot;h&quot;, &quot;v&quot;, &quot;h&quot;), df) The directions can be altered as desired. For example, to create a doubledecker plot, make all splits vertical except the last one: vcd::mosaic(Favorite ~ Age + Music, direction = c(&quot;v&quot;, &quot;v&quot;, &quot;h&quot;), df) Note that the direction vector is in order of splits (Age, Music, Favorite), not in the order in which the variables appear in the formula, where the last variable to be split is listed first, before the “~”. 16.3 Fill color Fill colors are applied and recycled according to the last cut dimension, i.e. the dependent variable–in this case favorite flavor ice cream. (If this is not working properly, update to the latest version of vcd. vcd::mosaic(Favorite ~ Age + Music, highlighting_fill = c(&quot;grey90&quot;, &quot;cornflowerblue&quot;), df) 16.4 Labels For official documentation on labeling options, see Labeling in the Strucplot Framework 16.4.1 Rotate labels The rot_labels = vector sets the rotation in degrees on the four sides of the plot–not on variable split order–in this order: top, right, bottom, left. (Different from the typical base graphics order!) The default is rot_labels = c(0, 90, 0, 90). vcd::mosaic(Favorite ~ Age + Music, labeling = vcd::labeling_border(rot_labels = c(45, -45, 0, 0)), df) 16.4.2 Abbreviate labels Labels are abbreviated in the order of the splits (as for direction =). The abbreviation algorithm appears to return the specified number of characters after vowels are eliminated (if necessary). For more formatting options, see &gt;?vcd::labeling_border. vcd::mosaic(Favorite ~ Age + Music, labeling = vcd::labeling_border(abbreviate_labs = c(3, 1, 6)), df) 16.5 Cell spacing vcd::mosaic(Favorite ~ Age + Music, spacing = vcd::spacing_equal(sp = unit(0, &quot;lines&quot;)), df) For more details, see &gt;?vcd::spacings 16.5.1 Mosaic using vcd::doubledecker data(Arthritis, package = &quot;vcd&quot;) vcd::doubledecker(Improved ~ Treatment + Sex, data=Arthritis) vcd::doubledecker(Music ~ Favorite + Age, xtabs(Freq ~ Age + Music + Favorite, df)) 16.6 Mosaic using ggplot To create mosaic plots in the ggplot2 framework, use geom_mosaic() which is available in the ggmosaic package: https://cran.r-project.org/web/packages/ggmosaic/vignettes/ggmosaic.html 16.7 Theory 16.8 When to use When you want to see the relationships in Multivariate Categorical Data 16.9 Considerations 16.9.1 Labels Legibility of the labels is problematic in mosaic plot especially when there are a lot of dimensions. This can be alleviated by - Abbreviate names - Rotating the labels 16.9.2 Aspect Ratio lengths are easier to judge than area, so try to use rectangles with same width or height Taller thinner rectangles are better (we are better at distinguishing length than area) 16.9.3 Gaps between rectangles No gap = most efficient However, a gap can help improve legibility, so try out different combinations Can have a gap at splits Can Vary gap size down the hierarchy 16.9.4 Color good for rates in the subgroup displaying residual emphasizing particular subgroup 16.10 External resources Chapter 7 of Graphical data analysis with R by Anthony Unwin Link: A comprehensive overview of mosaic plot in ggplot check out the link below. "],["heatmap.html", "17 Chart: Heatmap 17.1 Overview 17.2 tl;dr 17.3 Simple examples 17.4 Theory 17.5 External resources", " 17 Chart: Heatmap 17.1 Overview This section covers how to make heatmaps. 17.2 tl;dr Enough with these simple examples! I want a complicated one! Here’s a heatmap of occupational categories of sons and fathers in the US, UK, and Japan: And here’s the code: library(vcdExtra) # dataset library(dplyr) # manipulation library(ggplot2) # plotting library(viridis) # color palette # format data orderedclasses &lt;- c(&quot;Farm&quot;, &quot;LoM&quot;, &quot;UpM&quot;, &quot;LoNM&quot;, &quot;UpNM&quot;) mydata &lt;- Yamaguchi87 mydata$Son &lt;- factor(mydata$Son, levels = orderedclasses) mydata$Father &lt;- factor(mydata$Father, levels = orderedclasses) japan &lt;- mydata %&gt;% filter(Country == &quot;Japan&quot;) uk &lt;- mydata %&gt;% filter(Country == &quot;UK&quot;) us &lt;- mydata %&gt;% filter(Country == &quot;US&quot;) # convert to % of country and class total mydata_new &lt;- mydata %&gt;% group_by(Country, Father) %&gt;% mutate(Total = sum(Freq)) %&gt;% ungroup() # make custom theme theme_heat &lt;- theme_classic() + theme(axis.line = element_blank(), axis.ticks = element_blank()) # basic plot plot &lt;- ggplot(mydata_new, aes(x = Father, y = Son)) + geom_tile(aes(fill = Freq/Total), color = &quot;white&quot;) + coord_fixed() + facet_wrap(~Country) + theme_heat # plot with text overlay and viridis color palette plot + geom_text(aes(label = round(Freq/Total, 1)), color = &quot;white&quot;) + scale_fill_viridis() + # formatting ggtitle(&quot;Like Father, Like Son&quot;, subtitle = &quot;Heatmaps of occupational categories for fathers and sons, by country&quot;) + labs(caption = &quot;Source: vcdExtra::Yamaguchi87&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?vcdExtra::Yamaguchi87 into the console. 17.3 Simple examples Too complicated! Simplify, man! 17.3.1 Heatmap of two-dimensional bin counts For this heatmap, we will use the SpeedSki dataset. Only two variables, x and y are needed for two-dimensional bin count heatmaps. The third variable–i.e., the color–represents the bin count of points in the region it covers. Think of it as a two-dimensional histogram. To create a heatmap, simply substitute geom_point() with geom_bin2d(): library(ggplot2) # plotting library(GDAdata) # data (SpeedSki) ggplot(SpeedSki, aes(Year, Speed)) + geom_bin2d() 17.3.2 Heat map of dataframe To get a visual sense of the dataframe, you can use a heatmap. You can also look into scaling the columns to get a sense of your data on a common scale. In this example, we use geom_tile to graph all cells in the dataframe and color them by their value: library(pgmm) # data library(tidyverse) # processing/graphing library(viridis) # color palette data(wine) # convert to column, value wine_new &lt;- wine %&gt;% rownames_to_column() %&gt;% gather(colname, value, -rowname) ggplot(wine_new, aes(x = rowname, y = colname, fill = value)) + geom_tile() + scale_fill_viridis() + ggtitle(&quot;Italian Wine Dataframe&quot;) # only difference from above is scaling wine_scaled &lt;- data.frame(scale(wine)) %&gt;% rownames_to_column() %&gt;% gather(colname, value, -rowname) ggplot(wine_scaled, aes(x = rowname, y = colname, fill = value)) + geom_tile() + scale_fill_viridis() + ggtitle(&quot;Italian Wine Dataframe, Scaled&quot;) 17.3.3 Modifications You can change the color palette by specifying it explicitly in your chain of ggplot function calls. The bin width can be added inside the geom_bin2d() function call: library(viridis) # viridis color palette # create plot g1 &lt;- ggplot(SpeedSki, aes(Year, Speed)) + scale_fill_viridis() # modify color # show plot g1 + geom_bin2d(binwidth = c(5, 5)) # modify bin width Here are some other examples: # larger bin width g1 + geom_bin2d(binwidth = c(10, 10)) # hexagonal bins g1 + geom_hex(binwidth = c(5, 5)) # hexagonal bins + scatterplot layer g1 + geom_hex(binwidth = c(5, 5), alpha = .4) + geom_point(size = 2, alpha = 0.8) # hexagonal bins with custom color gradient/bin count ggplot(SpeedSki, aes(Year, Speed)) + scale_fill_gradient(low = &quot;#cccccc&quot;, high = &quot;#09005F&quot;) + # color geom_hex(bins = 10) # number of bins horizontally/vertically 17.4 Theory Heat maps are like a combination of scatterplots and histograms: they allow you to compare different parameters while also seeing their relative distributions. While heatmaps are visually striking, there are often better choices to get your point across. For more info, checkout this DataCamp section on heatmaps and alternatives. 17.5 External resources R Graph Gallery: Heatmaps: Has examples of creating heatmaps with the heatmap() function. How to make a simple heatmap in ggplot2: Create a heatmap with geom_tile(). "],["maps.html", "18 Spatial Data 18.1 High level packages 18.2 Shape files and more 18.3 State-of-the-art", " 18 Spatial Data This chapter is under active development. Check back soon for updates. There are an overwhelming number of R packages for analyzing and visualizing spatial data. In broad terms, spatial visualizations require a merging of non-spatial and spatial information. For example, if you wish to create a choropleth map of the murder rate by county in New York State, you need county level data on murder rates, and you also need geographic data for drawing the county boundaries, stored in what are called shape files. A rough divide exists between packages that don’t require you deal with shape files and those that do. The former work by taking care of the geographic data under the hood: you supply the data with a column for the location and the package takes care of figuring out how to draw those locations. Not surprisingly, there are more easy-to-use options for commonly drawn maps, such as world continents, U.S. states, etc. 18.1 High level packages 18.1.1 Choropleth maps Choropleth maps use color to indicate the value of a variable within a defined region, generally political boundaries. In this section we provide options that do not require you to work with shape files. “Mapping in R” by Hanjun Li and Chengchao Jin explain how to use the maps package to create choropleth maps both with base R graphics and ggplot2. The maps package is U.S.-centered but also contains spatial data on a few other locations, as documented in the package reference manual. The choroplethr package makes it simple to draw choropleth maps of U.S. states, countries, and census tracts, as well as countries of the world without dealing directly with shape files. The companion package, choroplethrZip, provides data for zip code level choropleths; choroplethrAdmin1 draws choropleths for administrative regions of world countries. The following is a brief tutorial on using these packages. Note: You must install also install choroplethrMaps for choroplethr to work. In addition, choroplethr requires a number of other dependencies which should be installed automatically, but if they aren’t, you can manually install the missing packages that you are notified about when you call library(choroplethr): maptools, and rgdal, sp. We’ll use the state.x77 dataset for this example: library(tidyverse) library(choroplethr) # data frame must contain &quot;region&quot; and &quot;value&quot; columns df_illiteracy &lt;- state.x77 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;state&quot;) %&gt;% transmute(region = tolower(`state`), value = Illiteracy) state_choropleth(df_illiteracy, title = &quot;State Illiteracy Rates, 1977&quot;, legend = &quot;Percent Illiterate&quot;) 18.1.2 Square bins Packages such as statebins create choropleth style maps with equal size regions that roughly represent the location of the region, but not the size or shape. Important: Don’t install statebins from CRAN; use the dev version – it contains many improvements, which are detailed in “Statebins Reimagined”. # devtools::install_github(&quot;hrbrmstr/statebins&quot;) library(statebins) df_illit &lt;- state.x77 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;state&quot;) %&gt;% select(state, Illiteracy) # Note: direction = 1 switches the order of the fill scale # so darker shades represent higher illiteracy rates # (The default is -1). statebins(df_illit, value_col=&quot;Illiteracy&quot;, name = &quot;%&quot;, direction = 1) + ggtitle(&quot;State Illiteracy Rates, 1977&quot;) + theme_statebins() 18.1.3 Longitude / latitude data Note that the options above work with political boundaries, based on the names of the regions that you provide. Such maps require packages with geographical boundary information. Longitude / latitude data, on the other hand, can be plotted simply as a scatterplot with x = longitude and y = latitude, without any background maps (just don’t mix up x &amp; y!) The first part of “Data wrangling visualisation and spatial analysis: R Workshop” by C. Brown, D. Schoeman, A. Richardson, and B. Venables provides a detailed walkthrough of spatial exploratory data analysis with copepod data (a type of zooplankton) using this technique with ggplot2::geom_point(). It is a highly recommended resource as it covers much of the data science pipeline from the context of the problem to obtaining data, cleaning and transforming it, exploring the data, and finally modeling and predicting. Adding a background map to lon/lat data requires pulling a map from a map API and setting a bounding box. This can be done easily with ggmap, which offers several different map source options. Google Maps API was the go-to, but they now require you to enable billing through Google Cloud Platorm. You get $300 in free credit, but if providing a credit card isn’t your thing, you may consider using Stamen Maps instead, with the get_stamenmap() function. Use the development version of the package; instructions and extensive examples are available on the package’s GitHub page “Stamen Maps with ggmap” by Mrugank Akarte offers an introductory tutorial. “Getting started Stamen maps with ggmap” will help you get started with Stamen maps through an example using the Sacramento dataset in the caret package. 18.2 Shape files and more “Plotting Maps with R: An Example-Based Tutorial” by Jonathan Santoso and Kevin Wibisono provides an introduction to working with shape files using the rgdal package and plotting maps with base R graphics, ggmap, leaflet, and tmap. “Mapping in R just got a whole lot easier” by Sharon Machlis (2017-03-03) offers a tutorial on using the tmap, tmaptools, and tigris packages to create choropleth maps. Note that with this approach, you will need to merge geographic shape files with your data files, and then map. “Step-by-Step Choropleth Map in R: A case of mapping Nepal” walks through the process of creating a choropleth map using rgdal and ggplot2. The second part of the copepod data tutorial of the mentioned above provides examples using the maps and sf packages. You’ll learn how to set bounding boxes and change the map projection. The tutorial concludes with an interactive map created with leaflet. 18.3 State-of-the-art If you’re serious about learning how to create maps in R, Geocomputation in R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow is the place to start. This book focuses on the tmap package, but is much more than a technical how-to, providing a theoretical context for working with geographical data. "],["network.html", "19 Interactive Networks 19.1 visNetwork (interactive)", " 19 Interactive Networks 19.1 visNetwork (interactive) visNetwork is a powerful R implementation of the interactive JavaScript vis.js library; it uses tidyverse piping: VisNetwork Docs. –&gt; The Vignette has clear worked-out examples: https://cran.r-project.org/web/packages/visNetwork/vignettes/Introduction-to-visNetwork.html The visNetwork documentation doesn’t provide the same level of explanation as the original, so it’s worth checking out the vis.js documentation as well: http://visjs.org/index.html In particular, the interactive examples are particularly useful for trying out different options. For example, you can test out physics options with this network configurator. 19.1.1 Minimum working example Create a node data frame with a minimum one of column (must be called id) with node names: # nodes boroughs &lt;- data.frame(id = c(&quot;The Bronx&quot;, &quot;Manhattan&quot;, &quot;Queens&quot;, &quot;Brooklyn&quot;, &quot;Staten Island&quot;)) Create a separate data frame of edges with from and to columns. # edges connections &lt;- data.frame(from = c(&quot;The Bronx&quot;, &quot;The Bronx&quot;, &quot;Queens&quot;, &quot;Queens&quot;, &quot;Manhattan&quot;, &quot;Brooklyn&quot;), to = c(&quot;Manhattan&quot;, &quot;Queens&quot;, &quot;Brooklyn&quot;, &quot;Manhattan&quot;, &quot;Brooklyn&quot;, &quot;Staten Island&quot;)) Draw the network with visNetwork(nodes, edges) library(visNetwork) visNetwork(boroughs, connections) Add labels by adding a label column to nodes: library(dplyr) boroughs &lt;- boroughs %&gt;% mutate(label = id) visNetwork(boroughs, connections) 19.1.2 Performance visNetwork can be very slow. %&gt;% visPhysics(stabilization = FALSE) starts rendering before the stabilization is complete, which does actually speed things up but allows you to see what’s happening, which makes a big difference in user experience. (It’s also fun to watch the network stabilize). Other performance tips are described here. 19.1.3 Helpful configuration tools %&gt;% visConfigure(enabled = TRUE) is a useful tool for configuring options interactively. Upon completion, click “generate options” for the code to reproduce the settings. More here (Note that changing options and then viewing them requires a lot of vertical scrolling in the browser. I’m not sure if anything can be done about this. If you have a solution, let me know!) 19.1.4 Coloring nodes Add a column of actual color names to the nodes data frame: boroughs &lt;- boroughs %&gt;% mutate(is.island = c(FALSE, TRUE, FALSE, FALSE, TRUE)) %&gt;% mutate(color = ifelse(is.island, &quot;blue&quot;, &quot;yellow&quot;)) visNetwork(boroughs, connections) 19.1.5 Directed nodes (arrows) visNetwork(boroughs, connections) %&gt;% visEdges(arrows = &quot;to;from&quot;, color = &quot;green&quot;) 19.1.6 Turn off the physics simulation It’s much faster without the simulation. The nodes are randomly placed and can be moved around without affecting the rest of the network, at least in the case of small networks. visNetwork(boroughs, connections) %&gt;% visEdges(physics = FALSE) 19.1.7 Grey out nodes far from selected (defined by “degree”) (Click a node to see effect.) # defaults to 1 degree visNetwork(boroughs, connections) %&gt;% visOptions(highlightNearest = TRUE) # set degree to 2 visNetwork(boroughs, connections) %&gt;% visOptions(highlightNearest = list(enabled = TRUE, degree = 2)) "],["timeseriesbasic.html", "20 Time Series 20.1 Overview 20.2 Single/Multiple Time Series 20.3 Secular Trend 20.4 Seasonal Trends 20.5 Frequency of Data", " 20 Time Series This chapter originated as a community contribution created by HaiqingXu This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 20.1 Overview This section discusses drawing graphics for time series data. 20.2 Single/Multiple Time Series We can draw time series using geom_line() with time on the x-axis. X-axis should be an object in the Date class, assuming there is no hour/minute/second data. library(tidyverse) ggplot(data = economics, aes(x = date, y = pop))+ geom_line(color = &quot;blue&quot;) + ggtitle(&quot;US Population, in Thousands&quot;) + labs(x = &quot;year&quot;, y = &quot;population&quot;) We can also draw multiple time series on one plot for comparison purpose: df &lt;- read_csv(&quot;data/mortgage.csv&quot;) df &lt;- df %&gt;% gather(key = TYPE, value = RATE, -DATE) %&gt;% mutate(TYPE = forcats::fct_reorder2(TYPE, DATE, RATE)) # puts legend in correct order g &lt;- ggplot(df, aes(DATE, RATE, color = TYPE)) + geom_line() + ggtitle(&quot;U.S. Mortgage Rates&quot;) + labs (x = &quot;&quot;, y = &quot;percent&quot;) + theme_grey(16) + theme(legend.title = element_blank()) g The following exmaple shows the closing price for four big technology companies in the US. When analyzing GDP, salary level and stock prices, it is often difficult to compare trends since the scales are so different. For example, since AAPL and MSFT prices per share are so much lower than GOOG’s price per share, it’s hard to discern the trends: library(tidyquant) stocks &lt;- c(&quot;AAPL&quot;, &quot;GOOG&quot;, &quot;IBM&quot;, &quot;MSFT&quot;) df &lt;- tq_get(stocks, from = as.Date(&quot;2013-01-01&quot;), to = as.Date(&quot;2013-12-31&quot;)) ggplot(df, aes(date, y = close, color = fct_reorder2(symbol, date, close))) + geom_line() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + theme(legend.title = element_blank()) In such a case, it can be helpful to rescale the data. We rescaled the data to make sure these four stocks have a price of 100 on Jan 2013: df &lt;- df %&gt;% group_by(symbol) %&gt;% mutate(rescaled_close = 100*close / close[1]) ggplot(df, aes(date, y = rescaled_close, color = fct_reorder2(symbol, date, rescaled_close))) + geom_line() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + theme(legend.title = element_blank()) 20.3 Secular Trend Instead of looking at observations over time, we often want to ovserve overall long-term trend in our time series data. In this case, we can use geom_smooth(). Here we will show secular trend using the Loess Smoother. AAPL &lt;- df %&gt;% filter(symbol == &quot;AAPL&quot;) g &lt;- ggplot(AAPL, aes(date, close)) + geom_point() g + geom_line(color = &quot;grey50&quot;) + geom_smooth(method = &quot;loess&quot;, se = FALSE, lwd = 1.5) + ggtitle(&quot;Loess Smoother for Apple Stock Price&quot;) + labs(x = &quot;Date&quot;, y = &quot;Price&quot;) Experiment with different smoothing parameters. g + geom_smooth(method = &quot;loess&quot;, span = .5, se = FALSE) 20.4 Seasonal Trends In addition to secular trends, there are also seasonal trends in time series data. One way is to visualize seasonal trends is to use fact on season(day of month, day of week etc.). library(lubridate) dfman &lt;- read_csv(&quot;data/ManchesterByTheSea.csv&quot;) ggplot(dfman, aes(Date, Gross)) + geom_line() + facet_grid(wday(Date, label = TRUE)~.) Or, let us create a monthly plot. monthplot(AirPassengers) 20.5 Frequency of Data What if you want to observe the frequency of time series data? A simple answer: use geom_point() in addition to geom_line(). # read file mydat &lt;- read_csv(&quot;data/WA_Sales_Products_2012-14.csv&quot;) %&gt;% mutate(Revenue = Revenue/1000000) # convert Quarter to a single numeric value Q mydat$Q &lt;- as.numeric(substr(mydat$Quarter, 2, 2)) # convert Q to end-of-quarter date mydat$Date &lt;- as.Date(paste0(mydat$Year, &quot;-&quot;, as.character(mydat$Q*3), &quot;-30&quot;)) Methoddata &lt;- mydat %&gt;% group_by(Date, `Order method type`) %&gt;% summarize(Revenue = sum(Revenue)) g &lt;- ggplot(Methoddata, aes(Date, Revenue, color = `Order method type`)) + geom_line(aes(group = `Order method type`)) + scale_x_date(limits = c(as.Date(&quot;2012-02-01&quot;), as.Date(&quot;2014-12-31&quot;)), date_breaks = &quot;6 months&quot;, date_labels = &quot;%b %Y&quot;) + ylab(&quot;Revenue in mil $&quot;) g + geom_point() There could be NA values in time series data. Using geom_point() with geom_line() is one way to detect missing values. Here we introduce another option: leave gaps. Methoddata$Date[year(Methoddata$Date)==2013] &lt;- NA g &lt;- ggplot(Methoddata, aes(Date, Revenue, color = `Order method type`)) + geom_path(aes(group = `Order method type`)) + scale_x_date(limits = c(as.Date(&quot;2012-02-01&quot;), as.Date(&quot;2014-12-31&quot;)), date_breaks = &quot;6 months&quot;, date_labels = &quot;%b %Y&quot;) + ylab(&quot;Revenue in mil $&quot;) g "],["tidyquant.html", "21 Stock data with tidyquant 21.1 Overview 21.2 What is tidyquant? 21.3 Installing tidyquant 21.4 Single timeseries 21.5 Multiple timeseries 21.6 External Resources", " 21 Stock data with tidyquant This chapter originated as a community contribution created by naotominakawa This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 21.1 Overview This section covers how to use the tidyquant package to conduct timeseries analysis. 21.2 What is tidyquant? tidyquant is an one-stop shop for financial analysis. It is suitable for analyzing timeseries data, such as financial and economic data. tidyquant connects to various data sources such as Yahoo! Finance, Morning Star, Bloomberg market data, etc. It also behaves well with other Tidyverse packages. 21.3 Installing tidyquant You can install tidyquant from CRAN: install.packages(&quot;tidyquant&quot;) If you want to see which functions are available, you can run the following: # to see which functions are available (not run) library(tidyquant) tq_transmute_fun_options() 21.4 Single timeseries Obtain historical data for single stock (for example, Google): # get historical data for single stock. e.g. google library(tidyquant) tq_get(&quot;GOOGL&quot;, get=&quot;stock.prices&quot;) ## # A tibble: 2,575 x 8 ## symbol date open high low close volume adjusted ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GOOGL 2011-01-03 299. 303. 299. 302. 4725670 302. ## 2 GOOGL 2011-01-04 303. 303. 300. 301. 3645351 301. ## 3 GOOGL 2011-01-05 300. 305. 300. 305. 5059535 305. ## 4 GOOGL 2011-01-06 306. 310. 305. 307. 4111484 307. ## 5 GOOGL 2011-01-07 308. 309. 305. 309. 4198198 309. ## 6 GOOGL 2011-01-10 308. 308. 305. 307. 3155242 307. ## 7 GOOGL 2011-01-11 309. 310. 308. 308. 2875721 308. ## 8 GOOGL 2011-01-12 310. 310. 308. 309. 3262135 309. ## 9 GOOGL 2011-01-13 309. 310. 307. 309. 2665332 309. ## 10 GOOGL 2011-01-14 309. 312. 309. 312. 4726469 312. ## # … with 2,565 more rows Calculate monthly return of single stock: library(dplyr) # calculate monthly return of single stock tq_get(c(&quot;GOOGL&quot;), get=&quot;stock.prices&quot;) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) ## # A tibble: 123 x 2 ## date monthly_return ## &lt;date&gt; &lt;dbl&gt; ## 1 2011-01-31 -0.00660 ## 2 2011-02-28 0.0217 ## 3 2011-03-31 -0.0434 ## 4 2011-04-29 -0.0727 ## 5 2011-05-31 -0.0277 ## 6 2011-06-30 -0.0428 ## 7 2011-07-29 0.192 ## 8 2011-08-31 -0.104 ## 9 2011-09-30 -0.0479 ## 10 2011-10-31 0.151 ## # … with 113 more rows Create a line chart of the closing price for single stock: # showing closing price for single stock library(ggplot2) tq_get(c(&quot;GOOGL&quot;), get=&quot;stock.prices&quot;) %&gt;% ggplot(aes(date, close)) + geom_line() Create a line chart of the monthly return for single stock: # showing monthly return for single stock tq_get(c(&quot;GOOGL&quot;), get=&quot;stock.prices&quot;) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) %&gt;% ggplot(aes(date, monthly_return)) + geom_line() 21.5 Multiple timeseries Obtain historical data for multiple stocks (for example, GAFA): # get historical data for multiple stocks. e.g. GAFA tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) ## # A tibble: 9,953 x 8 ## symbol date open high low close volume adjusted ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GOOGL 2011-01-03 299. 303. 299. 302. 4725670 302. ## 2 GOOGL 2011-01-04 303. 303. 300. 301. 3645351 301. ## 3 GOOGL 2011-01-05 300. 305. 300. 305. 5059535 305. ## 4 GOOGL 2011-01-06 306. 310. 305. 307. 4111484 307. ## 5 GOOGL 2011-01-07 308. 309. 305. 309. 4198198 309. ## 6 GOOGL 2011-01-10 308. 308. 305. 307. 3155242 307. ## 7 GOOGL 2011-01-11 309. 310. 308. 308. 2875721 308. ## 8 GOOGL 2011-01-12 310. 310. 308. 309. 3262135 309. ## 9 GOOGL 2011-01-13 309. 310. 307. 309. 2665332 309. ## 10 GOOGL 2011-01-14 309. 312. 309. 312. 4726469 312. ## # … with 9,943 more rows Create a multiple line chart of the closing prices of multiple stocks (again, GAFA). We can show each stock in a different color on the same graph: # Create a multiple line chart of the closing prices of the four stocks, # showing each stock in a different color on the same graph. tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% ggplot(aes(date, close, color=symbol)) + geom_line() Transform the data so each stock begins at 100 and replot (Standardize the data so that we can compare timeseries): # Create a multiple line chart of the closing prices of the four stocks, # showing each stock in a different color on the same graph. # Transform the data so each stock begins at 100 and replot. tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% group_by(symbol) %&gt;% mutate(close = 100*close/first(close)) %&gt;% ggplot(aes(date, close, color=symbol)) + geom_line() Calculate monthly return of multiple stocks (again, GAFA): # calculate monthly return of multiple stocks tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% group_by(symbol) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) ## # A tibble: 476 x 3 ## # Groups: symbol [4] ## symbol date monthly_return ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 GOOGL 2011-01-31 -0.00660 ## 2 GOOGL 2011-02-28 0.0217 ## 3 GOOGL 2011-03-31 -0.0434 ## 4 GOOGL 2011-04-29 -0.0727 ## 5 GOOGL 2011-05-31 -0.0277 ## 6 GOOGL 2011-06-30 -0.0428 ## 7 GOOGL 2011-07-29 0.192 ## 8 GOOGL 2011-08-31 -0.104 ## 9 GOOGL 2011-09-30 -0.0479 ## 10 GOOGL 2011-10-31 0.151 ## # … with 466 more rows Create a multiple line chart of monthly return of the four stocks. Again, we can show each stock in a different color on the same graph: # Create a multiple line chart of monthly return of the four stocks, # showing each stock in a different color on the same graph tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% group_by(symbol) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) %&gt;% ggplot(aes(date, monthly_return, color=symbol)) + geom_line() 21.6 External Resources tidyquant CRAN doc: formal documentation on the package tidyquant Github repo: Github repository for the tidyquant package with a great README "],["import.html", "22 Importing Data 22.1 Overview 22.2 Import built-in dataset 22.3 Import local data 22.4 Import web data 22.5 Import data from database 22.6 More resources", " 22 Importing Data This chapter originated as a community contribution created by ZhangZhida This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 22.1 Overview This section covers how to import data from built-in R sources, local files, web sources and databases. 22.2 Import built-in dataset R comes with quite a lot of built-in datasets, which R users can play around with. You are probably familiar with many of the built-in datasets like iris, mtcars, beavers, dataset, etc. Since datasets are preloaded, we can manipulate them directly. To see a full list of built-in R datasets and their descriptions, please refer to The R Datasets Package. We can also run data() to view the full list. The most convenient option for viewing is ??datasets since provides a list of datasets in the Help pane. Clicking on a dataset will bring up its help file. There’s lots of important information about the sources of the data and the meaning of the variables in these help files, so be sure to check them out. Most datasets are lazy-loaded, which means that although they don’t appear as objects in the global environment, they are there when you reference them. However, for some packages, you must use data() to access the datasets, as follows: library(pgmm) data(wine) This is a common source of frustration for students: “I installed the library and loaded the package but the data’s not there!” Forewarned is forearmed. Packages that we use that fall in this category include: lawstat, pgmm, and others. (Submit a PR to add to this list.) 22.3 Import local data This section covers base R functions for reading data. For tidyverse versions (read_csv, read_delim, read_table, etc.) see the Data Import chapter of *R for Data Science.) 22.3.1 Import text file The function read.table() is the most general function for reading text files. To use this function, we need to specify how we read the file. In other words, we need to specify some basic parameters like sep, header, etc. sep represents the separator, and header is set to TRUE if we want to read the first line as the header information. Other parameters are also useful in different cases. For example, na.strings indicates strings should be regarded as NA values. df &lt;- read.table(&quot;data/MusicIcecream.csv&quot;, sep=&quot;,&quot;, header=TRUE) head(df) ## Age Music Favorite Freq ## 1 old classical bubble gum 1 ## 2 old rock bubble gum 1 ## 3 old classical coffee 3 ## 4 old rock coffee 1 ## 5 young classical bubble gum 2 ## 6 young rock bubble gum 5 22.3.2 Import CSV file A Comma-Separated Values file (CSV) is a delimited text file that uses a comma to separate values. We can easily read a CSV file with built-in R functions. The read.csv() function provides two useful parameters. One is header, which can be set to FALSE if there is no header. The other is sep, which specifies the separator. For example, we can specify the separator to be sep=\"\\t if the CSV file value is seperated by the tab character. The default value of header and sep are TRUE and \",\", respectively. read.csv2() is another function for reading CSV files. The difference between read.csv() and read.csv2 is that, the former uses the tab \"\\t\" as the separator, while the latter one uses the semicolon \";\". This serves as an easy shortcut for different CSV formats used in different regions. Let’s see an example on reading a standard CSV file: df &lt;- read.csv(&quot;data/MusicIcecream.csv&quot;) head(df) ## Age Music Favorite Freq ## 1 old classical bubble gum 1 ## 2 old rock bubble gum 1 ## 3 old classical coffee 3 ## 4 old rock coffee 1 ## 5 young classical bubble gum 2 ## 6 young rock bubble gum 5 A small note while reading multiple files: let R know your current directory by using setwd(). Then, you can read any file in this directory by directly using the name of the file, without specifying the location. 22.3.3 Import JSON file A JSON file is a file that stores simple data structures and objects in JavaScript Object Notation (JSON) format, which is a standard data interchange format. For example, {\"name\":\"Vince\", \"age\":23, \"city\":\"New York\"} is an object with JSON format. In recent years, JSON has become the mainstream format to transfer data on websites. To read a JSON file, we can use the jsonlite package. The jsonlite package is a JSON parser/generator optimized for the web. Its main strength is that it implements a bidirectional mapping between JSON data and the most important R data types. In the example below, the argument simplifyDataFrame = TRUE will directly transform a list of JSON objects into a dataframe. If you want to know more about the arguments simplifyVector and simplifyMatrix, which provide flexible control on other R data formats to transform to, please refer to Getting started with JSON and jsonlite. library(jsonlite) # read JSON data raw_json_data &lt;- fromJSON(txt = &quot;data/WaterConsumptionInNYC.json&quot;, simplifyDataFrame = TRUE) # transform JSON to Data Frame df &lt;- as.data.frame(raw_json_data) head(df) ## new_york_city_population nyc_consumption_million_gallons_per_day ## 1 7102100 1512 ## 2 7071639 1506 ## 3 7089241 1309 ## 4 7109105 1382 ## 5 7181224 1424 ## 6 7234514 1465 ## per_capita_gallons_per_person_per_day year ## 1 213 1979 ## 2 213 1980 ## 3 185 1981 ## 4 194 1982 ## 5 198 1983 ## 6 203 1984 22.4 Import web data 22.4.1 Read a data file directly into the workspace Let’s take the example of Water Consumption In The New York City, which is on the NYC Open Data website. We can import data from a URL just as we do with local data files. library(tidyverse) # specify the URL link to the data source url &lt;- &quot;https://data.cityofnewyork.us/api/views/ia2d-e54m/rows.csv&quot; # read the URL df &lt;- read_csv(url) head(df) ## # A tibble: 6 x 4 ## Year `New York City Popu… `NYC Consumption(Million … `Per Capita(Gallons per… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1979 7102100 1512 213 ## 2 1980 7071639 1506 213 ## 3 1981 7089241 1309 185 ## 4 1982 7109105 1382 194 ## 5 1983 7181224 1424 198 ## 6 1984 7234514 1465 203 22.4.2 Read data from an API The best option here is to look for a package that has set this up for you, such as WHO, atus, and many others. Need ideas? These R packages import sports, weather, stock data and more, is a great place to start looking for such packages. If such a package does not exist for your data, use the httr to facilitate the API calls. The RStudio webinar Extracting Data from the Web Part I is an excellent resource for learning httr as is Sharon Machlis’s article, Get API data with R: No R package for the API you want? It’s easy to write your own function with the httr and jsonlite packages. 22.4.3 Scrape an HTML table using rvest Sometimes we wish to import data that appears as an HTML table on a web page. It might be a little messy, so best to first check if there’s another means for importing the data before moving forward. If not, rvest makes the process as painless as possible. Here’s a simple example. Suppose we wish to work with the borough data found on Wikipedia’s Boroughs of New York City page. First we read the page, find the tables, and then parse them with html_table: library(tidyverse) library(rvest) nyctables &lt;- read_html(&quot;https://en.wikipedia.org/wiki/Boroughs_of_New_York_City&quot;) %&gt;% html_nodes(&quot;table&quot;) %&gt;% html_table(fill = TRUE) nyctables is a list with three elements, one for each table on the page. Next we can check each list item until we find what we want, consulting the original web page to get a sense of where our table is located. (There are other methods for identifying what you need from a web page in more complex situations. See Additional Resources below.) It turns out that the table we want is the first list element: mytable &lt;- nyctables[[1]] head(mytable, 3) ## # A tibble: 3 x 9 ## `New York City&#39;s f… `New York City&#39;s f… `New York City&#39;s f… `New York City&#39;s … ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Jurisdiction Jurisdiction Population GDP ## 2 Borough County Estimate (2019) billions(2012 US$) ## 3 The Bronx Bronx 1,418,207 42.695 ## # … with 5 more variables: ## # New York City&#39;s five boroughs.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:&quot;[ &quot;}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:&quot; ]&quot;}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}.mw-parser-output .infobox .navbar{font-size:100%}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}vte &lt;chr&gt;, ## # New York City&#39;s five boroughs.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:&quot;[ &quot;}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:&quot; ]&quot;}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}.mw-parser-output .infobox .navbar{font-size:100%}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}vte.1 &lt;chr&gt;, ## # New York City&#39;s five boroughs.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:&quot;[ &quot;}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:&quot; ]&quot;}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}.mw-parser-output .infobox .navbar{font-size:100%}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}vte.2 &lt;chr&gt;, ## # New York City&#39;s five boroughs.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:&quot;[ &quot;}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:&quot; ]&quot;}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}.mw-parser-output .infobox .navbar{font-size:100%}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}vte.3 &lt;chr&gt;, ## # New York City&#39;s five boroughs.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:&quot;[ &quot;}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:&quot; ]&quot;}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}.mw-parser-output .infobox .navbar{font-size:100%}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}vte.4 &lt;chr&gt; We can see that the column names are all the same due to the merged header in the original. We’ll fix the column names and remove the rows we don’t need: colnames(mytable) &lt;- c(&quot;borough&quot;, &quot;county&quot;, &quot;population&quot;, &quot;gdp_total&quot;, &quot;gdp_per_capita&quot;, &quot;land_sq_miles&quot;, &quot;land_sq_km&quot;, &quot;density_sq_miles&quot;, &quot;density_sq_km&quot;) # remove unneeded rows mytable &lt;- mytable %&gt;% slice(-c(1, 2, 10)) # convert character to numeric data where appropriate mytable &lt;- mytable %&gt;% mutate_at(vars(population:density_sq_km), parse_number) Now we’re good to go. Let’s draw a plot! options(scipen = 999) # turn off scientific notation mytable %&gt;% slice(1:5) %&gt;% select(borough, gdp_per_capita, land_sq_miles, population) %&gt;% gather(var, value, -borough) %&gt;% ggplot(aes(value, fct_reorder2(borough, var==&quot;gdp_per_capita&quot;, value, .desc = FALSE), color = borough)) + geom_point() + ylab(&quot;&quot;) + facet_wrap(~var, ncol = 1, scales = &quot;free_x&quot;) + guides(color = FALSE) Additional Resources Excellent webinar from RStudio on using rvest – covers how to use the structure of the HTML and CSS on the page to scrape the information that you need, as well as using additional rvest functions such as html_text(), html_name(), html_attrs(), html_children(), etc. 22.5 Import data from database R provides packages to manipulate data from relational databases like PostgreSQL, MySQL, etc. One of those packages is odbc package, which is one database interface for communication between R and relational database management systems. More resources on package: odbc. Before we connect to a local database, we must satisfy the requirement of the ODBC driver, through which our R package can communicate with the database. To get help on how to install ODBC driver on systems like Windows, Linux, MacOS, please refer to this document: Install ODBC Driver. After we installed the ODBC driver, with odbc and DBI packages, we are able to manipulate the database. To read a table in the database, we usually take steps as follows. First, we build the connection to the database using dbConnect() function. Then, we can do some exploratory operations like listing all tables in the database. To query the data we want, we can send a SQL query into the database. Then we can retrieve the desired data and dfFetch() provides control on how many records to retrieve at a time. Finally, we finish reading and close the connection. library(odbc) library(DBI) # build connection with database con &lt;- dbConnect(odbc::odbc(), driver = &quot;PostgreSQL Driver&quot;, database = &quot;test_db&quot;, uid = &quot;postgres&quot;, pwd = &quot;password&quot;, host = &quot;localhost&quot;, port = 5432) # list all tables in the test_db database dbListTables(con) # read table test_table into Data Frame data &lt;- dbReadTable(con, &quot;test_table&quot;) # write an R Data Frame object to an SQL table # here we write the built-in data mtcars to a new_table in DB data &lt;- dbWriteTable(con, &quot;new_table&quot;, mtcars) # SQL query result &lt;- dbSendQuery(con, &quot;SELECT * FROM test_table&quot;) # Retrieve the first 10 results first_10 &lt;- dbFetch(result, n = 10) # Retrieve the rest of the results rest &lt;- dbFetch(result) # close the connection dbDisconnect(con) 22.6 More resources Import local file: This R Data Import Tutorial Is Everything You Need Import JSON file: Getting started with JSON and jsonlite Import web data: The RCurl Package Import database file Databases using R Documentation on odbc package odbc Install ODBC Driver On Your System Install ODBC Driver "],["tidy.html", "23 Walkthrough: Tidy Data &amp; dplyr 23.1 Overview 23.2 Installing packages 23.3 Viewing the data 23.4 What is Tidy data? 23.5 Tibbles 23.6 Test for missing values 23.7 Recode the missing values 23.8 Data wrangling verbs 23.9 Rename 23.10 Select 23.11 Mutate 23.12 Filter 23.13 Arrange 23.14 Summarize &amp; Group By 23.15 Pipe Operator 23.16 Tidying the transformed data 23.17 Helpful Links", " 23 Walkthrough: Tidy Data &amp; dplyr This chapter originated as a community contribution created by akshatapatel This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 23.1 Overview This example goes through some work with the biopsy dataset using dplyr functions to get to a tidy dataset. 23.1.1 Packages dplyr MASS tidyr 23.2 Installing packages Write the following statements in the console: install.packages('dplyr') install.packages('ggplot2') install.packages('tidyr') install.packages('MASS') Note: The first three packages are a part of the tidyverse, a collection of helpful packages in R, which can all be installed using install.packages('tidyverse'). dplyr is used for data wrangling and data transformation in data frames. The “d” in “dplyr” stands for “data frames” which is the most-used data type for storing datasets in R. 23.3 Viewing the data Let’s start with loading the package so we can get the data as a dataframe: #loading the dplyr library library(dplyr) #loading data from MASS:biopsy data(biopsy, package = &quot;MASS&quot;) class(biopsy) ## [1] &quot;data.frame&quot; #glimpse is a part of the dplyr package glimpse(biopsy) ## Rows: 699 ## Columns: 11 ## $ ID &lt;chr&gt; &quot;1000025&quot;, &quot;1002945&quot;, &quot;1015425&quot;, &quot;1016277&quot;, &quot;1017023&quot;, &quot;1017122&quot;… ## $ V1 &lt;int&gt; 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, 8, 7, 4, 4, 10, 6, 7, … ## $ V2 &lt;int&gt; 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1, 7, 4, 1, 1, 7, 1, 3, … ## $ V3 &lt;int&gt; 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1, 5, 6, 1, 1, 7, 1, 2, … ## $ V4 &lt;int&gt; 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, 10, 4, 1, 1, 6, 1, 10,… ## $ V5 &lt;int&gt; 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 7, 6, 2, 2, 4, 2, 5, 6… ## $ V6 &lt;int&gt; 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, 3, 9, 1, 1, 1, 10, 1, … ## $ V7 &lt;int&gt; 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, 5, 4, 2, 3, 4, 3, 5, 7… ## $ V8 &lt;int&gt; 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, 5, 3, 1, 1, 1, 1, 4, 1… ## $ V9 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 4, 1… ## $ class &lt;fct&gt; benign, benign, benign, benign, benign, malignant, benign, benig… head(biopsy) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## 1 1000025 5 1 1 1 2 1 3 1 1 benign ## 2 1002945 5 4 4 5 7 10 3 2 1 benign ## 3 1015425 3 1 1 1 2 2 3 1 1 benign ## 4 1016277 6 8 8 1 3 4 3 7 1 benign ## 5 1017023 4 1 1 3 2 1 3 1 1 benign ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant 23.4 What is Tidy data? What does it mean for your data to be tidy? Tidy data has a standardized format and it is a consistent way to organize your data in R. Here’s the definition of Tidy Data given by Hadley Wickham: A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: Each variable forms a column. Each observation forms a row. Each observational unit forms a value in the table. See r4ds on tidy data for more info. What are the advantages of tidy data? Uniformity : It is easier to learn the tools that work with the data because they have a consistent way of storing data. Most built-in R functions work with vectors of values. Thus, having variables as columns/vectors allows R’s vectorized nature to shine. Can you observe and tell why this data is messy? The names of the columns such as V1, V2 are not intuitive in what they contain; good sign it is untidy. They are not different variables, but are values of a common variable. Now, we will see the how to transform our data using dplyr functions and then look at how to tidy our transformed data. 23.5 Tibbles A tibble is a modern re-imagining of the data frame. It is particularly useful for large datasets because it only prints the first few rows. It helps you confront problems early, leading to cleaner code. # Converting a df to a tibble biopsy &lt;- tbl_df(biopsy) biopsy ## # A tibble: 699 x 11 ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; ## 1 1000025 5 1 1 1 2 1 3 1 1 benign ## 2 1002945 5 4 4 5 7 10 3 2 1 benign ## 3 1015425 3 1 1 1 2 2 3 1 1 benign ## 4 1016277 6 8 8 1 3 4 3 7 1 benign ## 5 1017023 4 1 1 3 2 1 3 1 1 benign ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant ## 7 1018099 1 1 1 1 2 10 3 1 1 benign ## 8 1018561 2 1 2 1 2 1 3 1 1 benign ## 9 1033078 2 1 1 1 2 1 1 1 5 benign ## 10 1033078 4 2 1 1 2 1 2 1 1 benign ## # … with 689 more rows 23.6 Test for missing values # Number of missing values in each column in the data frame colSums(is.na(biopsy)) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## 0 0 0 0 0 0 16 0 0 0 0 The dataset contains missing values which need to be addressed. 23.7 Recode the missing values One way to deal with missing values is to recode them with the average of all the other values in that column: #change all the NAs to mean of the column biopsy$V6[is.na(biopsy$V6)] &lt;- mean(biopsy$V6, na.rm = TRUE) colSums(is.na(biopsy)) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## 0 0 0 0 0 0 0 0 0 0 0 See our chapter on time series with missing data for more info about dealing with missing data. 23.8 Data wrangling verbs Here are the most commonly used functions that help wrangle and summarize data: Rename Select Mutate Filter Arrange Summarize Group_by Select and mutate functions manipulate the variable (the columns of the data frame). Filter and arrange functions manipulate the observations (the rows of the data) ,whereas the summarize function manipulates groups of observations. All the dplyr functions work on a copy of the data and return a modified copy. They do not change the original data frame. If we want to access the results afterwards, we need to save the modified copy. 23.9 Rename The names of the columns in our biopsy data are very vague and do not give us the meaning of the values in that column. We need to change the names of the column so that the viewer gets a sense of the values they’re referring to. rename(biopsy, thickness = V1,cell_size = V2, cell_shape = V3, marg_adhesion = V4, epithelial_cell_size = V5, bare_nuclei = V6, chromatin = V7, norm_nucleoli = V8, mitoses = V9) ## # A tibble: 699 x 11 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000025 5 1 1 1 2 ## 2 1002945 5 4 4 5 7 ## 3 1015425 3 1 1 1 2 ## 4 1016277 6 8 8 1 3 ## 5 1017023 4 1 1 3 2 ## 6 1017122 8 10 10 8 7 ## 7 1018099 1 1 1 1 2 ## 8 1018561 2 1 2 1 2 ## 9 1033078 2 1 1 1 2 ## 10 1033078 4 2 1 1 2 ## # … with 689 more rows, and 5 more variables: bare_nuclei &lt;dbl&gt;, ## # chromatin &lt;int&gt;, norm_nucleoli &lt;int&gt;, mitoses &lt;int&gt;, class &lt;fct&gt; The tibble shown above is not saved and cannot be used further. To use it afterwards we save it as a new tibble: #saving the rename function output biopsy_new&lt;-rename(biopsy, thickness = V1,cell_size = V2, cell_shape = V3, marg_adhesion = V4, epithelial_cell_size = V5, bare_nuclei = V6, chromatin = V7, norm_nucleoli = V8, mitoses = V9) head(biopsy_new,5) ## # A tibble: 5 x 11 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000025 5 1 1 1 2 ## 2 1002945 5 4 4 5 7 ## 3 1015425 3 1 1 1 2 ## 4 1016277 6 8 8 1 3 ## 5 1017023 4 1 1 3 2 ## # … with 5 more variables: bare_nuclei &lt;dbl&gt;, chromatin &lt;int&gt;, ## # norm_nucleoli &lt;int&gt;, mitoses &lt;int&gt;, class &lt;fct&gt; The biopsy_new data frame can now be used for further manipulation. 23.10 Select Select returns a subset of the data. Specifically, only the columns that are specified are included. In the biopsy data, we do not require the variables “chromatin” and “mitoses”. So, let’s drop them using a minus sign: #selecting all except the columns chromatin and mitoses biopsy_new&lt;-select(biopsy_new,-chromatin,-mitoses) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000025 5 1 1 1 2 ## 2 1002945 5 4 4 5 7 ## 3 1015425 3 1 1 1 2 ## 4 1016277 6 8 8 1 3 ## 5 1017023 4 1 1 3 2 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, class &lt;fct&gt; 23.11 Mutate The mutate function computes new variables from the already existing variables and adds them to the dataset. It gives information that the data already contained but was never displayed. The “V6” variable contains the values of the bare nucleus from 1.00 to 10.00. If we wish to normalize the variable, we can use the mutate function: #normalize the bare nuclei values maximum_bare_nuclei&lt;-max(biopsy_new$bare_nuclei,na.rm=TRUE) biopsy_new&lt;-mutate(biopsy_new,bare_nuclei=bare_nuclei/maximum_bare_nuclei) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000025 5 1 1 1 2 ## 2 1002945 5 4 4 5 7 ## 3 1015425 3 1 1 1 2 ## 4 1016277 6 8 8 1 3 ## 5 1017023 4 1 1 3 2 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, class &lt;fct&gt; 23.12 Filter Filter is the row-equivalent function of select; it returns a modified copy that contains only certain rows. This function filters rows based on the content and the conditions supplied in its argument. The filter function takes the data frame as the first argument. The next argument contains one or more logical tests. The rows/observations that pass these logical tests are returned in the result of the filter function. For our example, we only want the data of those tumor cells that have clump thickness greater than six as most of the malign tumors have this thickness looking at a plot of clump thickness vs tumor cell size grouped by class: library(ggplot2) ggplot(biopsy_new)+ geom_point(aes(x=thickness,y=cell_size,color=class))+ ggtitle(&quot;Plot of Clump Thickness Vs Tumor Cell Size&quot;) #normalize the bare nuclei values biopsy_new&lt;-filter(biopsy_new,thickness&gt;5.5) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1016277 6 8 8 1 3 ## 2 1017122 8 10 10 8 7 ## 3 1044572 8 7 5 10 7 ## 4 1047630 7 4 6 4 6 ## 5 1050670 10 7 7 6 4 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, class &lt;fct&gt; 23.13 Arrange Arrange reorders the rows of the data based on their contents in the ascending order by default. The doctors would want to view the data in the order of the cell size of the tumor. #arrange in the order of V2:cell size arrange(biopsy_new,cell_size) ## # A tibble: 186 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1050718 6 1 1 1 2 ## 2 1204898 6 1 1 1 2 ## 3 1223967 6 1 3 1 2 ## 4 543558 6 1 3 1 4 ## 5 63375 9 1 2 6 4 ## 6 752904 10 1 1 1 2 ## 7 1276091 6 1 1 3 2 ## 8 1238777 6 1 1 3 2 ## 9 1257608 6 1 1 1 1 ## 10 1224565 6 1 1 1 2 ## # … with 176 more rows, and 3 more variables: bare_nuclei &lt;dbl&gt;, ## # norm_nucleoli &lt;int&gt;, class &lt;fct&gt; This shows the data in increasing order of the cell size. To arrange the rows in decreasing order of V2, we add the desc() function to the variable before passing it to arrange. #arrange in the order of V2:cell size in decreasing order arrange(biopsy_new,desc(cell_size)) ## # A tibble: 186 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1017122 8 10 10 8 7 ## 2 1080185 10 10 10 8 6 ## 3 1100524 6 10 10 2 8 ## 4 1103608 10 10 10 4 8 ## 5 1112209 8 10 10 1 3 ## 6 1116116 9 10 10 1 10 ## 7 1123061 6 10 2 8 10 ## 8 1168736 10 10 10 10 10 ## 9 1170419 10 10 10 8 2 ## 10 1173216 10 10 10 3 10 ## # … with 176 more rows, and 3 more variables: bare_nuclei &lt;dbl&gt;, ## # norm_nucleoli &lt;int&gt;, class &lt;fct&gt; As you can see, there are a number of rows with the same value of V2:cell_size. To break the tie, you can add another variable to be used for ordering when the first variable has the same value. Here, we use the tie breaker as the order of variable V3: by cell shape and by ID: #arrange in the order of V2:cell size biopsy_new&lt;-arrange(biopsy_new,desc(cell_size),desc(cell_shape),ID) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1017122 8 10 10 8 7 ## 2 1073960 10 10 10 10 6 ## 3 1080185 10 10 10 8 6 ## 4 1100524 6 10 10 2 8 ## 5 1100524 6 10 10 2 8 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, class &lt;fct&gt; 23.14 Summarize &amp; Group By Summarize uses the data to create a new data frame with the summary statistics such as minimum, maximum, average, and so on. These statistical functions must be aggregate functions which take a vector of values as input and output a single value. The group_by function groups the data by the values of the variables. This, along with summarize, makes observations about groups of rows of the dataset. The doctors would want to see the maximum cell size and the thickness for each of the classes: benign and malignant. This can be done by grouping the data by class and finding the maximum of the required variables: biopsy_grouped &lt;- group_by(biopsy_new,class) summarize(biopsy_grouped, max(thickness), mean(cell_size), var(norm_nucleoli)) ## # A tibble: 2 x 4 ## class `max(thickness)` `mean(cell_size)` `var(norm_nucleoli)` ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 benign 8 2.67 5.93 ## 2 malignant 10 6.73 11.3 23.15 Pipe Operator What if we want to use the various data wrangling verbs together? This could be done by saving the result of each wrangling function in a new variable and using it for the next function as we did above. However, this is not recommended as: It requires extra typing and longer code. Unnecessary space is used up to save the various variables. If the data is large, this method slows down the analysis. The pipe operator can be used instead for the same purpose. The operator is placed between and object and the function. The pipe takes the object on its left and passes it as the first argument to the function to its right. The pipe operator is a part of the magrittr package. However, this package need not be loaded as the dplyr package makes life simpler and imports the pipe operator for us: biopsy_grouped &lt;- biopsy_new %&gt;% group_by(class) %&gt;% summarize(max(thickness),mean(cell_size),var(norm_nucleoli)) head(biopsy_grouped) ## # A tibble: 2 x 4 ## class `max(thickness)` `mean(cell_size)` `var(norm_nucleoli)` ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 benign 8 2.67 5.93 ## 2 malignant 10 6.73 11.3 23.16 Tidying the transformed data Have a look again at the messy data: # Messy Data head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell_size ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1017122 8 10 10 8 7 ## 2 1073960 10 10 10 10 6 ## 3 1080185 10 10 10 8 6 ## 4 1100524 6 10 10 2 8 ## 5 1100524 6 10 10 2 8 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, class &lt;fct&gt; Planning is required to decide which columns we need to keep unchanged, which ones to change, and what names are to be given to the new columns. The columns to keep are the ones that are already tidy. The ones to change are the ones that aren’t true variables but in fact levels of another variable. So, the ID and class columns are already tidy. These are kept as is. The columns V1:thickness, V2:cell_size, V3:cell_shape, V4:marg_adhesion, V5:epithelial_cell_size, V6:bare_nuclei, and V8:norm_nucleoli are not true variables but values of the variable Tumor_attributes. We can fix this with tidyr::gather(), which is used to convert data from messy to tidy. The gather function takes the data frame which we want to tidy as input. The next two parameters are the names of the key and the value columns in the tidy dataset. In our example, key=‘Tumor_Atrributes’ and value=‘Score’. You can also specify the columns that you do not want to be tidied, i.e. ID and class: #Tidy Data library(tidyr) tidy_df &lt;- biopsy_new %&gt;% gather(key = &quot;Tumor_Attributes&quot;, value = &quot;Score&quot;, -ID, -class) tidy_df ## # A tibble: 1,302 x 4 ## ID class Tumor_Attributes Score ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1017122 malignant thickness 8 ## 2 1073960 malignant thickness 10 ## 3 1080185 malignant thickness 10 ## 4 1100524 malignant thickness 6 ## 5 1100524 malignant thickness 6 ## 6 1103608 malignant thickness 10 ## 7 1112209 malignant thickness 8 ## 8 1116116 malignant thickness 9 ## 9 1116116 malignant thickness 9 ## 10 1168736 malignant thickness 10 ## # … with 1,292 more rows 23.17 Helpful Links r4ds on tidy data: It is always best to learn from the source, so a textbook written by Hadley Wickham is perfect. DataCamp dplyr course: This course covers the different fucntions in dplyr and how they manipulate data. "],["missing.html", "24 Missing Data 24.1 Overview 24.2 tl;dr 24.3 What are NAs? 24.4 Types of Missing Data 24.5 Missing Patterns 24.6 Handling Missing values 24.7 External Resources", " 24 Missing Data This chapter originated as a community contribution created by ujjwal95 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 24.1 Overview This section covers what kinds of missing values are encountered in data and how to handle them. 24.2 tl;dr It’s difficult to handle missing data! If your data has some missing values, which it most likely will, you can either remove such rows, such columns, or impute them. 24.3 What are NAs? Whenever data in some row or column in your data is missing, it comes up as NA. Let’s have a look at some data, shall we? Name Sex Age E_mail Education Income Melissa Female 27 NA NA 10000 Peter NA NA peter.parker@esu.edu NA 7500 Aang Male 110 aang@avatars.com NA 1000 Drake Male NA NA NA 50000 Bruce NA 45 bruce.wayne@wayne.org NA 10000000 Gwen Female 28 gwen.stacy@esu.edu NA 23000 Ash Male NA ash.ketchum@pokemon.com NA NA NA NA NA NA NA NA We can see the number of NAs in each column and row: colSums(is.na(data)) ## Name Sex Age E_mail Education Income ## 1 3 4 3 8 2 rowSums(is.na(data)) ## [1] 2 3 1 3 2 1 3 6 We can also see the ratio of the number of NAs in each column and row: colMeans(is.na(data)) ## Name Sex Age E_mail Education Income ## 0.125 0.375 0.500 0.375 1.000 0.250 rowMeans(is.na(data)) ## [1] 0.3333333 0.5000000 0.1666667 0.5000000 0.3333333 0.1666667 0.5000000 ## [8] 1.0000000 24.4 Types of Missing Data Missing Completely at Random (MCAR): These are missing data values which are not related to any missing or non-missing values in other columns in the data. Missing at Random (MAR): These are missing data which are linked to one or more groups in the data. The great thing about MAR is that MAR values can be predicted using other features. For example, it may be observed that people older than 70 generally do not enter their income. Most of the data we encounter is MAR. Missing Not at Random (MNAR): Generally, data which is not MAR is MNAR. A big problem is that there is not a huge distinction between MAR and MNAR. We generally assume MAR, unless otherwise known by an outside source. 24.5 Missing Patterns 24.5.1 Missing Patterns by columns We can see some missing patterns in data by columns, ggplot(tidy_names, aes(x = key, y = fct_rev(Name), fill = missing)) + geom_tile(color = &quot;white&quot;) + ggtitle(&quot;Names dataset with NAs added&quot;) + scale_fill_viridis_d() + theme_bw() And we can also add a scale to check the numerical values available in the dataset and look for any trends: library(scales) # for legend # Select columns having numeric values numeric_col_names &lt;- colnames(select_if(data, is.numeric)) filtered_for_numeric &lt;- tidy_names[tidy_names$key %in% numeric_col_names,] filtered_for_numeric$value &lt;- as.integer(filtered_for_numeric$value) # Use label=comma to remove scientific notation ggplot(data = filtered_for_numeric, aes(x = key, y = fct_rev(Name), fill = value)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient(low = &quot;grey80&quot;, high = &quot;red&quot;, na.value = &quot;black&quot;, label=comma) + theme_bw() Can you see the problem with the above graph? Notice that the scale is for all the variables, hence it cannot show the variable level differences! To solve this problem, we can standardize the variables: filtered_for_numeric &lt;- filtered_for_numeric %&gt;% group_by(key) %&gt;% mutate(Std = (value-mean(value, na.rm = TRUE))/sd(value, na.rm = TRUE)) %&gt;% ungroup() ggplot(filtered_for_numeric, aes(x = key, y = fct_rev(Name), fill = Std)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient2(low = &quot;blue&quot;, mid = &quot;white&quot;, high =&quot;yellow&quot;, na.value = &quot;black&quot;) + theme_bw() Now, we can see the missing trends better! Let us sort them by the number missing by each row and column: # convert missing to numeric so it can be summed up filtered_for_numeric &lt;- filtered_for_numeric %&gt;% mutate(missing2 = ifelse(missing == &quot;yes&quot;, 1, 0)) ggplot(filtered_for_numeric, aes(x = fct_reorder(key, -missing2, sum), y = fct_reorder(Name, -missing2, sum), fill = Std)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient2(low = &quot;blue&quot;, mid = &quot;white&quot;, high =&quot;yellow&quot;, na.value = &quot;black&quot;) + theme_bw() 24.5.2 Missing Patterns by rows We can also see missing patterns in data by rows using the mi package: library(mi) x &lt;- missing_data.frame(data) ## NOTE: In the following pairs of variables, the missingness pattern of the second is a subset of the first. ## Please verify whether they are in fact logically distinct variables. ## [,1] [,2] ## [1,] &quot;Age&quot; &quot;Income&quot; ## [2,] &quot;Education&quot; &quot;Income&quot; image(x) Did you notice that the Education variable has been skipped? That is because the whole column is missing. Let us try to see some patterns in the missing data: x@patterns ## [1] E_mail Sex, Age ## [3] nothing Age, E_mail ## [5] Sex nothing ## [7] Age, Income Name, Sex, Age, E_mail, Income ## 7 Levels: nothing E_mail Sex Sex, Age Age, E_mail ... Name, Sex, Age, E_mail, Income levels(x@patterns) ## [1] &quot;nothing&quot; &quot;E_mail&quot; ## [3] &quot;Sex&quot; &quot;Sex, Age&quot; ## [5] &quot;Age, E_mail&quot; &quot;Age, Income&quot; ## [7] &quot;Name, Sex, Age, E_mail, Income&quot; summary(x@patterns) ## nothing E_mail ## 2 1 ## Sex Sex, Age ## 1 1 ## Age, E_mail Age, Income ## 1 1 ## Name, Sex, Age, E_mail, Income ## 1 The extracat package is no longer on CRAN. We can visualize missing patterns using the visna (VISualize NA) function in the extracat package: extracat::visna(data) Here, the rows represent a missing pattern and the columns represent the column level missing values. The advantage of this graph is that it shows you only the missing patterns available in the data, not all the possible combinations of data (which will be 2^6 = 64), so that you can focus on the pattern in the data itself. We can sort the graph by most to least common missing pattern (i.e., by row): extracat::visna(data, sort = &quot;r&quot;) Or, by most to least missing values (i.e., by column): extracat::visna(data, sort = &quot;c&quot;) Or, by both row and column sort: extracat::visna(data, sort = &quot;b&quot;) 24.6 Handling Missing values There are multiple methods to deal with missing values. 24.6.1 Deletion of rows containing NAs Often we would delete rows that contain NAs when we are handling Missing Completely at Random data. We can delete the rows having NAs as below: na.omit(data) ## [1] Name Sex Age E_mail Education Income ## &lt;0 rows&gt; (or 0-length row.names) This method is called list-wise deletion. It removes all the rows having NAs. But we can see that the Education column is only NAs, so we can remove that column itself: edu_data &lt;- data[, !(colnames(data) %in% c(&quot;Education&quot;))] na.omit(edu_data) ## Name Sex Age E_mail Income ## 3 Aang Male 110 aang@avatars.com 1000 ## 6 Gwen Female 28 gwen.stacy@esu.edu 23000 Another method is pair-wise deletion, in which only the rows having missing values in the variable of interest are removed. 24.6.2 Imputation Techniques Imputation means to replace missing data with substituted values. These techniques are generally used with MAR data. 24.6.2.1 Mean/Median/Mode Imputation We can replace missing data in continuous variables with their mean/median and missing data in discrete/categorical variables with their mode. Either we can replace all the values in the missing variable directly, for example, if “Income” has a median of 15000, we can replace all the missing values in “Income” with 15000, in a technique known as Generalized Imputation. Or, we can replace all values on a similar case basis. For example, we notice that the income of people with Age &gt; 60 is much less than those with Age &lt; 60, on average, and hence we calculate the median income of each Age group separately, and impute values separately for each group. The problem with these methods is that they disturb the underlying distribution of the data. 24.6.3 Model Imputation There are several model based approaches for imputation of data, and several packages, like mice, Hmisc, and Amelia II, which deal with this. For more info, checkout this blog on DataScience+ about imputing missing data with the R mice package. 24.7 External Resources Missing Data Imputation - A PDF by the Stats Department at Columbia University regarding Missing-data Imputation How to deal with missing data in R - A 2 min read blogpost in missing data handling in R Imputing Missing Data in R; MICE package - A 9 min read on how to use the mice package to impute missing values in R How to Handle Missing Data - A great blogpost on how to handle missing data. "],["outliers.html", "25 Outliers 25.1 Overview 25.2 tl;dr 25.3 What are outliers? 25.4 Types of Outliers 25.5 Handling Outliers 25.6 External Resources", " 25 Outliers This chapter originated as a community contribution created by kiransaini This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 25.1 Overview This section covers what types of outliers are encountered in data and how to handle them. 25.2 tl;dr I want to see my outliers! Outliers are difficult to spot because judging a datapoint as an outlier depends on the data or model with which they are compared. It is important to detect outliers because they can distort predictions and affect the accuracy of the model. 25.3 What are outliers? Outliers are noticeably far from the bulk of the data. They can be errors, genuine extreme values, rare values, unusual values, cases of special interest, or data from another source. Outliers on individual variables can be spotted using boxplots and bivariate outliers can be spotted using scatterplots. There can also be higher dimensional outliers that are not outliers in lower dimensions. It is worth identifying outliers for a number of reasons. Bad outliers should always be corrected and many statistical methods may work poorly in presence of outliers, but genuine outlying values can be interesting in their own right. Let’s have a look at the outliers of the ‘carat’ variable in the diamonds dataset: 25.4 Types of Outliers 25.4.1 Univariate Outliers Univariate outliers are outlying along one dimension. The best-known approach for an initial look at the data is to use boxplots. Tukey suggests marking individual cases as outliers if they are more than 1.5 IQR (the interquartile range) outside the hinges (basically the quartiles). Outliers may change if they are grouped by another variable. Let’s have a look at outliers on the Sepal Width variable in the iris dataset, both when the data is grouped by Species and when it is not. The outliers are clearly different: p &lt;- ggplot(iris, aes(x=Species, y=Sepal.Width)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Boxplot for Sepal Width grouped by Species in iris dataset&quot;) p p &lt;- ggplot(iris, aes(y=Sepal.Width)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Boxplot for Sepal Width in iris dataset&quot;) p 25.4.2 Multivariate Outliers Multivariate outliers are outlying along more than one dimension. Scatterplots and parallel coordinate plots are useful for visualizing multivariate outliers. You could regard points as outliers that are far from the mass of the data, or you could regard points as outliers that do not fit the smooth model well. Some points are outliers on both criteria. Let’s have a look at outliers on the Petal Length and Sepal Width variables in the iris dataset. We can clearly see an outlier which is far from the mass of the data (lower left): ggplot(iris, aes(x=Sepal.Width, y=Petal.Length)) + geom_point() + ggtitle(&quot;Scatterplot for Petal Length vs Sepal Width in iris dataset&quot;) Let’s have a look at outliers on the Petal Length and Petal Width variables in the iris dataset by fitting a smooth model. Here the outliers are the points that do not fit the smooth model: ggplot(iris, aes(x=Petal.Width, y=Petal.Length)) + geom_point() + geom_smooth() + geom_density2d(col=&quot;red&quot;,bins=4) + ggtitle(&quot;Scatterplot for Petal Length vs Petal Width in iris dataset&quot;) Lets have a look at outliers in the diamond dataset using a parallel coordinate plot. We can see an outlier on the carat, cut, color, and clarity variables that is not an outlier on individual variables: library(GGally) ggparcoord(diamonds[1:1000,], columns=1:5, scale=&quot;uniminmax&quot;, alpha=0.8) + ggtitle(&quot;Parallel coordinate plot of diamonds dataset&quot;) 25.4.3 Categorical Outliers Outliers can be rare on a categorical scale. Certain combinations of categories are rare or should not occur at all. Fluctuation diagrams can be used to find such outliers. We can see rare cases in the HairEyeColor dataset: library(datasets) library(extracat) fluctile(HairEyeColor) 25.5 Handling Outliers Identifying outliers using plots and fitting models is relatively easy compared to what to do after identifying the outliers. Outliers can be rare cases, unusual values, or genuine errors. Genuine errors must be corrected if possible or else they must be removed. Imputation of outliers is complicated and appropriate background knowledge is required. A strategy for dealing with outliers is as follows Plot the one-dimensional distributions of the variables using boxplots. Examine any extreme outliers to see if they are rare values or errors and decide if they should be removed or imputed. For outliers which are extreme on one dimension, examine their values on other dimensions to decide whether they should be discarded or not. Discard values that are outliers on more than one dimension. Consider cases which are outliers in a higher dimensions but not in lower dimensions. Decide whether they are errors or not and consider discarding or imputing the errors. Plot boxplots and parallel coordinate plots by using grouping on a variable to find outliers in subsets of the data. 25.5.1 Not informative Consider the diamonds dataset. Let’s have a look at the width (y) and depth (z) variables: ggplot(diamonds, aes(y=y)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;#9B3535&quot;) + ggtitle(&quot;Ouliers on width variable in diamonds dataset&quot;) ggplot(diamonds, aes(y=z)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;#9B3535&quot;) + ggtitle(&quot;Ouliers on depth variable in diamonds dataset&quot;) ggplot(diamonds, aes(y, z)) + geom_point(col = &quot;#9B3535&quot;) + xlab(&quot;width&quot;) + ylab(&quot;depth&quot;) 25.5.2 More informative The plots are not very informative due to the outliers. The same plots after filtering the outliers are much more informative: d2 &lt;- filter(diamonds, y &gt; 2 &amp; y &lt; 11 &amp; z &gt; 1 &amp; z &lt; 8) ggplot(d2, aes(y=y)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Ouliers on width variable in diamonds dataset&quot;) d2 &lt;- filter(diamonds, y &gt; 2 &amp; y &lt; 11 &amp; z &gt; 1 &amp; z &lt; 8) ggplot(d2, aes(y=z)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Ouliers on depth variable in diamonds dataset&quot;) d2 &lt;- filter(diamonds, y &gt; 2 &amp; y &lt; 11 &amp; z &gt; 1 &amp; z &lt; 8) ggplot(d2, aes(y, z)) + geom_point(shape = 21, color = &quot;darkGrey&quot;, fill = &quot;lightBlue&quot;, stroke = 0.1) + xlab(&quot;width&quot;) + ylab(&quot;depth&quot;) 25.6 External Resources Identify, describe, plot, and remove the outliers from the dataset: Plotting and removing outliers from a dataset A Brief Overview of Outlier Detection Techniques: Discussion of the theoretical aspect of outlier detection "],["dates.html", "26 Dates in R 26.1 Introduction 26.2 Converting to Date class 26.3 Working with Date Class 26.4 Plotting with a Date class variable 26.5 Date and time classes", " 26 Dates in R 26.1 Introduction Working with dates and time can be very frustrating. In general, work with the least cumbersome class. That means if your variable is years, store it as an integer; there’s no reason to use a date or date-time class. If your variable does not involve time, use the Date class in R. 26.2 Converting to Date class You can convert character data to Date class with as.Date(): dchar &lt;- &quot;2018-10-12&quot; ddate &lt;- as.Date(dchar) Note that the two appear the same, although the class is different: dchar ## [1] &quot;2018-10-12&quot; ddate ## [1] &quot;2018-10-12&quot; class(dchar) ## [1] &quot;character&quot; class(ddate) ## [1] &quot;Date&quot; If the date is not in YYYY-MM-DD or YYYY/MM/DD form, you will need to specify the format to convert to Date class, using conversion specifications that begin with %, such as: as.Date(&quot;Thursday, January 6, 2005&quot;, format = &quot;%A, %B %d, %Y&quot;) ## [1] &quot;2005-01-06&quot; For a list of the conversion specifications available in R, see ?strptime. The tidyverse lubridate makes it easy to convert dates that are not in standard format with ymd(), ydm(), mdy(), myd(), dmy(), and dym() (among many other useful date-time functions): lubridate::mdy(&quot;April 13, 1907&quot;) ## [1] &quot;1907-04-13&quot; Try as.Date(\"April 13, 1907\") and you will see the benefit of using a lubridate function. 26.3 Working with Date Class It is well worth the effort to convert to Date class, because there’s a lot you can do with dates in a Date class that you can’t do if you store the dates as character data. Number of days between dates: as.Date(&quot;2017-11-02&quot;) - as.Date(&quot;2017-01-01&quot;) ## Time difference of 305 days Compare dates: as.Date(&quot;2017-11-12&quot;) &gt; as.Date(&quot;2017-3-3&quot;) ## [1] TRUE Note that Sys.Date() returns today’s date as a Date class: Sys.Date() ## [1] &quot;2021-03-27&quot; class(Sys.Date()) ## [1] &quot;Date&quot; R has functions to pull particular pieces of information from a date: today &lt;- Sys.Date() weekdays(today) ## [1] &quot;Saturday&quot; weekdays(today, abbreviate = TRUE) ## [1] &quot;Sat&quot; months(today) ## [1] &quot;March&quot; months(today, abbreviate = TRUE) ## [1] &quot;Mar&quot; quarters(today) ## [1] &quot;Q1&quot; The lubridate package provides additional functions to extract information from a date: today &lt;- Sys.Date() lubridate::year(today) ## [1] 2021 lubridate::yday(today) ## [1] 86 lubridate::month(today) ## [1] 3 lubridate::month(today, label = TRUE) ## [1] Mar ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec lubridate::mday(today) ## [1] 27 lubridate::week(today) ## [1] 13 lubridate::wday(today) ## [1] 7 26.4 Plotting with a Date class variable Both base R graphics and ggplot2 “know” how to work with a Date class variable, and label the axes properly: 26.4.1 base R df &lt;- read.csv(&quot;data/mortgage.csv&quot;) df$DATE &lt;- as.Date(df$DATE) plot(df$DATE, df$X5.1.ARM, type = &quot;l&quot;) # on the order of years plot(df$DATE[1:30], df$X5.1.ARM[1:30], type = &quot;l&quot;) # switch to months Note the the change in x-axis labels in the second graph. 26.4.2 ggplot2 # readr library(tidyverse) Note that unlike base Rread.csv(), readr::read_csv() automatically reads DATE in as a Date class since it’s in YYYY-MM-DD format: df &lt;- readr::read_csv(&quot;data/mortgage.csv&quot;) g &lt;- ggplot(df, aes(DATE, `30 YR FIXED`)) + geom_line() + theme_grey(14) g ggplot(df %&gt;% filter(DATE &lt; as.Date(&quot;2006-01-01&quot;)), aes(DATE, `30 YR FIXED`)) + geom_line() + theme_grey(14) Again, when the data is filtered, the x-axis labels switch from years to months. 26.4.2.1 Breaks, limits, labels We can control the x-axis breaks, limits, and labels with scale_x_date(): library(lubridate) g + scale_x_date(limits = c(ymd(&quot;2008-01-01&quot;), ymd(&quot;2008-12-31&quot;))) + ggtitle(&quot;limits = c(ymd(\\&quot;2008-01-01\\&quot;), ymd(\\&quot;2008-12-31\\&quot;))&quot;) g + scale_x_date(date_breaks = &quot;4 years&quot;) + ggtitle(&quot;scale_x_date(date_breaks = \\&quot;4 years\\&quot;)&quot;) g + scale_x_date(date_labels = &quot;%Y-%m&quot;) + ggtitle(&quot;scale_x_date(date_labels = \\&quot;%Y-%m\\&quot;)&quot;) (Yes, even in the tidyverse we cannot completely escape the % conversion specification notation. Remember ?strptime for help.) 26.4.2.2 Annotations We can use geom_vline() with annotate() to mark specific events in a time series: ggplot(df, aes(DATE, `30 YR FIXED`)) + geom_line() + geom_vline(xintercept = ymd(&quot;2008-09-29&quot;), color = &quot;blue&quot;) + annotate(&quot;text&quot;, x = ymd(&quot;2008-09-29&quot;), y = 3.75, label = &quot; Market crash\\n 9/29/08&quot;, color = &quot;blue&quot;, hjust = 0) + scale_x_date(limits = c(ymd(&quot;2008-01-01&quot;), ymd(&quot;2009-12-31&quot;)), date_breaks = &quot;1 year&quot;, date_labels = &quot;%Y&quot;) + theme_grey(16) + ggtitle(&quot;`geom_vline()` with `annotate()`&quot;) 26.5 Date and time classes Sys.time() ## [1] &quot;2021-03-27 15:51:36 UTC&quot; Considering submitting a pull request to expand this section. "],["percept.html", "27 Perception/Color Resources 27.1 Overview 27.2 Perception 27.3 Color 27.4 Quick tips on using color with ggplot2 27.5 Links", " 27 Perception/Color Resources 27.1 Overview This section has resources for learning about graphical perception and how to use colors effectively. 27.2 Perception Here are some links to some key books/articles on perception: Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods: Classic article from William Cleveland and Robert McGill The Elements of Graphing Data: Textbook by William Cleveland Visualizing Data: Textbook by William Cleveland Creating More Effective Graphs: Textbook by Naomi Robbins 27.3 Color Color is very subjective. It is important to choose the right ones so that your work is easy to understand. Color Brewer: Excellent resource for getting effective color palettes for different projects. Its main focus is on cartography, but it is super useful for any project involving color. You can choose between different types of data (sequential, diverging, qualitative), ensure your chosen palette is effective for colorblind users (or print friendly or photocopy safe), and easily export the color palette to different formats (Adobe, GIMP/Inkscape, JS, CSS). The best go-to for effective color palettes. Color Oracle: Not sure how effective your project will be to a colorblind user? The Color Oracle tool can help. You can click to see how your screen would look to someone with various color vision deficiencies. ColorPick Eyedropper: This Chrome extension allows you to copy hex color values from webpages. Simple and intuitive, it will make creating your awesome color palettes a lot easier. 27.4 Quick tips on using color with ggplot2 One of the most common problems is confusing color and fill. geom_point() and geom_line use color, many of the other geoms use fill. Some use both, such as geom_tile() in which case color is the border color and fill is the fill color. 27.4.1 Continuous data ColorBrewer scale_color_distiller(palette = \"PuBu\") or scale_fill_distiller(palette = \"PuBu\") (What doesn’t work: scale_color_brewer(palette = \"PuBu\")) Viridis scale_color_viridis_c() or scale_fill... (the c stands for continuous) Create your own + scale_color_gradient(low = \"white\", high = \"red\") or + scale_fill... + scale_color_gradient2(low = \"red\", mid = \"white\", high = \"blue\", midpoint = 50) or + scale_fill... + scale_color_gradientn(colours = c(\"red\", \"pink\", \"lightblue\", \"blue\")) or scale_fill... 27.4.2 Discrete data ColorBrewer scale_color_brewer(palette = \"PuBu\") or scale_fill... 27.4.2.1 Viridis scale_color_viridis_d() or scale_fill... (the d stands for discrete) Create your own + scale_color_manual(values = c(\"red\", \"yellow\", \"blue\")) or scale_fill... + scale_fill_manual(values = c(\"red\", \"yellow\", \"blue\")) or scale_fill... 27.5 Links An alternative to pink &amp; blue: Colors for gender data – excellent survey of the color pallettes used for gender by major news organizations, as well as suggestions for handling this tricky topic "],["themes.html", "28 Themes and Palettes 28.1 Overview 28.2 ggplot2 themes 28.3 RColorBrewer 28.4 ggthemes 28.5 ggthemr 28.6 ggsci 28.7 External Resources", " 28 Themes and Palettes This chapter originated as a community contribution created by ar3879 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 28.1 Overview Our graphs have to be informative and attractive to the audience to get their attention. Themes and colors play an important role in making the graphs attractive. This section covers how we can set different palettes and themes to suit the context and to make them look cool. 28.2 ggplot2 themes In ggplot2, we do have a set of themes, which we can set. A brief description of them is as follows: theme_gray(): signature ggplot2 theme theme_bw(): dark on light ggplot2 theme theme_linedraw(): uses black lines on white backgrounds only theme_light(): similar to linedraw() but with grey lines as well theme_dark(): lines on a dark background instead of light theme_minimal(): no background annotations, minimal feel theme_classic(): theme with no grid lines theme_void(): empty theme with no elements 28.2.1 ggplot2 theme example q &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5,alpha = 0.75) q + theme_minimal() There are several other packages available that set the themes and colors in many ways. We will discuss 4 of them. RColorBrewer ggthemes ggthemr ggsci 28.3 RColorBrewer Often, we find ourselves looking for the colors which make our graph look clear and cool. RColorBrewer offers a number of palettes, which we can use based on the context of our graph. There are three categories of these palettes: Sequential, Diverging, and Qualitative q &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) Sequential Palette: It represents the shade of the color from light to dark. It is usually used to represent interval data where low values can be shown with a light color and high values can be shown with a dark color. For instance –Blues, BuPu, YlGn, Reds, OrRd q + scale_colour_brewer(palette = &quot;Blues&quot;) Diverging Palette: It has darker colors of contrasting hues on both the ends, and lighter color in the middle. For instance –Spectral, RdGy, PuOr q + scale_colour_brewer(palette = &quot;PuOr&quot;) Qualitative Palette: It is usually used when we want to highlight the differences in the classes (categorial variables). For instance –set1, set2, set3, pastel1, pastel2 , dark2 q + scale_colour_brewer(palette = &quot;Pastel1&quot;) 28.4 ggthemes ggthemes provides additional geoms, scales, and themes to ggplot2. Some of them are really cool! We can change the theme and color of a graph based on the context. g1 &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5,alpha = 0.75) 28.4.1 ggthemes examples g1 + theme_economist() + scale_colour_economist() g1 + theme_igray() + scale_colour_tableau() g1 + theme_wsj() + scale_color_wsj() g1 + theme_igray() + scale_colour_colorblind() If we would like to use these colors in the graphs, which may not support using ggthemes, we can use the scales package to know what colors were used for a given palette. For example: show_col(colorblind_pal()(6)) 28.5 ggthemr ggthemr is used to set the theme of ggplot graphs. It has 17 different themes to change the way ggplot graphs look. Use of ggthemr is different from other other packages. We set the theme before using it. 28.5.1 ggthemr examples ggthemr(&quot;sky&quot;) ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) ggthemr(&quot;flat&quot;) ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) Interestingly, we can set more parameters to change the themes: ggthemr(&quot;lilac&quot;, type = &quot;outer&quot;, layout = &quot;scientific&quot;, spacing = 2) ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) 28.6 ggsci ggsci offers a number of palettes inspired by colors used in scientific journals, science fiction movies, and TV shows. For continous data, scale_fill_material(colname) is used, and for discrete data, scale_color_palname() or scale_fill_palname() are used. 28.6.1 ggsci for discrete data # we need to remove the theme set previously if we don&#39;t want to use it anymore ggthemr_reset() g1 &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) g1 + scale_color_startrek() g1 + scale_color_jama() g1 + scale_color_locuszoom() 28.6.2 ggsci for continuous data ggplot(diamonds, aes(carat, price)) + geom_hex(bins = 20, color = &quot;red&quot;) + scale_fill_material(&quot;orange&quot;) We can also find out the color used, so that we can use them in some other graphs created in base R: palette = pal_lancet(&quot;lanonc&quot;, alpha = 0.7)(9) show_col(palette) 28.7 External Resources RColorBrewer: Setting up Color Palettes in R ggthemes: Github page containing more examples ggthemr: Github Repository of the package ggsci: Scientific Journal and Sci-Fi Themed Color Palettes for ggplot2 "],["publish.html", "29 Publishing Resources 29.1 Overview 29.2 tl;dr 29.3 Bookdown 29.4 Essentials 29.5 Adding a custom domain name 29.6 Make a custom 404 page 29.7 Hooking up Travis 29.8 Notes on our workflow 29.9 Other resources", " 29 Publishing Resources 29.1 Overview This section discusses how we built edav.info/ and includes references for building sites and books of your own using R. 29.2 tl;dr Want to get started making a site complete with Travis CI like this one? Zach Bogart has created a bookdown-template you can clone and build off of to create your own site. For instructions, consult the README file. 29.3 Bookdown edav.info/ is built using Bookdown, “a free and open-source R package built on top of R Markdown to make it really easy to write books and long-form articles/reports.” The biggest selling-point for bookdown is that it allows you to make content that is both professional and adaptable. If you want to update a regular book, you need to issue another edition and go through a lot of hassle. With bookdown, you can publish it in different formats (including print, if desired) and be able to change things easily when needed. We chose bookdown for edav.info/ because it allows us to present a lot of content in a compact, searchable manner, while also letting students suggest updates and contribute to its structure. Again, it is professional and adaptable (The default bookdown output is essentially just an online book, but we tried to liven it up by adding a lot of helpful icons, logos, and banners to improve navigation). Below are some helpful references we used in creating edav.info/, which may be helpful if you are interested in creating your own website or online resource with R. 29.4 Essentials How to Start a Bookdown Book: The hardest part about bookdown is getting it up and running. Sean Kross has the best template instructions we found. We started this project by cloning his template repo and building off of it. Excellent descriptions on what all the files do and what is essential to start your project. bookdown: Authoring Books and Technical Documents with R Markdown: This textbook by Yihui Xie, author of the bookdown package, explains everything bookdown is able to accomplish (published using bookdown…because of course it is). An incredible informative reference which we always kept close by. Author’s blurb: A guide to authoring books with R Markdown, including how to generate figures and tables, and insert cross-references, citations, HTML widgets, and Shiny apps in R Markdown. RStudio Bookdown Talk: Yihui Xie (author of the bookdown package) discusses his package and what it can do in a one-hour talk. Good for seeing finished examples. bookdown.org: Site for the bookdown package. Has a bunch of popular books published using bookdown and some info about how to get started using the package. Creating Websites in R: This tutorial, written by Emily Zabor (a Columbia alum), provides a thorough walkthrough for creating websites using different R tools. She discusses how to make different kinds of sites (personal, package, project, blog) as well as GitHub integration and step-by-step instructions for getting setup with templates and hosting. Very detailed and worth perusing if interested in making your own site. 29.5 Adding a custom domain name There are several parts to adding a custom domain name. Buy a domain name and edit DNS settings We used Google domains. In the registrar page, click the DNS icon and add the following to Custom resource records: NAME TYPE TTL DATA @ A 1h 185.199.108.153 www CNAME 1h @ Note that some tutorials list older IP addresses. Check here for the recommended ones. Change settings in your repo In Settings, add your custom domain name in the GitHub Pages section. Add a CNAME file to the gh-pages branch This is a very simple text file named CNAME (all caps). The contents should be one line with the custom domain name. For more detail on steps 2 and 3, see: Emily Zabor’s Tutorial on Custom Domains 29.6 Make a custom 404 page Your site may be lovely, but a default 404 page is always a let down. Not if but when someone types part of your URL incorrectly or a link gets broken, you should make sure there is something to see other than a boring backend page you had no input in designing. This article explains the process, but all you have to do is make a file called 404.html in your root directory and GitHub will use it rather than the default. Because of this, there is really no excuse for not having one. Here’s a look at our 404 page. Hopefully you aren’t seeing it that often. Some considerations: Always include a link back to the site: Throw the user a life-saver. Make it clear that something went wrong: Don’t hide the fact that this page is because of some error. Use absolute paths: The URL that throws the 404 error may be nested within unexpected folders. Make sure if you have any images or links, they work regardless of the file path (use “/images/…” rather than “images/…”, maybe link directly to css/homepage, etc.) Other than that, have fun with it!: There are plenty of examples of people making excellent 404 pages. It should make a frustrating experience just a little bit more bearable. 29.7 Hooking up Travis This tutorial is designed to help you add Travis to your GitHub Pages bookdown web site. It assumes you already have a working web site, with pages stored in a gh-pages branch. We’re not necessarily recommending the gh-pages route; we chose it since we found examples that worked for us using this method. Since the /docs folder is a newer and cleaner approach, it is certainly possible that it provides a better way to organize the repo. That said, there are various tutorials for how to set up the gh-pages branch; it appears that the best way to do so is to create an orphan branch, as explained here. We should note that this makes it all seem very easy to add Travis, which actually was not the case at all for us. I guess everything looks easy in retrospect. If you run into trouble, let us know by filing an issue or submitting a pull request. More info on all the contribution stuff can be found on our contribute page. 29.7.1 Add Travis files to GitHub repo Add these files to your repo: https://github.com/rstudio/bookdown-demo/blob/master/.travis.yml No changes https://github.com/rstudio/bookdown-demo/blob/master/_build.sh Remove the last two lines if you’re only interested in a GitHub Pages book. https://github.com/rstudio/bookdown-demo/blob/master/_deploy.sh The only changes you need to make are to the git config lines. You need to use your GitHub email, but the username can be anything. 29.7.2 Add Travis service Create a Travis account on www.travis-ci.org by clicking on “Sign in with GitHub” on the top right. Click Authorize to allow Travis to have proper access to GitHub. Go back to GitHub and create a personal access token (PAT) if you don’t have one already. You can do so here. Note that you must save your PAT somewhere because you can’t access it once it’s created. Also note that the PAT provides a means to access your GitHub repo through an API, an alternative means to logging in with your username/password (There is an API Token in Travis but this is not the one to use). Return to your Travis profile (travis-ci.org/profile/[GITHUB username]) and click the button next to the appropriate repo to toggle it on. Click on Settings next to the button and add your saved GITHUB_PAT under Environmental Variables: set “Name” to “GITHUB_PAT” and “Value” to the value of the token. If all goes well, you can sit back, relax, and watch Travis do the work for you. via GIPHY 29.8 Notes on our workflow 29.8.1 29.9 Other resources blogdown: Creating Websites with R Markdown: Textbook on the blogdownpackage, another option for generating websites with R. Getting Started with GitHub Pages: Short article from GitHub Guides on creating/hosting a website using GitHub Pages. A Beginner’s Guide to Travis CI for R: Fantastic blog post by Julia Silge, includes debugging advice that helped us solve a problem involving installing packages with system requirements. "],["workflow.html", "30 Workflow Notes", " 30 Workflow Notes The purpose of this chapter is to provide information to research assistants, teaching assistants etc. on maintaining this website. Get familiar with the bookdown package. Start with a minimal bookdown book. Understand the workflow of contributing to EDAV using Git. Before you work on the EDAV repo, practice using the bookdown-practice repository. Feel free to submit pull requests to the upstream repo for practice purpose. "],["general.html", "31 General Resources 31.1 Books 31.2 Cheatsheets 31.3 Articles 31.4 Meetups 31.5 Twitter 31.6 Data 31.7 Packages", " 31 General Resources This is a long list of helpful general resources related to EDAV. If you have come across a good resource you don’t see here, consider adding it with a pull request (see the contribute page for more info). 31.1 Books A lot of these are available for students through Columbia Libraries, in both physical and e-book formats. Graphical Data Analysis with R: This book systematically goes through the different types of data, including categorical variables, continuous variables, and time series. The author shows different examples of plotting techniques using ggplot and promoting the “grammar of graphics” model. Code snippets included and available at the book’s website. R for Data Science: The classic. Everything from data types, programming, modeling, communicating, and those keyboard shortcuts you keep forgetting. To quote the book, “this book will teach you how to do data science with R.” Nuff said. 31.2 Cheatsheets Cheatsheet of cheatsheets: Paul van der Laken has put together a large collection of R resource links, including cheat sheets, style guides, package info, blogs, and other helpful resources. RStudio Cheatsheet Collection: Collection of downloadable cheatsheets from RStudio. Includes ones on R Markdown, Data Transformation (dplyr), and Data Visualization (ggplot2). They also have a R Markdown Reference Guide, which is great for remembering that one chunk option that’s on the tip of your tongue. R Base Graphics Cheatsheet: Oddly enough, despite the length of time it’s been around, it’s hard to find a base graphics cheatsheet. Joyce put this one together to help you out if you’re using base graphics. 31.3 Articles Ten Simple Rules for Better Figures: A helpful article discussing how to make the best figures possible by following ten basic rules such as “Avoid ‘chartjunk’” and “Know Your Audience”. Good to keep these rules in mind. The Simpsons by the Data: Nice example of telling a story with data (histograms, scatterplots, etc.). Also, it’s subject is everybody’s favorite TV family. 31.4 Meetups New York Open Statistical Programming Meetup: Meetups hosted by Jared Lander and Wes McKinney on a variety of topics in statistical programming, but with a focus on the R language. Past speakers have included J.J. Allaire (founder of RStudio) and Hadley Wickham (core tidyverse developer). Other attendees are generally eager to welcome newcomers and all of their talks are available on the Lander Analytics Youtube channel. 31.5 Twitter R likes Twitter. Here are some cool people doing work with #rstats: Hadley Wickham David Robinson Julia Silge 31.6 Data Members of the United States Congress (1789-Present) with lots of biographical information https://github.com/unitedstates/congress-legislators 31.7 Packages gmailr "],["chapter-index.html", "32 Chapter Index by Resource Type 32.1 Overview 32.2 Index", " 32 Chapter Index by Resource Type 32.1 Overview This page includes links to every chapter in edav.info/ Click on a banner to go to the desired page. If you’re wondering, here’s an explanation of what the banner colors mean. 32.2 Index "],["session-information.html", "33 Session information", " 33 Session information devtools::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.2 (2020-06-22) ## os Ubuntu 16.04.6 LTS ## system x86_64, linux-gnu ## ui X11 ## language en_US.UTF-8 ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz UTC ## date 2021-03-27 ## ## ─ Packages ─────────────────────────────────────────────────────────────────────────────────────── ## package * version date lib source ## abind 1.4-7 2017-09-03 [1] R-Forge (R 4.0.2) ## acs * 2.1.4 2019-02-19 [1] CRAN (R 4.0.2) ## arm 1.11-2 2020-07-27 [1] CRAN (R 4.0.2) ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 4.0.2) ## backports 1.2.1 2020-12-09 [1] CRAN (R 4.0.2) ## base64enc 0.1-3 2015-07-28 [1] CRAN (R 4.0.2) ## bitops 1.0-6 2013-08-17 [1] CRAN (R 4.0.2) ## bookdown 0.21 2020-10-13 [1] CRAN (R 4.0.2) ## boot 1.3-25 2020-04-26 [3] CRAN (R 4.0.2) ## broom 0.7.5 2021-02-19 [1] CRAN (R 4.0.2) ## ca 0.71.1 2020-01-24 [1] CRAN (R 4.0.2) ## cachem 1.0.4 2021-02-13 [1] CRAN (R 4.0.2) ## callr 3.5.1 2020-10-13 [1] CRAN (R 4.0.2) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 4.0.2) ## checkmate 2.0.0 2020-02-06 [1] CRAN (R 4.0.2) ## choroplethr * 3.7.0 2020-08-11 [1] CRAN (R 4.0.2) ## choroplethrMaps 1.0.1 2017-01-31 [1] CRAN (R 4.0.2) ## class 7.3-17 2020-04-26 [3] CRAN (R 4.0.2) ## classInt 0.4-3 2020-04-07 [1] CRAN (R 4.0.2) ## cli 2.3.1 2021-02-23 [1] CRAN (R 4.0.2) ## cluster * 2.1.0 2019-06-19 [3] CRAN (R 4.0.2) ## coda 0.19-4 2020-09-30 [1] CRAN (R 4.0.2) ## codetools 0.2-16 2018-12-24 [3] CRAN (R 4.0.2) ## colorspace 2.0-1 2021-03-18 [1] R-Forge (R 4.0.2) ## crayon 1.4.1 2021-02-08 [1] CRAN (R 4.0.2) ## curl 4.3 2019-12-02 [1] CRAN (R 4.0.2) ## data.table 1.14.0 2021-02-21 [1] CRAN (R 4.0.2) ## DBI 1.1.1 2021-01-15 [1] CRAN (R 4.0.2) ## dbplyr 2.1.0 2021-02-03 [1] CRAN (R 4.0.2) ## desc 1.3.0 2021-03-05 [1] CRAN (R 4.0.2) ## devtools 2.3.2 2020-09-18 [1] CRAN (R 4.0.2) ## digest 0.6.27 2020-10-24 [1] CRAN (R 4.0.2) ## dplyr * 1.0.5 2021-03-05 [1] CRAN (R 4.0.2) ## e1071 1.7-6 2021-03-18 [1] CRAN (R 4.0.2) ## ellipsis 0.3.1 2020-05-15 [1] CRAN (R 4.0.2) ## emo 0.0.0.9000 2021-03-27 [1] Github (hadley/emo@3f03b11) ## evaluate 0.14 2019-05-28 [1] CRAN (R 4.0.2) ## extracat * 1.7-6 2018-07-17 [1] Github (cran/extracat@4795f14) ## fansi 0.4.2 2021-01-15 [1] CRAN (R 4.0.2) ## farver 2.1.0 2021-02-28 [1] CRAN (R 4.0.2) ## fastmap 1.1.0 2021-01-25 [1] CRAN (R 4.0.2) ## forcats * 0.5.1.9000 2021-03-27 [1] Github (tidyverse/forcats@b5fce89) ## foreach 1.5.1 2020-10-15 [1] CRAN (R 4.0.2) ## foreign 0.8-80 2020-05-24 [3] CRAN (R 4.0.2) ## Formula 1.2-4 2020-10-16 [1] CRAN (R 4.0.2) ## fs 1.5.0 2020-07-31 [1] CRAN (R 4.0.2) ## furrr 0.2.2 2021-01-29 [1] CRAN (R 4.0.2) ## future 1.21.0 2020-12-10 [1] CRAN (R 4.0.2) ## GDAdata * 0.93 2015-05-02 [1] CRAN (R 4.0.2) ## generics 0.1.0 2020-10-31 [1] CRAN (R 4.0.2) ## GGally * 2.1.1 2021-03-08 [1] CRAN (R 4.0.2) ## ggmap 3.0.0 2019-02-05 [1] CRAN (R 4.0.2) ## ggplot2 * 3.3.3 2020-12-30 [1] CRAN (R 4.0.2) ## ggplot2movies * 0.0.1 2015-08-25 [1] CRAN (R 4.0.2) ## ggridges * 0.5.3 2021-01-08 [1] CRAN (R 4.0.2) ## ggsci * 2.9 2018-05-14 [1] CRAN (R 4.0.2) ## ggthemes * 4.2.4 2021-01-20 [1] CRAN (R 4.0.2) ## ggthemr * 1.1.0 2021-03-27 [1] Github (cttobin/ggthemr@4a31e0d) ## globals 0.14.0 2020-11-22 [1] CRAN (R 4.0.2) ## glue 1.4.2 2020-08-27 [1] CRAN (R 4.0.2) ## gnm 1.1-1 2020-02-03 [1] CRAN (R 4.0.2) ## gower 0.2.2 2020-06-23 [1] CRAN (R 4.0.2) ## gridExtra * 2.3 2017-09-09 [1] CRAN (R 4.0.2) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 4.0.2) ## haven 2.3.1 2020-06-01 [1] CRAN (R 4.0.2) ## hexbin 1.28.2 2021-01-08 [1] CRAN (R 4.0.2) ## highr 0.8 2019-03-20 [1] CRAN (R 4.0.2) ## Hmisc 4.5-0 2021-02-28 [1] CRAN (R 4.0.2) ## hms 1.0.0 2021-01-13 [1] CRAN (R 4.0.2) ## htmlTable 2.1.0 2020-09-16 [1] CRAN (R 4.0.2) ## htmltools 0.5.1.1 2021-01-22 [1] CRAN (R 4.0.2) ## htmlwidgets 1.5.3 2020-12-10 [1] CRAN (R 4.0.2) ## httr 1.4.2 2020-07-20 [1] CRAN (R 4.0.2) ## ipred 0.9-11 2021-03-12 [1] CRAN (R 4.0.2) ## isoband 0.2.4 2021-03-03 [1] CRAN (R 4.0.2) ## iterators 1.0.13 2020-10-15 [1] CRAN (R 4.0.2) ## jpeg 0.1-8.1 2019-10-24 [1] CRAN (R 4.0.2) ## jsonlite * 1.7.2 2020-12-09 [1] CRAN (R 4.0.2) ## KernSmooth 2.23-17 2020-04-26 [3] CRAN (R 4.0.2) ## knitr * 1.31 2021-01-27 [1] CRAN (R 4.0.2) ## labeling 0.4.2 2020-10-20 [1] CRAN (R 4.0.2) ## lattice * 0.20-41 2020-04-02 [3] CRAN (R 4.0.2) ## latticeExtra 0.6-30 2020-09-16 [1] R-Forge (R 4.0.2) ## lava 1.6.9 2021-03-11 [1] CRAN (R 4.0.2) ## lazyeval 0.2.2 2019-03-15 [1] CRAN (R 4.0.2) ## lifecycle 1.0.0 2021-02-15 [1] CRAN (R 4.0.2) ## likert * 1.3.5 2016-12-31 [1] CRAN (R 4.0.2) ## listenv 0.8.0 2019-12-05 [1] CRAN (R 4.0.2) ## lme4 1.1-26 2020-12-01 [1] CRAN (R 4.0.2) ## lmtest 0.9-38 2020-09-09 [1] CRAN (R 4.0.2) ## lubridate * 1.7.10 2021-02-26 [1] CRAN (R 4.0.2) ## magrittr 2.0.1 2020-11-17 [1] CRAN (R 4.0.2) ## maptools 1.1-1 2021-03-15 [1] CRAN (R 4.0.2) ## MASS 7.3-51.6 2020-04-26 [3] CRAN (R 4.0.2) ## Matrix * 1.2-18 2019-11-27 [3] CRAN (R 4.0.2) ## memoise 2.0.0 2021-01-26 [1] CRAN (R 4.0.2) ## mgcv 1.8-31 2019-11-09 [3] CRAN (R 4.0.2) ## mi * 1.0 2015-04-16 [1] CRAN (R 4.0.2) ## minqa 1.2.4 2014-10-09 [1] CRAN (R 4.0.2) ## mnormt 2.0.2 2020-09-01 [1] CRAN (R 4.0.2) ## modelr 0.1.8 2020-05-19 [1] CRAN (R 4.0.2) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.0.2) ## nlme 3.1-148 2020-05-24 [3] CRAN (R 4.0.2) ## nloptr 1.2.2.2 2020-07-02 [1] CRAN (R 4.0.2) ## nnet 7.3-14 2020-04-26 [3] CRAN (R 4.0.2) ## parallelly 1.24.0 2021-03-14 [1] CRAN (R 4.0.2) ## PerformanceAnalytics * 2.0.4 2020-02-06 [1] CRAN (R 4.0.2) ## pgmm * 1.2.4 2019-12-04 [1] CRAN (R 4.0.2) ## pillar 1.5.1 2021-03-05 [1] CRAN (R 4.0.2) ## pkgbuild 1.2.0 2020-12-15 [1] CRAN (R 4.0.2) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.0.2) ## pkgload 1.2.0 2021-02-23 [1] CRAN (R 4.0.2) ## plyr 1.8.6 2020-03-03 [1] CRAN (R 4.0.2) ## png 0.1-7 2013-12-03 [1] CRAN (R 4.0.2) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 4.0.2) ## processx 3.5.0 2021-03-23 [1] CRAN (R 4.0.2) ## prodlim 2019.11.13 2019-11-17 [1] CRAN (R 4.0.2) ## proxy 0.4-25 2021-03-05 [1] CRAN (R 4.0.2) ## ps 1.6.0 2021-02-28 [1] CRAN (R 4.0.2) ## psych 2.0.12 2020-12-16 [1] CRAN (R 4.0.2) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.0.2) ## quadprog 1.5-8 2019-11-20 [1] CRAN (R 4.0.2) ## Quandl 2.10.0 2019-06-12 [1] CRAN (R 4.0.2) ## quantmod * 0.4.18 2020-12-09 [1] CRAN (R 4.0.2) ## qvcalc 1.0.2 2020-02-15 [1] CRAN (R 4.0.2) ## R6 2.5.0 2020-10-28 [1] CRAN (R 4.0.2) ## rappdirs 0.3.3 2021-01-31 [1] CRAN (R 4.0.2) ## RColorBrewer 1.1-2 2014-12-07 [1] CRAN (R 4.0.2) ## Rcpp 1.0.6 2021-01-15 [1] CRAN (R 4.0.2) ## readr * 1.4.0 2020-10-05 [1] CRAN (R 4.0.2) ## readxl 1.3.1 2019-03-13 [1] CRAN (R 4.0.2) ## recipes 0.1.15 2020-11-11 [1] CRAN (R 4.0.2) ## relimp 1.0-5 2016-03-30 [1] CRAN (R 4.0.2) ## remotes 2.2.0 2020-07-21 [1] CRAN (R 4.0.2) ## reprex 1.0.0 2021-01-27 [1] CRAN (R 4.0.2) ## reshape 0.8.8 2018-10-23 [1] CRAN (R 4.0.2) ## reshape2 1.4.4 2020-04-09 [1] CRAN (R 4.0.2) ## rgdal 1.5-24 2021-02-23 [1] R-Forge (R 4.0.2) ## RgoogleMaps 1.4.5.3 2020-02-12 [1] CRAN (R 4.0.2) ## rjson 0.2.20 2018-06-08 [1] CRAN (R 4.0.2) ## RJSONIO 1.3-1.4 2020-01-15 [1] CRAN (R 4.0.2) ## rlang 0.4.10 2020-12-30 [1] CRAN (R 4.0.2) ## rmarkdown 2.7 2021-02-19 [1] CRAN (R 4.0.2) ## rpart 4.1-15 2019-04-12 [3] CRAN (R 4.0.2) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.0.2) ## rsample 0.0.9 2021-02-17 [1] CRAN (R 4.0.2) ## rstudioapi 0.13 2020-11-12 [1] CRAN (R 4.0.2) ## rvest * 1.0.0 2021-03-09 [1] CRAN (R 4.0.2) ## scales * 1.1.1 2020-05-11 [1] CRAN (R 4.0.2) ## selectr 0.4-2 2019-11-20 [1] CRAN (R 4.0.2) ## sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 4.0.2) ## sf 0.9-8 2021-03-17 [1] CRAN (R 4.0.2) ## Sleuth3 * 1.0-3 2019-01-25 [1] CRAN (R 4.0.2) ## sp 1.4-5 2021-01-10 [1] CRAN (R 4.0.2) ## statebins * 1.4.0 2021-03-27 [1] Github (hrbrmstr/statebins@da22fb7) ## statmod 1.4.35 2020-10-19 [1] CRAN (R 4.0.2) ## stringi 1.5.3 2020-09-09 [1] CRAN (R 4.0.2) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.0.2) ## survival 3.1-12 2020-04-10 [3] CRAN (R 4.0.2) ## testthat 3.0.2 2021-02-14 [1] CRAN (R 4.0.2) ## tibble * 3.1.0 2021-02-25 [1] CRAN (R 4.0.2) ## tidycensus 0.11.4 2021-01-20 [1] CRAN (R 4.0.2) ## tidyquant * 1.0.3 2021-03-05 [1] CRAN (R 4.0.2) ## tidyr * 1.1.3 2021-03-03 [1] CRAN (R 4.0.2) ## tidyselect 1.1.0 2020-05-11 [1] CRAN (R 4.0.2) ## tidyverse * 1.3.0 2019-11-21 [1] CRAN (R 4.0.2) ## tigris 1.0 2020-07-13 [1] CRAN (R 4.0.2) ## timeDate 3043.103 2018-11-29 [1] R-Forge (R 4.0.2) ## timetk 2.6.1 2021-01-18 [1] CRAN (R 4.0.2) ## tmvnsim 1.0-2 2016-12-15 [1] CRAN (R 4.0.2) ## TSP 1.1-10 2020-04-17 [1] CRAN (R 4.0.2) ## TTR * 0.24.2 2020-09-01 [1] CRAN (R 4.0.2) ## units 0.7-1 2021-03-16 [1] CRAN (R 4.0.2) ## usethis 2.0.1 2021-02-10 [1] CRAN (R 4.0.2) ## utf8 1.2.1 2021-03-12 [1] CRAN (R 4.0.2) ## uuid 0.1-4 2020-02-26 [1] CRAN (R 4.0.2) ## vcd 1.4-8 2020-09-21 [1] CRAN (R 4.0.2) ## vcdExtra 0.7-5 2021-01-25 [1] CRAN (R 4.0.2) ## vctrs 0.3.6 2020-12-17 [1] CRAN (R 4.0.2) ## viridis * 0.5.1 2018-03-29 [1] CRAN (R 4.0.2) ## viridisLite * 0.3.0 2018-02-01 [1] CRAN (R 4.0.2) ## visNetwork * 2.0.9 2019-12-06 [1] CRAN (R 4.0.2) ## WDI 2.7.2 2021-01-22 [1] CRAN (R 4.0.2) ## withr 2.4.1 2021-01-26 [1] CRAN (R 4.0.2) ## xfun 0.22 2021-03-11 [1] CRAN (R 4.0.2) ## XML * 3.99-0.6 2021-03-16 [1] CRAN (R 4.0.2) ## xml2 1.3.2 2020-04-23 [1] CRAN (R 4.0.2) ## xtable * 1.8-6 2020-06-19 [1] R-Forge (R 4.0.2) ## xts * 0.12.1 2020-09-09 [1] CRAN (R 4.0.2) ## yaml 2.2.1 2020-02-01 [1] CRAN (R 4.0.2) ## zoo * 1.8-9 2021-03-09 [1] CRAN (R 4.0.2) ## ## [1] /home/travis/R/Library ## [2] /usr/local/lib/R/site-library ## [3] /opt/R/4.0.2/lib/R/library "]]
